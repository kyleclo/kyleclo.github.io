---
layout: post
date: 2024-11-26 00:00:01-0800
inline: true
---

**OLMo 2** is here! In our latest paper ðŸš— [2 OLMo 2 Furious](https://arxiv.org/abs/2501.00656) ðŸ”¥, we discuss everything we've learned since OLMo 1 with deep dives into ðŸš– stable pretraining and ðŸš” mid-training which uses learning rate annealing, data curricula, and model checkpoint averaging. Our training recipe is state-of-the-art with respect to training FLOPs to performance!ðŸ“ˆ Check out the [blog post](https://allenai.org/blog/olmo2) and download our 7B and 13B model weights, data, etc on [HuggingFace](https://huggingface.co/collections/allenai/olmo-2-674117b93ab84e98afc72edc)!
