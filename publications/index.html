<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Kyle Lo </title> <meta name="author" content="Kyle Lo"> <meta name="description" content="NLP + HCI researcher at Ai2 in Seattle. Open language models &amp; data-centric methods. "> <meta name="keywords" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/kyle_lo_profile.jpg?5000aa7cef6c58c176ece7e412b4f608"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="kyleclo.com/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Kyle</span> Lo </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/mentorship/">mentorship </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/signal-and-noise-a-framework-for-reducing-uncertainty-in-language-model-evaluation-480.webp 480w,/assets/img/publication_preview/signal-and-noise-a-framework-for-reducing-uncertainty-in-language-model-evaluation-800.webp 800w,/assets/img/publication_preview/signal-and-noise-a-framework-for-reducing-uncertainty-in-language-model-evaluation-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/signal-and-noise-a-framework-for-reducing-uncertainty-in-language-model-evaluation.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="signal-and-noise-a-framework-for-reducing-uncertainty-in-language-model-evaluation.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Heineman2025SignalAndNoise" class="col-sm-8"> <div class="title">Signal and noise: A framework for reducing uncertainty in language model evaluation</div> <div class="author"> David Heineman,¬†Valentin Hofmann,¬†Ian Magnusson,¬†Yuling Gu,¬†Noah A Smith,¬†Hannaneh Hajishirzi,¬†<em>Kyle Lo</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jesse Dodge' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">1 more author</span> </div> <div class="periodical"> <em>In NeurIPS (Datasets and Benchmarks)</em>, Dec 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2508.13144" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/signal-and-noise-a-framework-for-reducing-uncertainty-in-language-model-evaluation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Developing large language models is expensive and involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. In this work, we analyze specific properties which make a benchmark more reliable for such decisions, and interventions to design higher-quality evaluation benchmarks. We introduce two key metrics that show differences in current benchmarks: signal, a benchmark‚Äôs ability to separate better models from worse models, and noise, a benchmark‚Äôs sensitivity to random variability between training steps. We demonstrate that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error. These results suggest that improving signal or noise will lead to more useful benchmarks, so we introduce three interventions designed to directly affect signal or noise. For example, we propose that switching to a metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and improved scaling law error. We also find that filtering noisy subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable multi-task evaluations. We also find that averaging the output of a model‚Äôs intermediate checkpoints to reduce noise leads to consistent improvements. We conclude by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise. We use 30 benchmarks for these experiments, and 375 open-weight language models from 60M to 32B parameters, resulting in a new, publicly ‚Ä¶</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Heineman2025SignalAndNoise</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Heineman, David and Hofmann, Valentin and Magnusson, Ian and Gu, Yuling and Smith, Noah A and Hajishirzi, Hannaneh and Lo, Kyle and Dodge, Jesse}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NeurIPS (Datasets and Benchmarks)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Signal and noise: A framework for reducing uncertainty in language model evaluation}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2508.13144}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/flexolmo-open-language-models-for-flexible-data-use-480.webp 480w,/assets/img/publication_preview/flexolmo-open-language-models-for-flexible-data-use-800.webp 800w,/assets/img/publication_preview/flexolmo-open-language-models-for-flexible-data-use-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/flexolmo-open-language-models-for-flexible-data-use.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="flexolmo-open-language-models-for-flexible-data-use.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Shi2025FlexolmoOpenLanguage" class="col-sm-8"> <div class="title">FlexOlmo: Open Language Models for Flexible Data Use</div> <div class="author"> Weijia Shi,¬†Akshita Bhagia,¬†Kevin Farhat,¬†Niklas Muennighoff,¬†Pete Walsh,¬†Jacob Morrison,¬†Dustin Schwenk, and <span class="more-authors" title="click to view 16 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '16 more authors' ? 'Shayne Longpre, Jake Poznanski, Allyson Ettinger, Daogao Liu, Margaret Li, Dirk Groeneveld, Mike Lewis, Wen-tau Yih, Luca Soldaini, Kyle Lo, Noah A Smith, Luke Zettlemoyer, Pang Wei Koh, Hannaneh Hajishirzi, Ali Farhadi, Sewon Min' : '16 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">16 more authors</span> </div> <div class="periodical"> <em>In NeurIPS</em>, Dec 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2507.07024" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/flexolmo-open-language-models-for-flexible-data-use.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We introduce FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners‚Äô preferences by keeping their data local and supporting fine-grained control of data access during inference.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Shi2025FlexolmoOpenLanguage</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shi, Weijia and Bhagia, Akshita and Farhat, Kevin and Muennighoff, Niklas and Walsh, Pete and Morrison, Jacob and Schwenk, Dustin and Longpre, Shayne and Poznanski, Jake and Ettinger, Allyson and Liu, Daogao and Li, Margaret and Groeneveld, Dirk and Lewis, Mike and Yih, Wen-tau and Soldaini, Luca and Lo, Kyle and Smith, Noah A and Zettlemoyer, Luke and Koh, Pang Wei and Hajishirzi, Hannaneh and Farhadi, Ali and Min, Sewon}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NeurIPS}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FlexOlmo: Open Language Models for Flexible Data Use}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=1rUj9ZN6Bz}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fluid-language-model-benchmarking-480.webp 480w,/assets/img/publication_preview/fluid-language-model-benchmarking-800.webp 800w,/assets/img/publication_preview/fluid-language-model-benchmarking-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/fluid-language-model-benchmarking.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fluid-language-model-benchmarking.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Hofmann2025FluidLanguageModel" class="col-sm-8"> <div class="title">Fluid language model benchmarking</div> <div class="author"> Valentin Hofmann,¬†David Heineman,¬†Ian Magnusson,¬†<em>Kyle Lo</em>,¬†Jesse Dodge,¬†Maarten Sap,¬†Pang Wei Koh, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Chun Wang, Hannaneh Hajishirzi, Noah A Smith' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">3 more authors</span> </div> <div class="periodical"> <em>In COLM</em>, Oct 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2509.11106" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/fluid-language-model-benchmarking.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Language model (LM) benchmarking faces several challenges: comprehensive evaluations are costly, benchmarks often fail to measure the intended capabilities, and evaluation quality can degrade due to labeling errors and benchmark saturation. Although various strategies have been proposed to mitigate these issues, they tend to address individual aspects in isolation, neglecting broader questions about overall evaluation quality. Here, we introduce Fluid Benchmarking, a new evaluation approach that advances LM benchmarking across multiple dimensions. Inspired by psychometrics, Fluid Benchmarking is based on the insight that the relative value of benchmark items depends on an LM‚Äôs capability level, suggesting that evaluation should adapt to each LM. Methodologically, Fluid Benchmarking estimates an item response model based on existing LM evaluation results and uses the inferred quantities to select evaluation items dynamically, similar to computerized adaptive testing in education. In our experiments, we compare Fluid Benchmarking against the common practice of random item sampling as well as more sophisticated baselines, including alternative methods grounded in item response theory. We examine four dimensions ‚Äì efficiency, validity, variance, and saturation ‚Äì and find that Fluid Benchmarking achieves superior performance in all of them (e.g., higher validity and less variance on MMLU with fifty times fewer items). Our analysis shows that the two components of Fluid Benchmarking have distinct effects: item response theory, used to map performance into a latent ability space, increases validity, while dynamic item ‚Ä¶</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Hofmann2025FluidLanguageModel</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hofmann, Valentin and Heineman, David and Magnusson, Ian and Lo, Kyle and Dodge, Jesse and Sap, Maarten and Koh, Pang Wei and Wang, Chun and Hajishirzi, Hannaneh and Smith, Noah A}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{COLM}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fluid language model benchmarking}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2509.11106}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/llms-as-research-tools-a-large-scale-survey-of-researchers-usage-and-perceptions-480.webp 480w,/assets/img/publication_preview/llms-as-research-tools-a-large-scale-survey-of-researchers-usage-and-perceptions-800.webp 800w,/assets/img/publication_preview/llms-as-research-tools-a-large-scale-survey-of-researchers-usage-and-perceptions-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/llms-as-research-tools-a-large-scale-survey-of-researchers-usage-and-perceptions.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="llms-as-research-tools-a-large-scale-survey-of-researchers-usage-and-perceptions.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Liao2024LlmsAsResearch" class="col-sm-8"> <div class="title">LLMs as Research Tools: A Large Scale Survey of Researchers‚Äô Usage and Perceptions</div> <div class="author"> Zhehui Liao,¬†Maria Antoniak,¬†Inyoung Cheong,¬†Evie Yu-Yen Cheng,¬†Ai-Heng Lee,¬†<em>Kyle Lo</em>,¬†Joseph Chee Chang, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Amy X Zhang' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">1 more author</span> </div> <div class="periodical"> <em>In COLM</em>, Oct 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2411.05025" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/llms-as-research-tools-a-large-scale-survey-of-researchers-usage-and-perceptions.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The rise of large language models (LLMs) has led many researchers to consider their usage for scientific work. Some have found benefits using LLMs to augment or automate aspects of their research pipeline, while others have urged caution due to risks and ethical concerns. Yet little work has sought to quantify and characterize how researchers use LLMs and why. We present the first large-scale survey of 816 verified research article authors to understand how the research community leverages and perceives LLMs as research tools. We examine participants‚Äô self-reported LLM usage, finding that 81% of researchers have already incorporated LLMs into different aspects of their research workflow. We also find that traditionally disadvantaged groups in academia (non-White, junior, and non-native English speaking researchers) report higher LLM usage and perceived benefits, suggesting potential for improved research equity. However, women, non-binary, and senior researchers have greater ethical concerns, potentially hindering adoption.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Liao2024LlmsAsResearch</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liao, Zhehui and Antoniak, Maria and Cheong, Inyoung and Cheng, Evie Yu-Yen and Lee, Ai-Heng and Lo, Kyle and Chang, Joseph Chee and Zhang, Amy X}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{COLM}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LLMs as Research Tools: A Large Scale Survey of Researchers' Usage and Perceptions}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2411.05025}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/contextualized-evaluations-judging-language-model-responses-to-underspecified-queries-480.webp 480w,/assets/img/publication_preview/contextualized-evaluations-judging-language-model-responses-to-underspecified-queries-800.webp 800w,/assets/img/publication_preview/contextualized-evaluations-judging-language-model-responses-to-underspecified-queries-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/contextualized-evaluations-judging-language-model-responses-to-underspecified-queries.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="contextualized-evaluations-judging-language-model-responses-to-underspecified-queries.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Malaviya2025ContextualizedEvaluationsJudging" class="col-sm-8"> <div class="title">Contextualized evaluations: Judging language model responses to underspecified queries</div> <div class="author"> Chaitanya Malaviya,¬†Joseph Chee Chang,¬†Dan Roth,¬†Mohit Iyyer,¬†Mark Yatskar,¬†and¬†<em>Kyle Lo</em> </div> <div class="periodical"> <em>TACL</em>, Jul 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1162/TACL.a.24" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2411.07237" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/contextualized-evaluations-judging-language-model-responses-to-underspecified-queries.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Language model users often issue queries that lack specification, where the context under which a query was issued‚Äîsuch as the user‚Äôs identity, the query‚Äôs intent, and the criteria for a response to be useful‚Äîis not explicit. For instance, a good response to a subjective query like ‚ÄúWhat book should I read next?‚Äù would depend on the user‚Äôs preferences, and a good response to an open-ended query like ‚ÄúHow do antibiotics work against bacteria?‚Äù would depend on the user‚Äôs expertise. This makes evaluation of responses to such queries an ill-posed task, as evaluators may make arbitrary judgments about the response quality. To remedy this, we present contextualized evaluations, a protocol that synthetically constructs context surrounding an underspecified query and provides it during evaluation. We find that the presence of context can 1) alter conclusions drawn from evaluation, even flipping benchmark ‚Ä¶</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Malaviya2025ContextualizedEvaluationsJudging</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Malaviya, Chaitanya and Chang, Joseph Chee and Roth, Dan and Iyyer, Mohit and Yatskar, Mark and Lo, Kyle}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1162/TACL.a.24}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{TACL}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Contextualized evaluations: Judging language model responses to underspecified queries}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2411.07237}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/organize-the-web-constructing-domains-enhances-pre-training-data-curation-480.webp 480w,/assets/img/publication_preview/organize-the-web-constructing-domains-enhances-pre-training-data-curation-800.webp 800w,/assets/img/publication_preview/organize-the-web-constructing-domains-enhances-pre-training-data-curation-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/organize-the-web-constructing-domains-enhances-pre-training-data-curation.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="organize-the-web-constructing-domains-enhances-pre-training-data-curation.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Wettig2025OrganizeWeb" class="col-sm-8"> <div class="title">Organize the Web: Constructing Domains Enhances Pre-Training Data Curation</div> <div class="author"> Alexander Wettig,¬†<em>Kyle Lo</em>,¬†Sewon Min,¬†Hannaneh Hajishirzi,¬†Danqi Chen,¬†and¬†Luca Soldaini </div> <div class="periodical"> <em>In ICML</em>, Jul 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.10341" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/organize-the-web-constructing-domains-enhances-pre-training-data-curation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Modern language models are trained on large, unstructured datasets consisting of trillions of tokens and obtained by crawling the web. The unstructured nature makes it difficult to reason about their contents and develop systematic approaches to data curation. In this paper, we unpack monolithic web corpora by developing taxonomies of their contents and organizing them into domains. We introduce WebOrganizer, a framework for organizing web pages in terms of both their topic and format. Using these two complementary notions of domains, we automatically annotate pre-training data by distilling annotations from a large language model into efficient classifiers. This allows us to study how data from different domains should be mixed to improve models on downstream tasks, and we show that we can combine insights about effective topics and formats to further boost performance. We demonstrate that our domain mixing also improves existing methods that select data based on quality. Furthermore, we study and compare how quality-based methods will implicitly change the domain mixture. Overall, our work demonstrates that constructing and mixing domains provides a valuable complement to quality-based data curation methods, opening new avenues for effective and insightful pre-training data curation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Wettig2025OrganizeWeb</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wettig, Alexander and Lo, Kyle and Min, Sewon and Hajishirzi, Hannaneh and Chen, Danqi and Soldaini, Luca}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICML}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Organize the Web: Constructing Domains Enhances Pre-Training Data Curation}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=boSqwdvJVC}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/molmo-and-pixmo-open-weights-and-open-data-for-state-of-the-art-vision-language-models-480.webp 480w,/assets/img/publication_preview/molmo-and-pixmo-open-weights-and-open-data-for-state-of-the-art-vision-language-models-800.webp 800w,/assets/img/publication_preview/molmo-and-pixmo-open-weights-and-open-data-for-state-of-the-art-vision-language-models-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/molmo-and-pixmo-open-weights-and-open-data-for-state-of-the-art-vision-language-models.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="molmo-and-pixmo-open-weights-and-open-data-for-state-of-the-art-vision-language-models.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Deitke2025MolmoAndPixmo" class="col-sm-8"> <div class="title">Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models</div> <div class="author"> Matt Deitke,¬†Christopher Clark,¬†Sangho Lee,¬†Rohun Tripathi,¬†Yue Yang,¬†Jae Sung Park,¬†Mohammadreza Salehi, and <span class="more-authors" title="click to view 43 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '43 more authors' ? 'Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi' : '43 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">43 more authors</span> </div> <div class="periodical"> <em>In CVPR</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <div>üèÜ <strong>Best Paper Honorable Mention</strong> üèÜ</div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2409.17146" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/molmo-and-pixmo-open-weights-and-open-data-for-state-of-the-art-vision-language-models.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Honorable Mention</p> </div> <div class="abstract hidden"> <p>Today‚Äôs most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&amp;A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Deitke2025MolmoAndPixmo</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Deitke, Matt and Clark, Christopher and Lee, Sangho and Tripathi, Rohun and Yang, Yue and Park, Jae Sung and Salehi, Mohammadreza and Muennighoff, Niklas and Lo, Kyle and Soldaini, Luca and Lu, Jiasen and Anderson, Taira and Bransom, Erin and Ehsani, Kiana and Ngo, Huong and Chen, YenSung and Patel, Ajay and Yatskar, Mark and Callison-Burch, Chris and Head, Andrew and Hendrix, Rose and Bastani, Favyen and VanderBilt, Eli and Lambert, Nathan and Chou, Yvonne and Chheda, Arnavi and Sparks, Jenna and Skjonsberg, Sam and Schmitz, Michael and Sarnat, Aaron and Bischoff, Byron and Walsh, Pete and Newell, Chris and Wolters, Piper and Gupta, Tanmay and Zeng, Kuo-Hao and Borchardt, Jon and Groeneveld, Dirk and Nam, Crystal and Lebrecht, Sophie and Wittlif, Caitlin and Schoenick, Carissa and Michel, Oscar and Krishna, Ranjay and Weihs, Luca and Smith, Noah A and Hajishirzi, Hannaneh and Girshick, Ross and Farhadi, Ali and Kembhavi, Aniruddha}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://openaccess.thecvf.com/content/CVPR2025/html/Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art_CVPR_2025_paper.html}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/drawedumath-evaluating-vision-language-models-with-expert-annotated-students-hand-drawn-math-images-480.webp 480w,/assets/img/publication_preview/drawedumath-evaluating-vision-language-models-with-expert-annotated-students-hand-drawn-math-images-800.webp 800w,/assets/img/publication_preview/drawedumath-evaluating-vision-language-models-with-expert-annotated-students-hand-drawn-math-images-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/drawedumath-evaluating-vision-language-models-with-expert-annotated-students-hand-drawn-math-images.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="drawedumath-evaluating-vision-language-models-with-expert-annotated-students-hand-drawn-math-images.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Baral2025DrawedumathEvaluatingVision" class="col-sm-8"> <div class="title">DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students‚Äô Hand-Drawn Math Images</div> <div class="author"> Sami Baral,¬†Li Lucy,¬†Ryan Knight,¬†Alice Ng,¬†Luca Soldaini,¬†Neil T Heffernan,¬†and¬†<em>Kyle Lo</em> </div> <div class="periodical"> <em>In NAACL</em>, Apr 2025 </div> <div class="periodical"> </div> <div class="links"> <div>üèÜ <strong>Outstanding Paper Award</strong> üèÜ</div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2501.14877" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/drawedumath-evaluating-vision-language-models-with-expert-annotated-students-hand-drawn-math-images.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Outstanding Paper Award</p> </div> <div class="abstract hidden"> <p>In real-world settings, vision language models (VLMs) should robustly handle naturalistic, noisy visual content as well as domain-specific language and concepts. For example, K-12 educators using digital learning platforms may need to examine and provide feedback across many images of students‚Äô math work. To assess the potential of VLMs to support educators in settings like this one, we introduce DrawEduMath, an English-language dataset of 2,030 images of students‚Äô handwritten responses to K-12 math problems. Teachers provided detailed annotations, including free-form descriptions of each image and 11,661 question-answer (QA) pairs. These annotations capture a wealth of pedagogical insights, ranging from students‚Äô problem-solving strategies to the composition of their drawings, diagrams, and writing. We evaluate VLMs on teachers‚Äô QA pairs, as well as 44,362 synthetic QA pairs derived from teachers‚Äô descriptions using language models (LMs). We show that even state-of-the-art VLMs leave much room for improvement on DrawEduMath questions. We also find that synthetic QAs, though imperfect, can yield similar model rankings as teacher-written QAs. We release DrawEduMath to support the evaluation of VLMs‚Äô abilities to reason mathematically over images gathered with educational contexts in mind.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Baral2025DrawedumathEvaluatingVision</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Baral, Sami and Lucy, Li and Knight, Ryan and Ng, Alice and Soldaini, Luca and Heffernan, Neil T and Lo, Kyle}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NAACL}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students' Hand-Drawn Math Images}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2025.naacl-long.352/}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/openscholar-synthesizing-scientific-literature-with-retrieval-augmented-lms-480.webp 480w,/assets/img/publication_preview/openscholar-synthesizing-scientific-literature-with-retrieval-augmented-lms-800.webp 800w,/assets/img/publication_preview/openscholar-synthesizing-scientific-literature-with-retrieval-augmented-lms-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/openscholar-synthesizing-scientific-literature-with-retrieval-augmented-lms.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="openscholar-synthesizing-scientific-literature-with-retrieval-augmented-lms.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Asai2024OpenscholarSynthesizingScientific" class="col-sm-8"> <div class="title">OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs</div> <div class="author"> Akari Asai,¬†Jacqueline He,¬†Rulin Shao,¬†Weijia Shi,¬†Amanpreet Singh,¬†Joseph Chee Chang,¬†<em>Kyle Lo</em>, and <span class="more-authors" title="click to view 18 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '18 more authors' ? 'Luca Soldaini, Sergey Feldman, Mike D‚Äôarcy, David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, Graham Neubig, Dan Weld, Doug Downey, Wen-tau Yih, Pang Wei Koh, Hannaneh Hajishirzi' : '18 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">18 more authors</span> </div> <div class="periodical"> <em>ArXiv</em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2411.14199" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/openscholar-synthesizing-scientific-literature-with-retrieval-augmented-lms.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Scientific progress depends on researchers‚Äô ability to synthesize the growing body of literature. Can large language models (LMs) assist scientists in this task? We introduce OpenScholar, a specialized retrieval-augmented LM that answers scientific queries by identifying relevant passages from 45 million open-access papers and synthesizing citation-backed responses. To evaluate OpenScholar, we develop ScholarQABench, the first large-scale multi-domain benchmark for literature search, comprising 2,967 expert-written queries and 208 long-form answers across computer science, physics, neuroscience, and biomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and PaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o hallucinates citations 78 to 90% of the time, OpenScholar achieves citation accuracy on par with human experts. OpenScholar‚Äôs datastore, retriever, and self-feedback inference loop also improves off-the-shelf LMs: for instance, OpenScholar-GPT4o improves GPT-4o‚Äôs correctness by 12%. In human evaluations, experts preferred OpenScholar-8B and OpenScholar-GPT4o responses over expert-written ones 51% and 70% of the time, respectively, compared to GPT4o‚Äôs 32%. We open-source all of our code, models, datastore, data and a public demo.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Asai2024OpenscholarSynthesizingScientific</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Asai, Akari and He, Jacqueline and Shao, Rulin and Shi, Weijia and Singh, Amanpreet and Chang, Joseph Chee and Lo, Kyle and Soldaini, Luca and Feldman, Sergey and D'arcy, Mike and Wadden, David and Latzke, Matt and Tian, Minyang and Ji, Pan and Liu, Shengyan and Tong, Hao and Wu, Bohao and Xiong, Yanyu and Zettlemoyer, Luke and Neubig, Graham and Weld, Dan and Downey, Doug and Yih, Wen-tau and Koh, Pang Wei and Hajishirzi, Hannaneh}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2411.14199}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/arxivdigestables-synthesizing-scientific-literature-into-tables-using-language-models-480.webp 480w,/assets/img/publication_preview/arxivdigestables-synthesizing-scientific-literature-into-tables-using-language-models-800.webp 800w,/assets/img/publication_preview/arxivdigestables-synthesizing-scientific-literature-into-tables-using-language-models-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/arxivdigestables-synthesizing-scientific-literature-into-tables-using-language-models.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="arxivdigestables-synthesizing-scientific-literature-into-tables-using-language-models.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Newman2024ArxivdigestablesSynthesizingScientific" class="col-sm-8"> <div class="title">ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models</div> <div class="author"> Benjamin Newman,¬†Yoonjoo Lee,¬†Aakanksha Naik,¬†Pao Siangliulue,¬†Raymond Fok,¬†Juho Kim,¬†Daniel S Weld, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Joseph Chee Chang, Kyle Lo' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">2 more authors</span> </div> <div class="periodical"> <em>In EMNLP</em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.22360" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/arxivdigestables-synthesizing-scientific-literature-into-tables-using-language-models.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>When conducting literature reviews, scientists often create literature review tables - tables whose rows are publications and whose columns constitute a schema, a set of aspects used to compare and contrast the papers. Can we automatically generate these tables using language models (LMs)? In this work, we introduce a framework that leverages LMs to perform this task by decomposing it into separate schema and value generation steps. To enable experimentation, we address two main challenges: First, we overcome a lack of high-quality datasets to benchmark table generation by curating and releasing arxivDIGESTables, a new dataset of 2,228 literature review tables extracted from ArXiv papers that synthesize a total of 7,542 research papers. Second, to support scalable evaluation of model generations against human-authored reference tables, we develop DecontextEval, an automatic evaluation method that aligns elements of tables with the same underlying aspects despite differing surface forms. Given these tools, we evaluate LMs‚Äô abilities to reconstruct reference tables, finding this task benefits from additional context to ground the generation (e.g. table captions, in-text references). Finally, through a human evaluation study we find that even when LMs fail to fully reconstruct a reference table, their generated novel aspects can still be useful.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Newman2024ArxivdigestablesSynthesizingScientific</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Newman, Benjamin and Lee, Yoonjoo and Naik, Aakanksha and Siangliulue, Pao and Fok, Raymond and Kim, Juho and Weld, Daniel S and Chang, Joseph Chee and Lo, Kyle}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{EMNLP}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2024.emnlp-main.538}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2-olmo-2-furious-480.webp 480w,/assets/img/publication_preview/2-olmo-2-furious-800.webp 800w,/assets/img/publication_preview/2-olmo-2-furious-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2-olmo-2-furious.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2-olmo-2-furious.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="OLMo2024Olmo" class="col-sm-8"> <div class="title">2 OLMo 2 Furious</div> <div class="author"> Team OLMo,¬†Pete Walsh,¬†Luca Soldaini,¬†Dirk Groeneveld,¬†<em>Kyle Lo</em>,¬†Shane Arora,¬†Akshita Bhagia, and <span class="more-authors" title="click to view 33 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '33 more authors' ? 'Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A Smith, Hannaneh Hajishirzi' : '33 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">33 more authors</span> </div> <div class="periodical"> <em>In COLM</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2501.00656" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2-olmo-2-furious.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from T√ºlu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly ‚Äì models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">OLMo2024Olmo</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{OLMo, Team and Walsh, Pete and Soldaini, Luca and Groeneveld, Dirk and Lo, Kyle and Arora, Shane and Bhagia, Akshita and Gu, Yuling and Huang, Shengyi and Jordan, Matt and Lambert, Nathan and Schwenk, Dustin and Tafjord, Oyvind and Anderson, Taira and Atkinson, David and Brahman, Faeze and Clark, Christopher and Dasigi, Pradeep and Dziri, Nouha and Guerquin, Michal and Ivison, Hamish and Koh, Pang Wei and Liu, Jiacheng and Malik, Saumya and Merrill, William and Miranda, Lester James V and Morrison, Jacob and Murray, Tyler and Nam, Crystal and Pyatkin, Valentina and Rangapur, Aman and Schmitz, Michael and Skjonsberg, Sam and Wadden, David and Wilhelm, Christopher and Wilson, Michael and Zettlemoyer, Luke and Farhadi, Ali and Smith, Noah A and Hajishirzi, Hannaneh}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{COLM}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{2 OLMo 2 Furious}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2501.00656}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/the-semantic-reader-project-augmenting-scholarly-documents-through-ai-powered-interactive-reading-interfaces-480.webp 480w,/assets/img/publication_preview/the-semantic-reader-project-augmenting-scholarly-documents-through-ai-powered-interactive-reading-interfaces-800.webp 800w,/assets/img/publication_preview/the-semantic-reader-project-augmenting-scholarly-documents-through-ai-powered-interactive-reading-interfaces-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/the-semantic-reader-project-augmenting-scholarly-documents-through-ai-powered-interactive-reading-interfaces.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="the-semantic-reader-project-augmenting-scholarly-documents-through-ai-powered-interactive-reading-interfaces.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3659096" class="col-sm-8"> <div class="title">The Semantic Reader Project: Augmenting Scholarly Documents Through AI-Powered Interactive Reading Interfaces</div> <div class="author"> <em>Kyle Lo</em>,¬†Joseph Chee Chang,¬†Andrew Head,¬†Jonathan Bragg,¬†Amy X. Zhang,¬†Cassidy Trier,¬†Chloe Anastasiades, and <span class="more-authors" title="click to view 48 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '48 more authors' ? 'Tal August, Russell Authur, Danielle Bragg, Erin Bransom, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Yen-Sung Chen, Evie Yu-Yen Cheng, Yvonne Chou, Doug Downey, Rob Evans, Raymond Fok, Fangzhou Hu, Regan Huff, Dongyeop Kang, Tae Soo Kim, Rodney Kinney, Aniket Kittur, Hyeonsu B. Kang, Egor Klevak, Bailey Kuehl, Michael J. Langan, Matt Latzke, Jaron Lochner, Kelsey MacMillan, Eric Marsh, Tyler Murray, Aakanksha Naik, Ngoc-Uyen Nguyen, Srishti Palani, Soya Park, Caroline Paulic, Napol Rachatasumrit, Smita Rao, Paul Sayre, Zejiang Shen, Pao Siangliulue, Luca Soldaini, Huy Tran, Madeleine Zuylen, Lucy Lu Wang, Christopher Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Marti A. Hearst, Daniel S. Weld' : '48 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">48 more authors</span> </div> <div class="periodical"> <em>Communications of the ACM</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3659096" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2303.14334" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/the-semantic-reader-project-augmenting-scholarly-documents-through-ai-powered-interactive-reading-interfaces.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Scholarly publications are key to the transfer of knowledge from scholars to others. However, research papers are information-dense, and as the volume of the scientific literature grows, the greater the need for new technology to support scholars. In contrast to the process of finding papers, which has been transformed by Internet technology, the experience of reading research papers has changed little in decades. For instance, the PDF format for sharing papers remains widely used due to its portability but has significant downsides, inter alia, static content and poor accessibility for low-vision readers. This paper explores the question ‚ÄúCan recent advances in AI and HCI power intelligent, interactive, and accessible reading interfaces‚Äîeven for legacy PDFs?‚Äù We describe the Semantic Reader Project, a collaborative effort across multiple institutions to explore automatic creation of dynamic reading interfaces for research papers. Through this project, we‚Äôve developed a collection of novel reading interfaces and evaluated them with study participants and real-world users to show improved reading experiences for scholars. We‚Äôve also released a production research paper reading interface that will continuously incorporate novel features from our research as they mature. We structure this paper around five key opportunities for AI assistance in scholarly reading ‚Äîdiscovery, efficiency, comprehension, synthesis, and accessibility‚Äîand present an overview of our progress and discuss remaining open challenges.Augmenting scholarly documents through AI-powered interactive reading interfaces.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1145/3659096</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lo, Kyle and Chang, Joseph Chee and Head, Andrew and Bragg, Jonathan and Zhang, Amy X. and Trier, Cassidy and Anastasiades, Chloe and August, Tal and Authur, Russell and Bragg, Danielle and Bransom, Erin and Cachola, Isabel and Candra, Stefan and Chandrasekhar, Yoganand and Chen, Yen-Sung and Cheng, Evie Yu-Yen and Chou, Yvonne and Downey, Doug and Evans, Rob and Fok, Raymond and Hu, Fangzhou and Huff, Regan and Kang, Dongyeop and Kim, Tae Soo and Kinney, Rodney and Kittur, Aniket and Kang, Hyeonsu B. and Klevak, Egor and Kuehl, Bailey and Langan, Michael J. and Latzke, Matt and Lochner, Jaron and MacMillan, Kelsey and Marsh, Eric and Murray, Tyler and Naik, Aakanksha and Nguyen, Ngoc-Uyen and Palani, Srishti and Park, Soya and Paulic, Caroline and Rachatasumrit, Napol and Rao, Smita and Sayre, Paul and Shen, Zejiang and Siangliulue, Pao and Soldaini, Luca and Tran, Huy and van Zuylen, Madeleine and Wang, Lucy Lu and Wilhelm, Christopher and Wu, Caroline and Yang, Jiangjiang and Zamarron, Angele and Hearst, Marti A. and Weld, Daniel S.}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3659096}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Communications of the ACM}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Semantic Reader Project: Augmenting Scholarly Documents Through AI-Powered Interactive Reading Interfaces}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3659096}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{67}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/routerretriever-exploring-the-benefits-of-routing-over-multiple-expert-embedding-models-480.webp 480w,/assets/img/publication_preview/routerretriever-exploring-the-benefits-of-routing-over-multiple-expert-embedding-models-800.webp 800w,/assets/img/publication_preview/routerretriever-exploring-the-benefits-of-routing-over-multiple-expert-embedding-models-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/routerretriever-exploring-the-benefits-of-routing-over-multiple-expert-embedding-models.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="routerretriever-exploring-the-benefits-of-routing-over-multiple-expert-embedding-models.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Lee2024RouterRetrieverET" class="col-sm-8"> <div class="title">RouterRetriever: Exploring the Benefits of Routing over Multiple Expert Embedding Models</div> <div class="author"> Hyunji Lee,¬†Luca Soldaini,¬†Arman Cohan,¬†Minjoon Seo,¬†and¬†<em>Kyle Lo</em> </div> <div class="periodical"> <em>ArXiv</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2409.02685" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/routerretriever-exploring-the-benefits-of-routing-over-multiple-expert-embedding-models.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Information retrieval methods often rely on a single embedding model trained on large, general-domain datasets like MSMARCO. While this approach can produce a retriever with reasonable overall performance, models trained on domain-specific data often yield better results within their respective domains. While prior work in information retrieval has tackled this through multi-task training, the topic of combining multiple domain-specific expert retrievers remains unexplored, despite its popularity in language model generation. In this work, we introduce RouterRetriever, a retrieval model that leverages multiple domain-specific experts along with a routing mechanism to select the most appropriate expert for each query. It is lightweight and allows easy addition or removal of experts without additional training. Evaluation on the BEIR benchmark demonstrates that RouterRetriever outperforms both MSMARCO-trained (+2.1 absolute nDCG@10) and multi-task trained (+3.2) models. This is achieved by employing our routing mechanism, which surpasses other routing techniques (+1.8 on average) commonly used in language modeling. Furthermore, the benefit generalizes well to other datasets, even in the absence of a specific expert on the dataset. To our knowledge, RouterRetriever is the first work to demonstrate the advantages of using multiple domain-specific expert embedding models with effective routing over a single, general-purpose embedding model in retrieval tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Lee2024RouterRetrieverET</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lee, Hyunji and Soldaini, Luca and Cohan, Arman and Seo, Minjoon and Lo, Kyle}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RouterRetriever: Exploring the Benefits of Routing over Multiple Expert Embedding Models}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2409.02685}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{2409.02685}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/olmoe-open-mixture-of-experts-language-models-480.webp 480w,/assets/img/publication_preview/olmoe-open-mixture-of-experts-language-models-800.webp 800w,/assets/img/publication_preview/olmoe-open-mixture-of-experts-language-models-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/olmoe-open-mixture-of-experts-language-models.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="olmoe-open-mixture-of-experts-language-models.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muennighoff2024OLMoEOM" class="col-sm-8"> <div class="title">OLMoE: Open Mixture-of-Experts Language Models</div> <div class="author"> Niklas Muennighoff,¬†Luca Soldaini,¬†Dirk Groeneveld,¬†<em>Kyle Lo</em>,¬†Jacob Daniel Morrison,¬†Sewon Min,¬†Weijia Shi, and <span class="more-authors" title="click to view 17 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '17 more authors' ? 'Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hanna Hajishirzi' : '17 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">17 more authors</span> </div> <div class="periodical"> <em>ArXiv</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2409.02060" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/olmoe-open-mixture-of-experts-language-models.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Muennighoff2024OLMoEOM</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Muennighoff, Niklas and Soldaini, Luca and Groeneveld, Dirk and Lo, Kyle and Morrison, Jacob Daniel and Min, Sewon and Shi, Weijia and Walsh, Pete and Tafjord, Oyvind and Lambert, Nathan and Gu, Yuling and Arora, Shane and Bhagia, Akshita and Schwenk, Dustin and Wadden, David and Wettig, Alexander and Hui, Binyuan and Dettmers, Tim and Kiela, Douwe and Farhadi, Ali and Smith, Noah A. and Koh, Pang Wei and Singh, Amanpreet and Hajishirzi, Hanna}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{OLMoE: Open Mixture-of-Experts Language Models}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2409.02060}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/olmo-accelerating-the-science-of-language-models-480.webp 480w,/assets/img/publication_preview/olmo-accelerating-the-science-of-language-models-800.webp 800w,/assets/img/publication_preview/olmo-accelerating-the-science-of-language-models-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/olmo-accelerating-the-science-of-language-models.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="olmo-accelerating-the-science-of-language-models.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="groeneveld-etal-2024-olmo" class="col-sm-8"> <div class="title">OLMo: Accelerating the Science of Language Models</div> <div class="author"> Dirk Groeneveld,¬†Iz Beltagy,¬†Evan Walsh,¬†Akshita Bhagia,¬†Rodney Kinney,¬†Oyvind Tafjord,¬†Ananya Jha, and <span class="more-authors" title="click to view 36 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '36 more authors' ? 'Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, William Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah Smith, Hannaneh Hajishirzi' : '36 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">36 more authors</span> </div> <div class="periodical"> <em>In ACL</em>, Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <div>üèÜ <strong>Best Paper Award</strong> üèÜ</div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2024.acl-long.841" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2402.00838" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://aclanthology.org/2024.acl-long.841" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/olmo-accelerating-the-science-of-language-models.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Award</p> </div> <div class="abstract hidden"> <p>Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">groeneveld-etal-2024-olmo</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Groeneveld, Dirk and Beltagy, Iz and Walsh, Evan and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, William and Strubell, Emma and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah and Hajishirzi, Hannaneh}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACL}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2024.acl-long.841}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{OLM}o: Accelerating the Science of Language Models}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2024.acl-long.841}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/evaluating-language-model-math-reasoning-via-grounding-in-educational-curricula-480.webp 480w,/assets/img/publication_preview/evaluating-language-model-math-reasoning-via-grounding-in-educational-curricula-800.webp 800w,/assets/img/publication_preview/evaluating-language-model-math-reasoning-via-grounding-in-educational-curricula-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/evaluating-language-model-math-reasoning-via-grounding-in-educational-curricula.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="evaluating-language-model-math-reasoning-via-grounding-in-educational-curricula.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Lucy2024EvaluatingLM" class="col-sm-8"> <div class="title">MathFish : Evaluating Language Model Math Reasoning via Grounding in Educational Curricula</div> <div class="author"> Li Lucy,¬†Tal August,¬†Rose E. Wang,¬†Luca Soldaini,¬†Courtney Allison,¬†and¬†<em>Kyle Lo</em> </div> <div class="periodical"> <em>ArXiv</em>, Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2408.04226" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/evaluating-language-model-math-reasoning-via-grounding-in-educational-curricula.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>To ensure that math curriculum is grade-appropriate and aligns with critical skills or concepts in accordance with educational standards, pedagogical experts can spend months carefully reviewing published math problems. Drawing inspiration from this process, our work presents a novel angle for evaluating language models‚Äô (LMs) mathematical abilities, by investigating whether they can discern skills and concepts enabled by math content. We contribute two datasets: one consisting of 385 fine-grained descriptions of K-12 math skills and concepts, or standards, from Achieve the Core (ATC), and another of 9.9K math problems labeled with these standards (MathFish). We develop two tasks for evaluating LMs‚Äô abilities to assess math problems: (1) verifying whether a problem aligns with a given standard, and (2) tagging a problem with all aligned standards. Working with experienced teachers, we find that LMs struggle to tag and verify standards linked to problems, and instead predict labels that are close to ground truth, but differ in subtle ways. We also show that LMs often generate problems that do not fully align with standards described in prompts, suggesting the need for careful scrutiny on use cases involving LMs for generating curricular materials. Finally, we categorize problems in GSM8k using math standards, allowing us to better understand why some problems are more difficult to solve for models than others.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Lucy2024EvaluatingLM</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lucy, Li and August, Tal and Wang, Rose E. and Soldaini, Luca and Allison, Courtney and Lo, Kyle}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MathFish : Evaluating Language Model Math Reasoning via Grounding in Educational Curricula}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2408.04226}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dolma-an-open-corpus-of-three-trillion-tokens-for-language-model-pretraining-research-480.webp 480w,/assets/img/publication_preview/dolma-an-open-corpus-of-three-trillion-tokens-for-language-model-pretraining-research-800.webp 800w,/assets/img/publication_preview/dolma-an-open-corpus-of-three-trillion-tokens-for-language-model-pretraining-research-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/dolma-an-open-corpus-of-three-trillion-tokens-for-language-model-pretraining-research.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dolma-an-open-corpus-of-three-trillion-tokens-for-language-model-pretraining-research.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="soldaini-etal-2024-dolma" class="col-sm-8"> <div class="title">Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research</div> <div class="author"> Luca Soldaini,¬†Rodney Kinney,¬†Akshita Bhagia,¬†Dustin Schwenk,¬†David Atkinson,¬†Russell Authur,¬†Ben Bogin, and <span class="more-authors" title="click to view 29 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '29 more authors' ? 'Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, Kyle Lo' : '29 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">29 more authors</span> </div> <div class="periodical"> <em>In ACL</em>, Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <div>üèÜ <strong>Best Paper Award</strong> üèÜ</div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2024.acl-long.840" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2402.00159" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://aclanthology.org/2024.acl-long.840" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/dolma-an-open-corpus-of-three-trillion-tokens-for-language-model-pretraining-research.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Award</p> </div> <div class="abstract hidden"> <p>Information about pretraining corpora used to train the current best-performing language models is seldom discussed: commercial models rarely detail their data, and even open models are often released without accompanying training data or recipes to reproduce them. As a result, it is challenging to conduct and advance scientific research on language modeling, such as understanding how training data impacts model capabilities and limitations. To facilitate scientific research on language model pretraining, we curate and release Dolma, a three-trillion-token English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. We extensively document Dolma, including its design principles, details about its construction, and a summary of its contents. We present analyses and experimental results on intermediate states of Dolma to share what we have learned about important data curation practices. Finally, we open-source our data curation toolkit to enable reproduction of our work as well as support further research in large-scale data curation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">soldaini-etal-2024-dolma</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and Schwenk, Dustin and Atkinson, David and Authur, Russell and Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar, Yanai and Hofmann, Valentin and Jha, Ananya and Kumar, Sachin and Lucy, Li and Lyu, Xinxi and Lambert, Nathan and Magnusson, Ian and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew and Ravichander, Abhilasha and Richardson, Kyle and Shen, Zejiang and Strubell, Emma and Subramani, Nishant and Tafjord, Oyvind and Walsh, Evan and Zettlemoyer, Luke and Smith, Noah and Hajishirzi, Hannaneh and Beltagy, Iz and Groeneveld, Dirk and Dodge, Jesse and Lo, Kyle}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACL}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2024.acl-long.840}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2024.acl-long.840}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/kiwi-a-dataset-of-knowledge-intensive-writing-instructions-for-answering-research-questions-480.webp 480w,/assets/img/publication_preview/kiwi-a-dataset-of-knowledge-intensive-writing-instructions-for-answering-research-questions-800.webp 800w,/assets/img/publication_preview/kiwi-a-dataset-of-knowledge-intensive-writing-instructions-for-answering-research-questions-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/kiwi-a-dataset-of-knowledge-intensive-writing-instructions-for-answering-research-questions.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kiwi-a-dataset-of-knowledge-intensive-writing-instructions-for-answering-research-questions.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xu-etal-2024-kiwi" class="col-sm-8"> <div class="title">KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions</div> <div class="author"> Fangyuan Xu,¬†<em>Kyle Lo</em>,¬†Luca Soldaini,¬†Bailey Kuehl,¬†Eunsol Choi,¬†and¬†David Wadden </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics ACL 2024</em>, Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2024.findings-acl.770" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2403.03866" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://aclanthology.org/2024.findings-acl.770" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/kiwi-a-dataset-of-knowledge-intensive-writing-instructions-for-answering-research-questions.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) adapted to follow user instructions are now widely deployed as conversational agents. In this work, we examine one increasingly common instruction-following task: providing writing assistance to compose a long-form answer. To evaluate the capabilities of current LLMs on this task, we construct KIWI, a dataset of knowledge-intensive writing instructions in the scientific domain. Given a research question, an initial model-generated answer and a set of relevant papers, an expert annotator iteratively issues instructions for the model to revise and improve its answer. We collect 1,260 interaction turns from 234 interaction sessions with three state-of-the-art LLMs. Each turn includes a user instruction, a model response, and a human evaluation of the model response. Through a detailed analysis of the collected responses, we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits. Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement. Our findings indicate that KIWI will be a valuable resource to measure progress and improve LLMs‚Äô instruction-following capabilities for knowledge intensive writing tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2024-kiwi</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Fangyuan and Lo, Kyle and Soldaini, Luca and Kuehl, Bailey and Choi, Eunsol and Wadden, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of the Association for Computational Linguistics ACL 2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2024.findings-acl.770}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{KIWI}: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2024.findings-acl.770}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/datacomp-lm-in-search-of-the-next-generation-of-training-sets-for-language-models-480.webp 480w,/assets/img/publication_preview/datacomp-lm-in-search-of-the-next-generation-of-training-sets-for-language-models-800.webp 800w,/assets/img/publication_preview/datacomp-lm-in-search-of-the-next-generation-of-training-sets-for-language-models-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/datacomp-lm-in-search-of-the-next-generation-of-training-sets-for-language-models.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="datacomp-lm-in-search-of-the-next-generation-of-training-sets-for-language-models.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Gadre2023DataCompIS" class="col-sm-8"> <div class="title">DataComp-LM: In search of the next generation of training sets for language models</div> <div class="author"> Jeffrey Li,¬†Alex Fang,¬†Georgios Smyrnis,¬†Maor Ivgi,¬†Matt Jordan,¬†Samir Gadre,¬†Hritik Bansal, and <span class="more-authors" title="click to view 52 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '52 more authors' ? 'Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, Vaishaal Shankar' : '52 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">52 more authors</span> </div> <div class="periodical"> <em>ArXiv</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.11794" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/datacomp-lm-in-search-of-the-next-generation-of-training-sets-for-language-models.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We introduce DataComp for Language Models (DCLM), a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing at model scales ranging from 412M to 7B parameters. As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set. The resulting dataset, DCLM-Baseline enables training a 7B parameter language model from scratch to 64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-Baseline represents a 6.6 percentage point improvement on MMLU while being trained with 40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and Llama 3 8B on MMLU (63% &amp; 66%), and performs similarly on an average of 53 natural language understanding tasks while being trained with 6.6x less compute than Llama 3 8B. Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Gadre2023DataCompIS</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Jeffrey and Fang, Alex and Smyrnis, Georgios and Ivgi, Maor and Jordan, Matt and Gadre, Samir and Bansal, Hritik and Guha, Etash and Keh, Sedrick and Arora, Kushal and Garg, Saurabh and Xin, Rui and Muennighoff, Niklas and Heckel, Reinhard and Mercat, Jean and Chen, Mayee and Gururangan, Suchin and Wortsman, Mitchell and Albalak, Alon and Bitton, Yonatan and Nezhurina, Marianna and Abbas, Amro and Hsieh, Cheng-Yu and Ghosh, Dhruba and Gardner, Josh and Kilian, Maciej and Zhang, Hanlin and Shao, Rulin and Pratt, Sarah and Sanyal, Sunny and Ilharco, Gabriel and Daras, Giannis and Marathe, Kalyani and Gokaslan, Aaron and Zhang, Jieyu and Chandu, Khyathi and Nguyen, Thao and Vasiljevic, Igor and Kakade, Sham and Song, Shuran and Sanghavi, Sujay and Faghri, Fartash and Oh, Sewoong and Zettlemoyer, Luke and Lo, Kyle and El-Nouby, Alaaeldin and Pouransari, Hadi and Toshev, Alexander and Wang, Stephanie and Groeneveld, Dirk and Soldaini, Luca and Koh, Pang Wei and Jitsev, Jenia and Kollar, Thomas and Dimakis, Alexandros G. and Carmon, Yair and Dave, Achal and Schmidt, Ludwig and Shankar, Vaishaal}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DataComp-LM: In search of the next generation of training sets for language models}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2406.11794}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/one-thousand-and-one-pairs-a-novel-challenge-for-long-context-language-models-480.webp 480w,/assets/img/publication_preview/one-thousand-and-one-pairs-a-novel-challenge-for-long-context-language-models-800.webp 800w,/assets/img/publication_preview/one-thousand-and-one-pairs-a-novel-challenge-for-long-context-language-models-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/one-thousand-and-one-pairs-a-novel-challenge-for-long-context-language-models.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="one-thousand-and-one-pairs-a-novel-challenge-for-long-context-language-models.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Karpinska2024OneTA" class="col-sm-8"> <div class="title">One Thousand and One Pairs: A "novel" challenge for long-context language models</div> <div class="author"> Marzena Karpinska,¬†Katherine Thai,¬†<em>Kyle Lo</em>,¬†Tanya Goyal,¬†and¬†Mohit Iyyer </div> <div class="periodical"> <em>ArXiv</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.16264" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/one-thousand-and-one-pairs-a-novel-challenge-for-long-context-language-models.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Synthetic long-context LLM benchmarks (e.g., "needle-in-the-haystack") test only surface-level retrieval capabilities, but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? We address this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, our annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. Our experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that we evaluate: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest accuracy at 55.8%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Karpinska2024OneTA</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Karpinska, Marzena and Thai, Katherine and Lo, Kyle and Goyal, Tanya and Iyyer, Mohit}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{One Thousand and One Pairs: A "novel" challenge for long-context language models}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2406.16264}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sciriff-a-resource-to-enhance-language-model-instruction-following-over-scientific-literature-480.webp 480w,/assets/img/publication_preview/sciriff-a-resource-to-enhance-language-model-instruction-following-over-scientific-literature-800.webp 800w,/assets/img/publication_preview/sciriff-a-resource-to-enhance-language-model-instruction-following-over-scientific-literature-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/sciriff-a-resource-to-enhance-language-model-instruction-following-over-scientific-literature.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sciriff-a-resource-to-enhance-language-model-instruction-following-over-scientific-literature.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Wadden2024SciRIFFAR" class="col-sm-8"> <div class="title">SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature</div> <div class="author"> David Wadden,¬†Kejian Shi,¬†Jacob Daniel Morrison,¬†Aakanksha Naik,¬†Shruti Singh,¬†Nitzan Barzilay,¬†<em>Kyle Lo</em>, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Tom Hope, Luca Soldaini, Shannon Zejiang Shen, Doug Downey, Hanna Hajishirzi, Arman Cohan' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">6 more authors</span> </div> <div class="periodical"> <em>ArXiv</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.07835" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/sciriff-a-resource-to-enhance-language-model-instruction-following-over-scientific-literature.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We present SciRIFF (Scientific Resource for Instruction-Following and Finetuning), a dataset of 137K instruction-following demonstrations for 54 tasks covering five essential scientific literature understanding capabilities: information extraction, summarization, question answering, claim verification, and classification. SciRIFF demonstrations are notable for their long input contexts, detailed task specifications, and complex structured outputs. While instruction-following resources are available in specific domains such as clinical medicine and chemistry, SciRIFF is the first dataset focused on extracting and synthesizing information from research literature across a wide range of scientific fields. To demonstrate the utility of SciRIFF, we develop a sample-efficient strategy to adapt a general instruction-following model for science by performing additional finetuning on a mix of general-domain and SciRIFF demonstrations. In evaluations on nine held-out scientific tasks, our model ‚Äì called SciTulu ‚Äì improves over a strong LLM baseline by 28.1% and 6.5% at the 7B and 70B scales respectively, while maintaining general instruction-following performance within 2% of the baseline. We are optimistic that SciRIFF will facilitate the development and evaluation of LLMs to help researchers navigate the ever-growing body of scientific literature. We release our dataset, model checkpoints, and data processing and evaluation code to enable further research.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Wadden2024SciRIFFAR</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wadden, David and Shi, Kejian and Morrison, Jacob Daniel and Naik, Aakanksha and Singh, Shruti and Barzilay, Nitzan and Lo, Kyle and Hope, Tom and Soldaini, Luca and Shen, Shannon Zejiang and Downey, Doug and Hajishirzi, Hanna and Cohan, Arman}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2406.07835}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/know-your-audience-the-benefits-and-pitfalls-of-generating-plain-language-summaries-beyond-the-general-audience-480.webp 480w,/assets/img/publication_preview/know-your-audience-the-benefits-and-pitfalls-of-generating-plain-language-summaries-beyond-the-general-audience-800.webp 800w,/assets/img/publication_preview/know-your-audience-the-benefits-and-pitfalls-of-generating-plain-language-summaries-beyond-the-general-audience-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/know-your-audience-the-benefits-and-pitfalls-of-generating-plain-language-summaries-beyond-the-general-audience.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="know-your-audience-the-benefits-and-pitfalls-of-generating-plain-language-summaries-beyond-the-general-audience.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3613904.3642289" class="col-sm-8"> <div class="title">Know Your Audience: The benefits and pitfalls of generating plain language summaries beyond the "general" audience</div> <div class="author"> Tal August,¬†<em>Kyle Lo</em>,¬†Noah A. Smith,¬†and¬†Katharina Reinecke </div> <div class="periodical"> <em>In CHI</em>, Honolulu, HI, USA, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3613904.3642289" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://dl.acm.org/doi/abs/10.1145/3613904.3642289" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/know-your-audience-the-benefits-and-pitfalls-of-generating-plain-language-summaries-beyond-the-general-audience.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Language models (LMs) show promise as tools for communicating science to the general public by simplifying and summarizing complex language. Because models can be prompted to generate text for a specific audience (e.g., college-educated adults), LMs might be used to create multiple versions of plain language summaries for people with different familiarities of scientific topics. However, it is not clear what the benefits and pitfalls of adaptive plain language are. When is simplifying necessary, what are the costs in doing so, and do these costs differ for readers with different background knowledge? Through three within-subjects studies in which we surface summaries for different envisioned audiences to participants of different backgrounds, we found that while simpler text led to the best reading experience for readers with little to no familiarity in a topic, high familiarity readers tended to ignore certain details in overly plain summaries (e.g., study limitations). Our work provides methods and guidance on ways of adapting plain language summaries beyond the single ‚Äúgeneral‚Äù audience.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3613904.3642289</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{August, Tal and Lo, Kyle and Smith, Noah A. and Reinecke, Katharina}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CHI}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3613904.3642289}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{26}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Know Your Audience: The benefits and pitfalls of generating plain language summaries beyond the "general" audience}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3613904.3642289}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/followir-evaluating-and-teaching-information-retrieval-models-to-follow-instructions-480.webp 480w,/assets/img/publication_preview/followir-evaluating-and-teaching-information-retrieval-models-to-follow-instructions-800.webp 800w,/assets/img/publication_preview/followir-evaluating-and-teaching-information-retrieval-models-to-follow-instructions-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/followir-evaluating-and-teaching-information-retrieval-models-to-follow-instructions.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="followir-evaluating-and-teaching-information-retrieval-models-to-follow-instructions.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Weller2024FollowIREA" class="col-sm-8"> <div class="title">FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions</div> <div class="author"> Orion Weller,¬†Benjamin Chang,¬†Sean MacAvaney,¬†<em>Kyle Lo</em>,¬†Arman Cohan,¬†Benjamin Van Durme,¬†Dawn Lawrie, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Luca Soldaini' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">1 more author</span> </div> <div class="periodical"> <em>ArXiv</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.15246" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/followir-evaluating-and-teaching-information-retrieval-models-to-follow-instructions.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Modern Language Models (LMs) are capable of following long and complex instructions that enable a large and diverse set of user requests. While Information Retrieval (IR) models use these LMs as the backbone of their architectures, virtually none of them allow users to provide detailed instructions alongside queries, thus limiting their ability to satisfy complex information needs. In this work, we study the use of instructions in IR systems. First, we introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR repurposes detailed instructions ‚Äì also known as narratives ‚Äì developed for professional assessors to evaluate retrieval systems. In particular, we build our benchmark from three collections curated for shared tasks at the Text REtrieval Conference (TREC). These collections contains hundreds to thousands of labeled documents per query, making them suitable for our exploration. Through this process, we can measure how well IR models follow instructions, through a new pairwise evaluation framework. Our results indicate that existing retrieval models fail to correctly use instructions, using them for basic keywords and struggling to understand long-form information. However, we show that it is possible for IR models to learn to follow complex instructions: our new FollowIR-7B model has significant improvements after fine-tuning on our training set.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Weller2024FollowIREA</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Weller, Orion and Chang, Benjamin and MacAvaney, Sean and Lo, Kyle and Cohan, Arman and Durme, Benjamin Van and Lawrie, Dawn and Soldaini, Luca}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2403.15246}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fables-evaluating-faithfulness-and-content-selection-in-book-length-summarization-480.webp 480w,/assets/img/publication_preview/fables-evaluating-faithfulness-and-content-selection-in-book-length-summarization-800.webp 800w,/assets/img/publication_preview/fables-evaluating-faithfulness-and-content-selection-in-book-length-summarization-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/fables-evaluating-faithfulness-and-content-selection-in-book-length-summarization.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fables-evaluating-faithfulness-and-content-selection-in-book-length-summarization.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim2024FABLESEF" class="col-sm-8"> <div class="title">FABLES: Evaluating faithfulness and content selection in book-length summarization</div> <div class="author"> Yekyung Kim,¬†Yapei Chang,¬†Marzena Karpinska,¬†Aparna Garimella,¬†Varun Manjunatha,¬†<em>Kyle Lo</em>,¬†Tanya Goyal, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Mohit Iyyer' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">1 more author</span> </div> <div class="periodical"> <em>ArXiv</em>, Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2404.01261" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/fables-evaluating-faithfulness-and-content-selection-in-book-length-summarization.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>While long-context large language models (LLMs) can technically summarize book-length documents (&gt;100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Kim2024FABLESEF</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kim, Yekyung and Chang, Yapei and Karpinska, Marzena and Garimella, Aparna and Manjunatha, Varun and Lo, Kyle and Goyal, Tanya and Iyyer, Mohit}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FABLES: Evaluating faithfulness and content selection in book-length summarization}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2404.01261}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/accelerating-scientific-paper-skimming-with-augmented-intelligence-through-customizable-faceted-highlights-480.webp 480w,/assets/img/publication_preview/accelerating-scientific-paper-skimming-with-augmented-intelligence-through-customizable-faceted-highlights-800.webp 800w,/assets/img/publication_preview/accelerating-scientific-paper-skimming-with-augmented-intelligence-through-customizable-faceted-highlights-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/accelerating-scientific-paper-skimming-with-augmented-intelligence-through-customizable-faceted-highlights.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="accelerating-scientific-paper-skimming-with-augmented-intelligence-through-customizable-faceted-highlights.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3665648" class="col-sm-8"> <div class="title">Accelerating Scientific Paper Skimming with Augmented Intelligence Through Customizable Faceted Highlights</div> <div class="author"> Raymond Fok,¬†Luca Soldaini,¬†Cassidy Trier,¬†Erin Bransom,¬†Kelsey MacMillan,¬†Evie Cheng,¬†Hita Kambhamettu, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Jonathan Bragg, Kyle Lo, Marti A. Hearst, Andrew Head, Daniel S. Weld' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">5 more authors</span> </div> <div class="periodical"> <em>ACM Transactions on Interactive Intelligent Systems</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3665648" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://dl.acm.org/doi/abs/10.1145/3665648" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/accelerating-scientific-paper-skimming-with-augmented-intelligence-through-customizable-faceted-highlights.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Scholars need to keep up with an exponentially increasing flood of scientific papers. To aid this challenge, we introduce Scim, a novel intelligent interface that helps scholars skim papers to rapidly review and gain a cursory understanding of its contents. Scim supports the skimming process by highlighting salient content within a paper, directing a scholar‚Äôs attention. These automatically-extracted highlights are faceted by content type, evenly distributed across a paper, and have a density configurable by scholars. We evaluate Scim with an in-lab usability study and a longitudinal diary study, revealing how its highlights facilitate the more efficient construction of a conceptualization of a paper. Finally, we describe the process of scaling highlights from their conception within Scim, a research prototype, to production on over 521,000 papers within the Semantic Reader, a publicly-available augmented reading interface for scientific papers. We conclude by discussing design considerations and tensions for the design of future skimming tools with augmented intelligence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1145/3665648</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fok, Raymond and Soldaini, Luca and Trier, Cassidy and Bransom, Erin and MacMillan, Kelsey and Cheng, Evie and Kambhamettu, Hita and Bragg, Jonathan and Lo, Kyle and Hearst, Marti A. and Head, Andrew and Weld, Daniel S.}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3665648}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Interactive Intelligent Systems}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Accelerating Scientific Paper Skimming with Augmented Intelligence Through Customizable Faceted Highlights}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3665648}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/infolossqa-characterizing-and-recovering-information-loss-in-text-simplification-480.webp 480w,/assets/img/publication_preview/infolossqa-characterizing-and-recovering-information-loss-in-text-simplification-800.webp 800w,/assets/img/publication_preview/infolossqa-characterizing-and-recovering-information-loss-in-text-simplification-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/infolossqa-characterizing-and-recovering-information-loss-in-text-simplification.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="infolossqa-characterizing-and-recovering-information-loss-in-text-simplification.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Trienes2024InfoLossQACA" class="col-sm-8"> <div class="title">InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification</div> <div class="author"> Jan Trienes,¬†Sebastian Antony Joseph,¬†Jorg Schlotterer,¬†Christin Seifert,¬†<em>Kyle Lo</em>,¬†Wei Xu,¬†Byron C. Wallace, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Junyi Jessy Li' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">1 more author</span> </div> <div class="periodical"> <em>ArXiv</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2401.16475" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/infolossqa-characterizing-and-recovering-information-loss-in-text-simplification.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Text simplification aims to make technical texts more accessible to laypeople but often results in deletion of information and vagueness. This work proposes InfoLossQA, a framework to characterize and recover simplification-induced information loss in form of question-and-answer (QA) pairs. Building on the theory of Question Under Discussion, the QA pairs are designed to help readers deepen their knowledge of a text. We conduct a range of experiments with this framework. First, we collect a dataset of 1,000 linguist-curated QA pairs derived from 104 LLM simplifications of scientific abstracts of medical studies. Our analyses of this data reveal that information loss occurs frequently, and that the QA pairs give a high-level overview of what information was lost. Second, we devise two methods for this task: end-to-end prompting of open-source and commercial language models, and a natural language inference pipeline. With a novel evaluation framework considering the correctness of QA pairs and their linguistic suitability, our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Trienes2024InfoLossQACA</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Trienes, Jan and Joseph, Sebastian Antony and Schlotterer, Jorg and Seifert, Christin and Lo, Kyle and Xu, Wei and Wallace, Byron C. and Li, Junyi Jessy}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2401.16475}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/open-domain-multi-document-summarization-a-comprehensive-study-of-model-brittleness-under-retrieval-480.webp 480w,/assets/img/publication_preview/open-domain-multi-document-summarization-a-comprehensive-study-of-model-brittleness-under-retrieval-800.webp 800w,/assets/img/publication_preview/open-domain-multi-document-summarization-a-comprehensive-study-of-model-brittleness-under-retrieval-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/open-domain-multi-document-summarization-a-comprehensive-study-of-model-brittleness-under-retrieval.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="open-domain-multi-document-summarization-a-comprehensive-study-of-model-brittleness-under-retrieval.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="giorgi-etal-2023-open" class="col-sm-8"> <div class="title">Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval</div> <div class="author"> John Giorgi,¬†Luca Soldaini,¬†Bo Wang,¬†Gary Bader,¬†<em>Kyle Lo</em>,¬†Lucy Wang,¬†and¬†Arman Cohan </div> <div class="periodical"> <em>In Findings of EMNLP</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2023.findings-emnlp.549" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2212.10526" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://aclanthology.org/2023.findings-emnlp.549" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/open-domain-multi-document-summarization-a-comprehensive-study-of-model-brittleness-under-retrieval.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Multi-document summarization (MDS) assumes a set of topic-related documents are provided as input. In practice, this document set is not always available; it would need to be retrieved given an information need, i.e. a question or topic statement, a setting we dub ‚Äúopen-domain‚Äô MDS. We study this more challenging setting by formalizing the task and bootstrapping it using existing datasets, retrievers and summarizers. Via extensive automatic and human evaluation, we determine: (1) state-of-the-art summarizers suffer large reductions in performance when applied to open-domain MDS, (2) additional training in the open-domain setting can reduce this sensitivity to imperfect retrieval, and (3) summarizers are insensitive to the retrieval of duplicate documents and the order of retrieved documents, but highly sensitive to other errors, like the retrieval of irrelevant documents. Based on our results, we provide practical guidelines to enable future work on open-domain MDS, e.g. how to choose the number of retrieved documents to summarize. Our results suggest that new retrieval and summarization methods and annotated resources for training and evaluation are necessary for further progress in the open-domain setting.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">giorgi-etal-2023-open</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Giorgi, John and Soldaini, Luca and Wang, Bo and Bader, Gary and Lo, Kyle and Wang, Lucy and Cohan, Arman}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of EMNLP}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.findings-emnlp.549}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.findings-emnlp.549}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/decomposing-complex-queries-for-tip-of-the-tongue-retrieval-480.webp 480w,/assets/img/publication_preview/decomposing-complex-queries-for-tip-of-the-tongue-retrieval-800.webp 800w,/assets/img/publication_preview/decomposing-complex-queries-for-tip-of-the-tongue-retrieval-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/decomposing-complex-queries-for-tip-of-the-tongue-retrieval.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="decomposing-complex-queries-for-tip-of-the-tongue-retrieval.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lin-etal-2023-decomposing" class="col-sm-8"> <div class="title">Decomposing Complex Queries for Tip-of-the-tongue Retrieval</div> <div class="author"> Kevin Lin,¬†<em>Kyle Lo</em>,¬†Joseph Gonzalez,¬†and¬†Dan Klein </div> <div class="periodical"> <em>In Findings of EMNLP</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2023.findings-emnlp.367" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2305.15053" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://aclanthology.org/2023.findings-emnlp.367" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/decomposing-complex-queries-for-tip-of-the-tongue-retrieval.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>When re-finding items, users who forget or are uncertain about identifying details often rely on creative strategies for expressing their information needs‚Äîcomplex queries that describe content elements (e.g., book characters or events), information beyond the document text (e.g., descriptions of book covers), or personal context (e.g., when they read a book). Standard retrieval models that rely on lexical or semantic overlap between query and document text are challenged in such retrieval settings, known as tip-of-the-tongue (TOT) retrieval. We introduce a simple but effective framework for handling such complex queries by decomposing the query with an LLM into individual clues routing those as subqueries to specialized retrievers, and ensembling the results. Our approach takes advantage of off-the-shelf retrievers (e.g., CLIP for retrieving images of book covers) or incorporate retriever-specific logic (e.g., date constraints). We show that our framework incorporating query decomposition into retrievers can improve gold book recall up to 6% absolute gain for Recall@5 on a new collection of 14,441 real-world query-book pairs from an online community for resolving TOT inquiries.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lin-etal-2023-decomposing</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lin, Kevin and Lo, Kyle and Gonzalez, Joseph and Klein, Dan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of EMNLP}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.findings-emnlp.367}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Decomposing Complex Queries for Tip-of-the-tongue Retrieval}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.findings-emnlp.367}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/papermage-a-unified-toolkit-for-processing-representing-and-manipulating-visually-rich-scientific-documents-480.webp 480w,/assets/img/publication_preview/papermage-a-unified-toolkit-for-processing-representing-and-manipulating-visually-rich-scientific-documents-800.webp 800w,/assets/img/publication_preview/papermage-a-unified-toolkit-for-processing-representing-and-manipulating-visually-rich-scientific-documents-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/papermage-a-unified-toolkit-for-processing-representing-and-manipulating-visually-rich-scientific-documents.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="papermage-a-unified-toolkit-for-processing-representing-and-manipulating-visually-rich-scientific-documents.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lo-etal-2023-papermage" class="col-sm-8"> <div class="title">PaperMage: A Unified Toolkit for Processing, Representing, and Manipulating Visually-Rich Scientific Documents</div> <div class="author"> <em>Kyle Lo</em>,¬†Zejiang Shen,¬†Benjamin Newman,¬†Joseph Chang,¬†Russell Authur,¬†Erin Bransom,¬†Stefan Candra, and <span class="more-authors" title="click to view 10 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '10 more authors' ? 'Yoganand Chandrasekhar, Regan Huff, Bailey Kuehl, Amanpreet Singh, Chris Wilhelm, Angele Zamarron, Marti A. Hearst, Daniel Weld, Doug Downey, Luca Soldaini' : '10 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">10 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <div>üèÜ <strong>Best Paper Award</strong> üèÜ</div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2023.emnlp-demo.45" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/2023.emnlp-demo.45" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/papermage-a-unified-toolkit-for-processing-representing-and-manipulating-visually-rich-scientific-documents.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Award</p> </div> <div class="abstract hidden"> <p>Despite growing interest in applying natural language processing (NLP) and computer vision (CV) models to the scholarly domain, scientific documents remain challenging to work with. They‚Äôre often in difficult-to-use PDF formats, and the ecosystem of models to process them is fragmented and incomplete. We introduce PaperMage, an open-source Python toolkit for analyzing and processing visually-rich, structured scientific documents. PaperMage offers clean and intuitive abstractions for seamlessly representing and manipulating both textual and visual document elements. PaperMage achieves this by integrating disparate state-of-the-art NLP and CV models into a unified framework, and provides turn-key recipes for common scientific document processing use-cases. PaperMage has powered multiple research prototypes of AI applications over scientific documents, along with Semantic Scholar‚Äôs large-scale production system for processing millions of PDFs. GitHub: https://github.com/allenai/papermage</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lo-etal-2023-papermage</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lo, Kyle and Shen, Zejiang and Newman, Benjamin and Chang, Joseph and Authur, Russell and Bransom, Erin and Candra, Stefan and Chandrasekhar, Yoganand and Huff, Regan and Kuehl, Bailey and Singh, Amanpreet and Wilhelm, Chris and Zamarron, Angele and Hearst, Marti A. and Weld, Daniel and Downey, Doug and Soldaini, Luca}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.emnlp-demo.45}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{P}aper{M}age: A Unified Toolkit for Processing, Representing, and Manipulating Visually-Rich Scientific Documents}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.emnlp-demo.45}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/paloma-a-benchmark-for-evaluating-language-model-fit-480.webp 480w,/assets/img/publication_preview/paloma-a-benchmark-for-evaluating-language-model-fit-800.webp 800w,/assets/img/publication_preview/paloma-a-benchmark-for-evaluating-language-model-fit-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/paloma-a-benchmark-for-evaluating-language-model-fit.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paloma-a-benchmark-for-evaluating-language-model-fit.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Magnusson2023PalomaAB" class="col-sm-8"> <div class="title">Paloma: A Benchmark for Evaluating Language Model Fit</div> <div class="author"> Ian Magnusson,¬†Akshita Bhagia,¬†Valentin Hofmann,¬†Luca Soldaini,¬†A. Jha,¬†Oyvind Tafjord,¬†Dustin Schwenk, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Evan Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz Beltagy, Hanna Hajishirzi, Noah A. Smith, Kyle Richardson, Jesse Dodge' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">9 more authors</span> </div> <div class="periodical"> <em>ArXiv</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2312.10523" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/paloma-a-benchmark-for-evaluating-language-model-fit.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Language models (LMs) commonly report perplexity on monolithic data held out from training. Implicitly or explicitly, this data is composed of domains‚Äîvarying distributions of language. Rather than assuming perplexity on one distribution extrapolates to others, Perplexity Analysis for Language Model Assessment (Paloma), measures LM fit to 585 text domains, ranging from nytimes.com to r/depression on Reddit. We invite submissions to our benchmark and organize results by comparability based on compliance with guidelines such as removal of benchmark contamination from pretraining. Submissions can also record parameter and training token count to make comparisons of Pareto efficiency for performance as a function of these measures of cost. We populate our benchmark with results from 6 baselines pretrained on popular corpora. In case studies, we demonstrate analyses that are possible with Paloma, such as finding that pretraining without data beyond Common Crawl leads to inconsistent fit to many domains.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Magnusson2023PalomaAB</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Magnusson, Ian and Bhagia, Akshita and Hofmann, Valentin and Soldaini, Luca and Jha, A. and Tafjord, Oyvind and Schwenk, Dustin and Walsh, Evan Pete and Elazar, Yanai and Lo, Kyle and Groeneveld, Dirk and Beltagy, Iz and Hajishirzi, Hanna and Smith, Noah A. and Richardson, Kyle and Dodge, Jesse}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Paloma: A Benchmark for Evaluating Language Model Fit}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2312.10523}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/a-question-answering-framework-for-decontextualizing-user-facing-snippets-from-scientific-documents-480.webp 480w,/assets/img/publication_preview/a-question-answering-framework-for-decontextualizing-user-facing-snippets-from-scientific-documents-800.webp 800w,/assets/img/publication_preview/a-question-answering-framework-for-decontextualizing-user-facing-snippets-from-scientific-documents-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/a-question-answering-framework-for-decontextualizing-user-facing-snippets-from-scientific-documents.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="a-question-answering-framework-for-decontextualizing-user-facing-snippets-from-scientific-documents.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="newman-etal-2023-question" class="col-sm-8"> <div class="title">A Question Answering Framework for Decontextualizing User-facing Snippets from Scientific Documents</div> <div class="author"> Benjamin Newman,¬†Luca Soldaini,¬†Raymond Fok,¬†Arman Cohan,¬†and¬†<em>Kyle Lo</em> </div> <div class="periodical"> <em>In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2023.emnlp-main.193" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2305.14772" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://aclanthology.org/2023.emnlp-main.193" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/a-question-answering-framework-for-decontextualizing-user-facing-snippets-from-scientific-documents.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Many real-world applications (e.g., note taking, search) require extracting a sentence or paragraph from a document and showing that snippet to a human outside of the source document. Yet, users may find snippets difficult to understand as they lack context from the original document. In this work, we use language models to rewrite snippets from scientific documents to be read on their own. First, we define the requirements and challenges for this user-facing decontextualization task, such as clarifying where edits occur and handling references to other documents. Second, we propose a framework that decomposes the task into three stages: question generation, question answering, and rewriting. Using this framework, we collect gold decontextualizations from experienced scientific article readers. We then conduct a range of experiments across state-of-the-art commercial and open-source language models to identify how to best provide missing-but-relevant information to models for our task. Finally, we develop QaDecontext, a simple prompting strategy inspired by our framework that improves over end-to-end prompting. We conclude with analysis that finds, while rewriting is easy, question generation and answering remain challenging for today‚Äôs models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">newman-etal-2023-question</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Newman, Benjamin and Soldaini, Luca and Fok, Raymond and Cohan, Arman and Lo, Kyle}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.emnlp-main.193}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Question Answering Framework for Decontextualizing User-facing Snippets from Scientific Documents}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.emnlp-main.193}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/back-to-basics-a-simple-recipe-for-improving-out-of-domain-retrieval-in-dense-encoders-480.webp 480w,/assets/img/publication_preview/back-to-basics-a-simple-recipe-for-improving-out-of-domain-retrieval-in-dense-encoders-800.webp 800w,/assets/img/publication_preview/back-to-basics-a-simple-recipe-for-improving-out-of-domain-retrieval-in-dense-encoders-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/back-to-basics-a-simple-recipe-for-improving-out-of-domain-retrieval-in-dense-encoders.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="back-to-basics-a-simple-recipe-for-improving-out-of-domain-retrieval-in-dense-encoders.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Lee2023BackTB" class="col-sm-8"> <div class="title">Back to Basics: A Simple Recipe for Improving Out-of-Domain Retrieval in Dense Encoders</div> <div class="author"> Hyunji Lee,¬†Luca Soldaini,¬†Arman Cohan,¬†Minjoon Seo,¬†and¬†<em>Kyle Lo</em> </div> <div class="periodical"> <em>ArXiv</em>, Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.09765" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/back-to-basics-a-simple-recipe-for-improving-out-of-domain-retrieval-in-dense-encoders.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Prevailing research practice today often relies on training dense retrievers on existing large datasets such as MSMARCO and then experimenting with ways to improve zero-shot generalization capabilities to unseen domains. While prior work has tackled this challenge through resource-intensive steps such as data augmentation, architectural modifications, increasing model size, or even further base model pretraining, comparatively little investigation has examined whether the training procedures themselves can be improved to yield better generalization capabilities in the resulting models. In this work, we recommend a simple recipe for training dense encoders: Train on MSMARCO with parameter-efficient methods, such as LoRA, and opt for using in-batch negatives unless given well-constructed hard negatives. We validate these recommendations using the BEIR benchmark and find results are persistent across choice of dense encoder and base model size and are complementary to other resource-intensive strategies for out-of-domain generalization such as architectural modifications or additional pretraining. We hope that this thorough and impartial study around various training techniques, which augments other resource-intensive methods, offers practical insights for developing a dense retrieval model that effectively generalizes, even when trained on a single dataset.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Lee2023BackTB</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lee, Hyunji and Soldaini, Luca and Cohan, Arman and Seo, Minjoon and Lo, Kyle}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Back to Basics: A Simple Recipe for Improving Out-of-Domain Retrieval in Dense Encoders}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2311.09765}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/the-rise-of-open-science-tracking-the-evolution-and-perceived-value-of-data-and-methods-link-sharing-practices-480.webp 480w,/assets/img/publication_preview/the-rise-of-open-science-tracking-the-evolution-and-perceived-value-of-data-and-methods-link-sharing-practices-800.webp 800w,/assets/img/publication_preview/the-rise-of-open-science-tracking-the-evolution-and-perceived-value-of-data-and-methods-link-sharing-practices-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/the-rise-of-open-science-tracking-the-evolution-and-perceived-value-of-data-and-methods-link-sharing-practices.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="the-rise-of-open-science-tracking-the-evolution-and-perceived-value-of-data-and-methods-link-sharing-practices.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Cao2023TheRO" class="col-sm-8"> <div class="title">The Rise of Open Science: Tracking the Evolution and Perceived Value of Data and Methods Link-Sharing Practices</div> <div class="author"> Hancheng Cao,¬†Jesse Dodge,¬†<em>Kyle Lo</em>,¬†Daniel A. McFarland,¬†and¬†Lucy Lu Wang </div> <div class="periodical"> <em>ArXiv</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.03193" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/the-rise-of-open-science-tracking-the-evolution-and-perceived-value-of-data-and-methods-link-sharing-practices.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In recent years, funding agencies and journals increasingly advocate for open science practices (e.g. data and method sharing) to improve the transparency, access, and reproducibility of science. However, quantifying these practices at scale has proven difficult. In this work, we leverage a large-scale dataset of 1.1M papers from arXiv that are representative of the fields of physics, math, and computer science to analyze the adoption of data and method link-sharing practices over time and their impact on article reception. To identify links to data and methods, we train a neural text classification model to automatically classify URL types based on contextual mentions in papers. We find evidence that the practice of link-sharing to methods and data is spreading as more papers include such URLs over time. Reproducibility efforts may also be spreading because the same links are being increasingly reused across papers (especially in computer science); and these links are increasingly concentrated within fewer web domains (e.g. Github) over time. Lastly, articles that share data and method links receive increased recognition in terms of citation count, with a stronger effect when the shared links are active (rather than defunct). Together, these findings demonstrate the increased spread and perceived value of data and method sharing practices in open science.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Cao2023TheRO</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Hancheng and Dodge, Jesse and Lo, Kyle and McFarland, Daniel A. and Wang, Lucy Lu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Rise of Open Science: Tracking the Evolution and Perceived Value of Data and Methods Link-Sharing Practices}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2310.03193}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/booookscore-a-systematic-exploration-of-book-length-summarization-in-the-era-of-llms-480.webp 480w,/assets/img/publication_preview/booookscore-a-systematic-exploration-of-book-length-summarization-in-the-era-of-llms-800.webp 800w,/assets/img/publication_preview/booookscore-a-systematic-exploration-of-book-length-summarization-in-the-era-of-llms-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/booookscore-a-systematic-exploration-of-book-length-summarization-in-the-era-of-llms.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="booookscore-a-systematic-exploration-of-book-length-summarization-in-the-era-of-llms.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Chang2023BooookScoreAS" class="col-sm-8"> <div class="title">BooookScore: A systematic exploration of book-length summarization in the era of LLMs</div> <div class="author"> Yapei Chang,¬†<em>Kyle Lo</em>,¬†Tanya Goyal,¬†and¬†Mohit Iyyer </div> <div class="periodical"> <em>ArXiv</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.00785" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/booookscore-a-systematic-exploration-of-book-length-summarization-in-the-era-of-llms.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Summarizing book-length documents (&gt;100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BooookScore, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving $15K and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than the oft-repetitive ones generated by LLaMA 2. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by human annotators. We release code and annotations after blind review to spur more principled research on book-length summarization.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Chang2023BooookScoreAS</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chang, Yapei and Lo, Kyle and Goyal, Tanya and Iyyer, Mohit}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BooookScore: A systematic exploration of book-length summarization in the era of LLMs}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2310.00785}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/when-do-generative-query-and-document-expansions-fail-a-comprehensive-study-across-methods-retrievers-and-datasets-480.webp 480w,/assets/img/publication_preview/when-do-generative-query-and-document-expansions-fail-a-comprehensive-study-across-methods-retrievers-and-datasets-800.webp 800w,/assets/img/publication_preview/when-do-generative-query-and-document-expansions-fail-a-comprehensive-study-across-methods-retrievers-and-datasets-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/when-do-generative-query-and-document-expansions-fail-a-comprehensive-study-across-methods-retrievers-and-datasets.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="when-do-generative-query-and-document-expansions-fail-a-comprehensive-study-across-methods-retrievers-and-datasets.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Weller2023WhenDG" class="col-sm-8"> <div class="title">When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets</div> <div class="author"> Orion Weller,¬†<em>Kyle Lo</em>,¬†David Wadden,¬†Dawn J Lawrie,¬†Benjamin Van Durme,¬†Arman Cohan,¬†and¬†Luca Soldaini </div> <div class="periodical"> <em>ArXiv</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.08541" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/when-do-generative-query-and-document-expansions-fail-a-comprehensive-study-across-methods-retrievers-and-datasets.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Using large language models (LMs) for query or document expansion can improve generalization in information retrieval. However, it is unknown whether these techniques are universally beneficial or only effective in specific settings, such as for particular retrieval models, dataset domains, or query types. To answer this, we conduct the first comprehensive analysis of LM-based expansion. We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models. We show this trend holds across a set of eleven expansion techniques, twelve datasets with diverse distribution shifts, and twenty-four retrieval models. Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positives). Our results suggest the following recipe: use expansions for weaker models or when the target dataset significantly differs from training corpus in format; otherwise, avoid expansions to keep the relevance signal clear.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Weller2023WhenDG</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Weller, Orion and Lo, Kyle and Wadden, David and Lawrie, Dawn J and Durme, Benjamin Van and Cohan, Arman and Soldaini, Luca}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2309.08541}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/are-layout-infused-language-models-robust-to-layout-distribution-shifts-a-case-study-with-scientific-documents-480.webp 480w,/assets/img/publication_preview/are-layout-infused-language-models-robust-to-layout-distribution-shifts-a-case-study-with-scientific-documents-800.webp 800w,/assets/img/publication_preview/are-layout-infused-language-models-robust-to-layout-distribution-shifts-a-case-study-with-scientific-documents-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/are-layout-infused-language-models-robust-to-layout-distribution-shifts-a-case-study-with-scientific-documents.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="are-layout-infused-language-models-robust-to-layout-distribution-shifts-a-case-study-with-scientific-documents.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen-etal-2023-layout" class="col-sm-8"> <div class="title">Are Layout-Infused Language Models Robust to Layout Distribution Shifts? A Case Study with Scientific Documents</div> <div class="author"> Catherine Chen,¬†Zejiang Shen,¬†Dan Klein,¬†Gabriel Stanovsky,¬†Doug Downey,¬†and¬†<em>Kyle Lo</em> </div> <div class="periodical"> <em>In Findings of ACL</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2023.findings-acl.844" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2306.01058" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/are-layout-infused-language-models-robust-to-layout-distribution-shifts-a-case-study-with-scientific-documents.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Recent work has shown that infusing layout features into language models (LMs) improves processing of visually-rich documents such as scientific papers. Layout-infused LMs are often evaluated on documents with familiar layout features (e.g., papers from the same publisher), but in practice models encounter documents with unfamiliar distributions of layout features, such as new combinations of text sizes and styles, or new spatial configurations of textual elements. In this work we test whether layout-infused LMs are robust to layout distribution shifts. As a case study we use the task of scientific document structure recovery, segmenting a scientific paper into its structural categories (e.g., ‚Äútitle‚Äù, ‚Äúcaption‚Äù, ‚Äúreference‚Äù). To emulate distribution shifts that occur in practice we re-partition the GROTOAP2 dataset. We find that under layout distribution shifts model performance degrades by up to 20 F1. Simple training strategies, such as increasing training diversity, can reduce this degradation by over 35% relative F1; however, models fail to reach in-distribution performance in any tested out-of-distribution conditions. This work highlights the need to consider layout distribution shifts during model evaluation, and presents a methodology for conducting such evaluations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen-etal-2023-layout</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Catherine and Shen, Zejiang and Klein, Dan and Stanovsky, Gabriel and Downey, Doug and Lo, Kyle}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of ACL}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.findings-acl.844}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Are Layout-Infused Language Models Robust to Layout Distribution Shifts? A Case Study with Scientific Documents}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.findings-acl.844}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/efficiency-pentathlon-a-standardized-arena-for-efficiency-evaluation-480.webp 480w,/assets/img/publication_preview/efficiency-pentathlon-a-standardized-arena-for-efficiency-evaluation-800.webp 800w,/assets/img/publication_preview/efficiency-pentathlon-a-standardized-arena-for-efficiency-evaluation-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/efficiency-pentathlon-a-standardized-arena-for-efficiency-evaluation.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="efficiency-pentathlon-a-standardized-arena-for-efficiency-evaluation.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Peng2023EfficiencyPA" class="col-sm-8"> <div class="title">Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation</div> <div class="author"> Hao Peng,¬†Qingqing Cao,¬†Jesse Dodge,¬†Matthew E. Peters,¬†Jared Fernandez,¬†Tom Sherborne,¬†<em>Kyle Lo</em>, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Evan Pete Walsh, Noah Smith, Hanna Hajishirzi' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">7 more authors</span> </div> <div class="periodical"> <em>ArXiv</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2307.09701" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/efficiency-pentathlon-a-standardized-arena-for-efficiency-evaluation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model‚Äôs lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Peng2023EfficiencyPA</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Peng, Hao and Cao, Qingqing and Dodge, Jesse and Peters, Matthew E. and Fernandez, Jared and Sherborne, Tom and Lo, Kyle and Skjonsberg, Sam and Strubell, Emma and Plessas, Darrell and Beltagy, Iz and Walsh, Evan Pete and Smith, Noah and Hajishirzi, Hanna}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2307.09701}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/longeval-guidelines-for-human-evaluation-of-faithfulness-in-long-form-summarization-480.webp 480w,/assets/img/publication_preview/longeval-guidelines-for-human-evaluation-of-faithfulness-in-long-form-summarization-800.webp 800w,/assets/img/publication_preview/longeval-guidelines-for-human-evaluation-of-faithfulness-in-long-form-summarization-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/longeval-guidelines-for-human-evaluation-of-faithfulness-in-long-form-summarization.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="longeval-guidelines-for-human-evaluation-of-faithfulness-in-long-form-summarization.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="krishna-etal-2023-longeval" class="col-sm-8"> <div class="title">LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization</div> <div class="author"> Kalpesh Krishna,¬†Erin Bransom,¬†Bailey Kuehl,¬†Mohit Iyyer,¬†Pradeep Dasigi,¬†Arman Cohan,¬†and¬†<em>Kyle Lo</em> </div> <div class="periodical"> <em>In EACL</em>, May 2023 </div> <div class="periodical"> </div> <div class="links"> <div>üèÜ <strong>Outstanding Paper Award</strong> üèÜ</div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2301.13298" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://aclanthology.org/2023.eacl-main.121" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/longeval-guidelines-for-human-evaluation-of-faithfulness-in-long-form-summarization.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Outstanding Paper Award</p> </div> <div class="abstract hidden"> <p>While human evaluation remains best practice for accurately judging the faithfulness of automatically-generated summaries, few solutions exist to address the increased difficulty and workload when evaluating long-form summaries. Through a survey of 162 papers on long-form summarization, we first shed light on current human evaluation practices surrounding long-form summaries. We find that 73% of these papers do not perform any human evaluation on model-generated summaries, while other works face new difficulties that manifest when dealing with long documents (e.g., low inter-annotator agreement). Motivated by our survey, we present LongEval, a set of guidelines for human evaluation of faithfulness in long-form summaries that addresses the following challenges: (1) How can we achieve high inter-annotator agreement on faithfulness scores? (2) How can we minimize annotator workload while maintaining accurate faithfulness scores? and (3) Do humans benefit from automated alignment between summary and source snippets? We deploy LongEval in annotation studies on two long-form summarization datasets in different domains (SQuALITY and PubMed), and we find that switching to a finer granularity of judgment (e.g., clause-level) reduces inter-annotator variance in faithfulness scores (e.g., std-dev from 18.5 to 6.8). We also show that scores from a partial annotation of fine-grained units highly correlates with scores from a full annotation workload (0.89 Kendall‚Äôs tau using 50% judgements). We release our human judgments, annotation templates, and software as a Python library for future research.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">krishna-etal-2023-longeval</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Krishna, Kalpesh and Bransom, Erin and Kuehl, Bailey and Iyyer, Mohit and Dasigi, Pradeep and Cohan, Arman and Lo, Kyle}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{EACL}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{L}ong{E}val: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.eacl-main.121}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/complex-mathematical-symbol-definition-structures-a-dataset-and-model-for-coordination-resolution-in-definition-extraction-480.webp 480w,/assets/img/publication_preview/complex-mathematical-symbol-definition-structures-a-dataset-and-model-for-coordination-resolution-in-definition-extraction-800.webp 800w,/assets/img/publication_preview/complex-mathematical-symbol-definition-structures-a-dataset-and-model-for-coordination-resolution-in-definition-extraction-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/complex-mathematical-symbol-definition-structures-a-dataset-and-model-for-coordination-resolution-in-definition-extraction.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="complex-mathematical-symbol-definition-structures-a-dataset-and-model-for-coordination-resolution-in-definition-extraction.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="MartinBoyle2023ComplexMS" class="col-sm-8"> <div class="title">Complex Mathematical Symbol Definition Structures: A Dataset and Model for Coordination Resolution in Definition Extraction</div> <div class="author"> Anna Martin-Boyle,¬†Andrew Head,¬†<em>Kyle Lo</em>,¬†Risham Sidhu,¬†Marti A. Hearst,¬†and¬†Dongyeop Kang </div> <div class="periodical"> <em>ArXiv</em>, May 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2305.14660" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/complex-mathematical-symbol-definition-structures-a-dataset-and-model-for-coordination-resolution-in-definition-extraction.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">MartinBoyle2023ComplexMS</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Martin-Boyle, Anna and Head, Andrew and Lo, Kyle and Sidhu, Risham and Hearst, Marti A. and Kang, Dongyeop}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Complex Mathematical Symbol Definition Structures: A Dataset and Model for Coordination Resolution in Definition Extraction}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2305.14660}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/citesee-augmenting-citations-in-scientific-papers-with-persistent-and-personalized-historical-context-480.webp 480w,/assets/img/publication_preview/citesee-augmenting-citations-in-scientific-papers-with-persistent-and-personalized-historical-context-800.webp 800w,/assets/img/publication_preview/citesee-augmenting-citations-in-scientific-papers-with-persistent-and-personalized-historical-context-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/citesee-augmenting-citations-in-scientific-papers-with-persistent-and-personalized-historical-context.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="citesee-augmenting-citations-in-scientific-papers-with-persistent-and-personalized-historical-context.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3544548.3580847" class="col-sm-8"> <div class="title">CiteSee: Augmenting Citations in Scientific Papers with Persistent and Personalized Historical Context</div> <div class="author"> Joseph Chee Chang,¬†Amy X. Zhang,¬†Jonathan Bragg,¬†Andrew Head,¬†<em>Kyle Lo</em>,¬†Doug Downey,¬†and¬†Daniel S. Weld </div> <div class="periodical"> <em>In CHI</em>, Hamburg, Germany, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <div>üèÜ <strong>Best Paper Award</strong> üèÜ</div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3544548.3580847" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2302.07302" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3580847" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/citesee-augmenting-citations-in-scientific-papers-with-persistent-and-personalized-historical-context.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Award</p> </div> <div class="abstract hidden"> <p>When reading a scholarly article, inline citations help researchers contextualize the current article and discover relevant prior work. However, it can be challenging to prioritize and make sense of the hundreds of citations encountered during literature reviews. This paper introduces CiteSee, a paper reading tool that leverages a user‚Äôs publishing, reading, and saving activities to provide personalized visual augmentations and context around citations. First, CiteSee connects the current paper to familiar contexts by surfacing known citations a user had cited or opened. Second, CiteSee helps users prioritize their exploration by highlighting relevant but unknown citations based on saving and reading history. We conducted a lab study that suggests CiteSee is significantly more effective for paper discovery than three baselines. A field deployment study shows CiteSee helps participants keep track of their explorations and leads to better situational awareness and increased paper discovery via inline citation when conducting real-world literature reviews.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3544548.3580847</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chang, Joseph Chee and Zhang, Amy X. and Bragg, Jonathan and Head, Andrew and Lo, Kyle and Downey, Doug and Weld, Daniel S.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CHI}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3544548.3580847}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CiteSee: Augmenting Citations in Scientific Papers with Persistent and Personalized Historical Context}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3544548.3580847}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/paper-plain-making-medical-research-papers-approachable-to-healthcare-consumers-with-natural-language-processing-480.webp 480w,/assets/img/publication_preview/paper-plain-making-medical-research-papers-approachable-to-healthcare-consumers-with-natural-language-processing-800.webp 800w,/assets/img/publication_preview/paper-plain-making-medical-research-papers-approachable-to-healthcare-consumers-with-natural-language-processing-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/paper-plain-making-medical-research-papers-approachable-to-healthcare-consumers-with-natural-language-processing.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-plain-making-medical-research-papers-approachable-to-healthcare-consumers-with-natural-language-processing.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3589955" class="col-sm-8"> <div class="title">Paper Plain: Making Medical Research Papers Approachable to Healthcare Consumers with Natural Language Processing</div> <div class="author"> Tal August,¬†Lucy Lu Wang,¬†Jonathan Bragg,¬†Marti A. Hearst,¬†Andrew Head,¬†and¬†<em>Kyle Lo</em> </div> <div class="periodical"> <em>ACM Transactions of Computer-Human Interaction (TOCHI)</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3589955" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2203.00130" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://dl.acm.org/doi/abs/10.1145/3589955" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/paper-plain-making-medical-research-papers-approachable-to-healthcare-consumers-with-natural-language-processing.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>When seeking information not covered in patient-friendly documents, healthcare consumers may turn to the research literature. Reading medical papers, however, can be a challenging experience. To improve access to medical papers, we explore four features enabled by natural language processing: definitions of unfamiliar terms, in-situ plain language section summaries, a collection of key questions that guides readers to answering passages, and plain language summaries of those passages. We embody these features into a prototype system, Paper Plain. We evaluate Paper Plain, finding that participants who used the prototype system had an easier time reading research papers without a loss in paper comprehension compared to those who used a typical PDF reader. Altogether, the study results suggest that guiding readers to relevant passages and providing plain language summaries alongside the original paper content can make reading medical papers easier and give readers more confidence to approach these papers.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1145/3589955</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{August, Tal and Wang, Lucy Lu and Bragg, Jonathan and Hearst, Marti A. and Head, Andrew and Lo, Kyle}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3589955}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions of Computer-Human Interaction (TOCHI)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Paper Plain: Making Medical Research Papers Approachable to Healthcare Consumers with Natural Language Processing}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3589955}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/beyond-summarization-designing-ai-support-for-real-world-expository-writing-tasks-480.webp 480w,/assets/img/publication_preview/beyond-summarization-designing-ai-support-for-real-world-expository-writing-tasks-800.webp 800w,/assets/img/publication_preview/beyond-summarization-designing-ai-support-for-real-world-expository-writing-tasks-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/beyond-summarization-designing-ai-support-for-real-world-expository-writing-tasks.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="beyond-summarization-designing-ai-support-for-real-world-expository-writing-tasks.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Shen2023BeyondSD" class="col-sm-8"> <div class="title">Beyond Summarization: Designing AI Support for Real-World Expository Writing Tasks</div> <div class="author"> Zejiang Shen,¬†Tal August,¬†Pao Siangliulue,¬†<em>Kyle Lo</em>,¬†Jonathan Bragg,¬†Jeff Hammerbacher,¬†Doug Downey, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Joseph Chee Chang, David Sontag' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">2 more authors</span> </div> <div class="periodical"> <em>In Intelligent and Interactive Writing Assistants (In2Writing) Workshop</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2304.02623" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/beyond-summarization-designing-ai-support-for-real-world-expository-writing-tasks.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Large language models have introduced exciting new opportunities and challenges in designing and developing new AI-assisted writing support tools. Recent work has shown that leveraging this new technology can transform writing in many scenarios such as ideation during creative writing, editing support, and summarization. However, AI-supported expository writing‚Äìincluding real-world tasks like scholars writing literature reviews or doctors writing progress notes‚Äìis relatively understudied. In this position paper, we argue that developing AI supports for expository writing has unique and exciting research challenges and can lead to high real-world impacts. We characterize expository writing as evidence-based and knowledge-generating: it contains summaries of external documents as well as new information or knowledge. It can be seen as the product of authors‚Äô sensemaking process over a set of source documents, and the interplay between reading, reflection, and writing opens up new opportunities for designing AI support. We sketch three components for AI support design and discuss considerations for future research.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Shen2023BeyondSD</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shen, Zejiang and August, Tal and Siangliulue, Pao and Lo, Kyle and Bragg, Jonathan and Hammerbacher, Jeff and Downey, Doug and Chang, Joseph Chee and Sontag, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Intelligent and Interactive Writing Assistants (In2Writing) Workshop}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beyond Summarization: Designing AI Support for Real-World Expository Writing Tasks}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2304.02623}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/scim-intelligent-skimming-support-for-scientific-papers-480.webp 480w,/assets/img/publication_preview/scim-intelligent-skimming-support-for-scientific-papers-800.webp 800w,/assets/img/publication_preview/scim-intelligent-skimming-support-for-scientific-papers-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/scim-intelligent-skimming-support-for-scientific-papers.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="scim-intelligent-skimming-support-for-scientific-papers.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3581641.3584034" class="col-sm-8"> <div class="title">Scim: Intelligent Skimming Support for Scientific Papers</div> <div class="author"> Raymond Fok,¬†Hita Kambhamettu,¬†Luca Soldaini,¬†Jonathan Bragg,¬†<em>Kyle Lo</em>,¬†Marti Hearst,¬†Andrew Head, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Daniel S Weld' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">1 more author</span> </div> <div class="periodical"> <em>In IUI</em>, Sydney, NSW, Australia, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3581641.3584034" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2205.04561" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://dl.acm.org/doi/abs/10.1145/3581641.3584034" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/scim-intelligent-skimming-support-for-scientific-papers.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Scholars need to keep up with an exponentially increasing flood of scientific papers. To aid this challenge, we introduce Scim, a novel intelligent interface that helps experienced researchers skim ‚Äì or rapidly review ‚Äì a paper to attain a cursory understanding of its contents. Scim supports the skimming process by highlighting salient paper contents in order to direct a reader‚Äôs attention. The system‚Äôs highlights are faceted by content type, evenly distributed across a paper, and have a density configurable by readers at both the global and local level. We evaluate Scim with both an in-lab usability study and a longitudinal diary study, revealing how its highlights facilitate the more efficient construction of a conceptualization of a paper. We conclude by discussing design considerations and tensions for the design of future intelligent skimming tools.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3581641.3584034</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fok, Raymond and Kambhamettu, Hita and Soldaini, Luca and Bragg, Jonathan and Lo, Kyle and Hearst, Marti and Head, Andrew and Weld, Daniel S}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IUI}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3581641.3584034}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Scim: Intelligent Skimming Support for Scientific Papers}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3581641.3584034}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/limeade-from-ai-explanations-to-advice-taking-480.webp 480w,/assets/img/publication_preview/limeade-from-ai-explanations-to-advice-taking-800.webp 800w,/assets/img/publication_preview/limeade-from-ai-explanations-to-advice-taking-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/limeade-from-ai-explanations-to-advice-taking.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="limeade-from-ai-explanations-to-advice-taking.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3589345" class="col-sm-8"> <div class="title">LIMEADE: From AI Explanations to Advice Taking</div> <div class="author"> Benjamin Charles Germain Lee,¬†Doug Downey,¬†<em>Kyle Lo</em>,¬†and¬†Daniel S. Weld </div> <div class="periodical"> <em>ACM Transactions on Interactive Intelligent Systems</em>, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3589345" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2003.04315" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://dl.acm.org/doi/abs/10.1145/3589345" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/limeade-from-ai-explanations-to-advice-taking.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Research in human-centered AI has shown the benefits of systems that can explain their predictions. Methods that allow an AI to take advice from humans in response to explanations are similarly useful. While both capabilities are well-developed for transparent learning models (e.g., linear models and GA2Ms), and recent techniques (e.g., LIME and SHAP) can generate explanations for opaque models, little attention has been given to advice methods for opaque models. This paper introduces LIMEADE, the first general framework that translates both positive and negative advice (expressed using high-level vocabulary such as that employed by post-hoc explanations) into an update to an arbitrary, underlying opaque model. We demonstrate the generality of our approach with case studies on seventy real-world models across two broad domains: image classification and text recommendation. We show our method improves accuracy compared to a rigorous baseline on the image classification domains. For the text modality, we apply our framework to a neural recommender system for scientific papers on a public website; our user study shows that our framework leads to significantly higher perceived user control, trust, and satisfaction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1145/3589345</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lee, Benjamin Charles Germain and Downey, Doug and Lo, Kyle and Weld, Daniel S.}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3589345}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Interactive Intelligent Systems}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LIMEADE: From AI Explanations to Advice Taking}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3589345}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/the-semantic-scholar-open-data-platform-480.webp 480w,/assets/img/publication_preview/the-semantic-scholar-open-data-platform-800.webp 800w,/assets/img/publication_preview/the-semantic-scholar-open-data-platform-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/the-semantic-scholar-open-data-platform.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="the-semantic-scholar-open-data-platform.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kinney2023TheSS" class="col-sm-8"> <div class="title">The Semantic Scholar Open Data Platform</div> <div class="author"> Rodney Michael Kinney,¬†Chloe Anastasiades,¬†Russell Authur,¬†Iz Beltagy,¬†Jonathan Bragg,¬†Alexandra Buraczynski,¬†Isabel Cachola, and <span class="more-authors" title="click to view 41 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '41 more authors' ? 'Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph Gorney, David W. Graham, F.Q. Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey MacMillan, Tyler Murray, Christopher Newell, Smita Rao, Shaurya Rohatgi, Paul L Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian, A. Tanaka, Alex D Wade, Linda M. Wagner, Lucy Lu Wang, Christopher Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Madeleine Zuylen, Daniel S. Weld' : '41 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">41 more authors</span> </div> <div class="periodical"> <em>ArXiv</em>, Jan 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2301.10140" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/the-semantic-scholar-open-data-platform.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The volume of scientific output is creating an urgent need for automated tools to help scientists keep up with developments in their field. Semantic Scholar (S2) is an open data platform and website aimed at accelerating science by helping scholars discover and understand scientific literature. We combine public and proprietary data sources using state-of-the-art techniques for scholarly PDF content extraction and automatic knowledge graph construction to build the Semantic Scholar Academic Graph, the largest open scientific literature graph to-date, with 200M+ papers, 80M+ authors, 550M+ paper-authorship edges, and 2.4B+ citation edges. The graph includes advanced semantic features such as structurally parsed text, natural language summaries, and vector embeddings. In this paper, we describe the components of the S2 data processing pipeline and the associated APIs offered by the platform. We will update this living document to reflect changes as we add new data offerings and improve existing services.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Kinney2023TheSS</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kinney, Rodney Michael and Anastasiades, Chloe and Authur, Russell and Beltagy, Iz and Bragg, Jonathan and Buraczynski, Alexandra and Cachola, Isabel and Candra, Stefan and Chandrasekhar, Yoganand and Cohan, Arman and Crawford, Miles and Downey, Doug and Dunkelberger, Jason and Etzioni, Oren and Evans, Rob and Feldman, Sergey and Gorney, Joseph and Graham, David W. and Hu, F.Q. and Huff, Regan and King, Daniel and Kohlmeier, Sebastian and Kuehl, Bailey and Langan, Michael and Lin, Daniel and Liu, Haokun and Lo, Kyle and Lochner, Jaron and MacMillan, Kelsey and Murray, Tyler and Newell, Christopher and Rao, Smita and Rohatgi, Shaurya and Sayre, Paul L and Shen, Zejiang and Singh, Amanpreet and Soldaini, Luca and Subramanian, Shivashankar and Tanaka, A. and Wade, Alex D and Wagner, Linda M. and Wang, Lucy Lu and Wilhelm, Christopher and Wu, Caroline and Yang, Jiangjiang and Zamarron, Angele and van Zuylen, Madeleine and Weld, Daniel S.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Semantic Scholar Open Data Platform}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2301.10140}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/scifact-open-towards-open-domain-scientific-claim-verification-480.webp 480w,/assets/img/publication_preview/scifact-open-towards-open-domain-scientific-claim-verification-800.webp 800w,/assets/img/publication_preview/scifact-open-towards-open-domain-scientific-claim-verification-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/scifact-open-towards-open-domain-scientific-claim-verification.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="scifact-open-towards-open-domain-scientific-claim-verification.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wadden-etal-2022-scifact" class="col-sm-8"> <div class="title">SciFact-Open: Towards open-domain scientific claim verification</div> <div class="author"> David Wadden,¬†<em>Kyle Lo</em>,¬†Bailey Kuehl,¬†Arman Cohan,¬†Iz Beltagy,¬†Lucy Lu Wang,¬†and¬†Hannaneh Hajishirzi </div> <div class="periodical"> <em>In Findings of EMNLP</em>, Dec 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.findings-emnlp.347" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/scifact-open-towards-open-domain-scientific-claim-verification.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>While research on scientific claim verification has led to the development of powerful systems that appear to approach human performance, these approaches have yet to be tested in a realistic setting against large corpora of scientific literature. Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. Drawing upon pooling techniques from information retrieval, we collect evidence for scientific claims by pooling and annotating the top predictions of four state-of-the-art scientific claim verification models. We find that systems developed on smaller corpora struggle to generalize to SciFact-Open, exhibiting performance drops of at least 15 F1. In addition, analysis of the evidence in SciFact-Open reveals interesting phenomena likely to appear when claim verification systems are deployed in practice, e.g., cases where the evidence supports only a special case of the claim. Our dataset is available at https://github.com/dwadden/scifact-open.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wadden-etal-2022-scifact</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wadden, David and Lo, Kyle and Kuehl, Bailey and Cohan, Arman and Beltagy, Iz and Wang, Lucy Lu and Hajishirzi, Hannaneh}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of EMNLP}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{S}ci{F}act-Open: Towards open-domain scientific claim verification}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.findings-emnlp.347}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/bloom-a-176b-parameter-open-access-multilingual-language-model-480.webp 480w,/assets/img/publication_preview/bloom-a-176b-parameter-open-access-multilingual-language-model-800.webp 800w,/assets/img/publication_preview/bloom-a-176b-parameter-open-access-multilingual-language-model-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/bloom-a-176b-parameter-open-access-multilingual-language-model.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bloom-a-176b-parameter-open-access-multilingual-language-model.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Scao2022BLOOMA1" class="col-sm-8"> <div class="title">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</div> <div class="author"> Teven Le Scao,¬†Angela Fan,¬†Christopher Akiki,¬†Elizabeth-Jane Pavlick,¬†Suzana Ili‚Äôc,¬†Daniel Hesslow,¬†Roman Castagn‚Äôe, and <span class="more-authors" title="click to view 383 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '383 more authors' ? 'Alexandra Sasha Luccioni, Franccois Yvon, Matthias Gall√©, Jonathan Tow, Alexander M. Rush, Stella Rose Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno√Æt Sagot, Niklas Muennighoff, Albert Villanova Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurenccon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa Etxabe, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris C. Emezue, Christopher Klamm, Colin Leong, Daniel Alexander Strien, David Ifeoluwa Adelani, Dragomir R. Radev, Eduardo G. Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G√©rard Dupont, Germ√°n Kruszewski, Giada Pistilli, Hady ElSahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jorg Frohberg, Josephine L. Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Werra, Leon Weber, Long Phan, Loubna Ben Allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu√±oz, Maraim Masoud, Mar‚Äôia Grandury, Mario vSavsko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad Ali Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto L‚Äôopez, R. Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, S. Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal V. Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault F√©vry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiang Tang, Zheng Xin Yong, Zhiqing Sun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick Platen, Pierre Cornette, Pierre Franccois Lavall‚Äôee, R√©mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St√©phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur‚Äôelie N‚Äôev‚Äôeol, Charles Lovering, Daniel H Garrette, Deepak R. Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, S. Osher Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenvek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ananda Santa Rosa Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Olusola Ajibade, Bharat Kumar Saxena, Carlos Mu√±oz Ferrandis, Danish Contractor, David M. Lansky, Davis David, Douwe Kiela, Duong Anh Nguyen, Edward Tan, Emily Baylor, Ezinwanne Ozoani, Fatim T Mirza, Frankline Ononiwu, Habib Rezanejad, H.A. Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, J. Lawrence Passmore, Joshua Seltzer, Julio Bonis Sanz, Karen Fort, L√≠via Macedo Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, M. K. K. Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nourhan Fahmy, Olanrewaju Modupe Samuel, Ran An, R. P. Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu Nguyen Hai Le, Yoyo Yang, Zachary Kyle Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Kumar Singh, Benjamin Beilharz, Bo Wang, Caio Matheus Fonseca Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl√©mentine Fourrier, Daniel Le‚Äôon Perin‚Äôan, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully A. Burns, Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthi Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn Bykhovetz, Maiko Takeuchi, Marc P√†mies, Mar√≠a Andrea Castillo, Marianna Nezhurina, Mario Sanger, Matthias Samwald, Michael Cullan, Michael Weinberg, M Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patricia Haller, R. Chandrasekhar, R. Eisenberg, Robert Martin, Rodrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Pratap Bharati, T. A. Laud, Th‚Äôeo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yashasvi Bajaj, Y. Venkatraman, Yifan Xu, Ying Xu, Yun-chao Xu, Zhee Xao Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, Thomas Wolf' : '383 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">383 more authors</span> </div> <div class="periodical"> <em>ArXiv</em>, Nov 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2211.05100" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/bloom-a-176b-parameter-open-access-multilingual-language-model.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Scao2022BLOOMA1</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Elizabeth-Jane and Ili'c, Suzana and Hesslow, Daniel and Castagn'e, Roman and Luccioni, Alexandra Sasha and Yvon, Franccois and Gall{\'e}, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella Rose and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Beno{\i}t and Muennighoff, Niklas and del Moral, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Laurenccon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Etxabe, Aitor Soroa and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris C. and Klamm, Christopher and Leong, Colin and van Strien, Daniel Alexander and Adelani, David Ifeoluwa and Radev, Dragomir R. and Ponferrada, Eduardo G. and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and Toni, Francesco De and Dupont, G{\'e}rard and Kruszewski, Germ{\'a}n and Pistilli, Giada and ElSahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and Gonzalez-Dios, Itziar and de la Rosa, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, Jorg and Tobing, Josephine L. and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and von Werra, Leandro and Weber, Leon and Phan, Long and Allal, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Mu{\~n}oz, Manuel Romero and Masoud, Maraim and Grandury, Mar'ia and vSavsko, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad Ali and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and de Gibert, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and L'opez, Roberto and Ribeiro, R. and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, S. and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M Saiful and Al-shaibani, Maged S. and Manica, Matteo and Nayak, Nihal V. and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and Ben-David, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and F{\'e}vry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiang and Yong, Zheng Xin and Sun, Zhiqing and Brody, Shaked and Uri, Y and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and von Platen, Patrick and Cornette, Pierre and Lavall'ee, Pierre Franccois and Lacroix, R{\'e}mi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, St{\'e}phane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and N'ev'eol, Aur'elie and Lovering, Charles and Garrette, Daniel H and Tunuguntla, Deepak R. and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and van der Wal, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Pais, S. Osher and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zdenvek and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ananda Santa Rosa and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin Olusola and Saxena, Bharat Kumar and Ferrandis, Carlos Mu{\~n}oz and Contractor, Danish and Lansky, David M. and David, Davis and Kiela, Douwe and Nguyen, Duong Anh and Tan, Edward and Baylor, Emily and Ozoani, Ezinwanne and Mirza, Fatim T and Ononiwu, Frankline and Rezanejad, Habib and Jones, H.A. and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, J. Lawrence and Seltzer, Joshua and Sanz, Julio Bonis and Fort, Karen and Dutra, L{\'i}via Macedo and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, M. K. K. and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nourhan and Samuel, Olanrewaju Modupe and An, Ran and Kromann, R. P. and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas L. and Roy, Sourav and Viguier, Sylvain and Le, Thanh-Cong and Oyebade, Tobi and Le, Trieu Nguyen Hai and Yang, Yoyo and Nguyen, Zachary Kyle and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and Miranda-Escalada, Antonio and Singh, Ayush Kumar and Beilharz, Benjamin and Wang, Bo and de Brito, Caio Matheus Fonseca and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Cl{\'e}mentine and Perin'an, Daniel Le'on and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully A. and Vrabec, Helena U. and Bello, Iman I.B. and Dash, Isha and Kang, Ji Soo and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthi and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and de Bykhovetz, Madeleine Hahn and Takeuchi, Maiko and P{\`a}mies, Marc and Castillo, Mar{\'i}a Andrea and Nezhurina, Marianna and Sanger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and Wolf, M and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patricia and Chandrasekhar, R. and Eisenberg, R. and Martin, Robert and Canalli, Rodrigo L. and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil Pratap and Laud, T. A. and Gigant, Th'eo and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yashasvi and Venkatraman, Y. and Xu, Yifan and Xu, Ying and Xu, Yun-chao and Tan, Zhee Xao and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2211.05100}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/multi-lexsum-real-world-summaries-of-civil-rights-lawsuits-at-multiple-granularities-480.webp 480w,/assets/img/publication_preview/multi-lexsum-real-world-summaries-of-civil-rights-lawsuits-at-multiple-granularities-800.webp 800w,/assets/img/publication_preview/multi-lexsum-real-world-summaries-of-civil-rights-lawsuits-at-multiple-granularities-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/multi-lexsum-real-world-summaries-of-civil-rights-lawsuits-at-multiple-granularities.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="multi-lexsum-real-world-summaries-of-civil-rights-lawsuits-at-multiple-granularities.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="shen2022multilexsum" class="col-sm-8"> <div class="title">Multi-LexSum: Real-world Summaries of Civil Rights Lawsuits at Multiple Granularities</div> <div class="author"> Zejiang Shen,¬†<em>Kyle Lo</em>,¬†Lauren Yu,¬†Nathan Dahlberg,¬†Margo Schlanger,¬†and¬†Doug Downey </div> <div class="periodical"> <em>In NeurIPS (Datasets and Benchmarks)</em>, Nov 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=z1d8fUiS8Cr" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">OpenReview</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/multi-lexsum-real-world-summaries-of-civil-rights-lawsuits-at-multiple-granularities.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>With the advent of large language models, methods for abstractive summarization have made great strides, creating potential for use in applications to aid knowledge workers processing unwieldy document collections. One such setting is the Civil Rights Litigation Clearinghouse (CRLC, https://clearinghouse.net), which posts information about large-scale civil rights lawsuits, serving lawyers, scholars, and the general public. Today, summarization in the CRLC requires extensive training of lawyers and law students who spend hours per case understanding multiple relevant documents in order to produce high-quality summaries of key events and outcomes. Motivated by this ongoing real-world summarization effort, we introduce Multi-LexSum, a collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing. Multi-LexSum presents a challenging multi-document summarization task given the length of the source documents, often exceeding two hundred pages per case. Furthermore, Multi-LexSum is distinct from other datasets in its multiple target summaries, each at a different granularity (ranging from one-sentence "extreme" summaries to multi-paragraph narrations of over five hundred words). We present extensive analysis demonstrating that despite the high-quality summaries in the training data (adhering to strict content and style guidelines), state-of-the-art summarization models perform poorly on this task. We release Multi-LexSum for further summarization research and to facilitate the development of applications to assist in the CRLC‚Äôs mission at https://multilexsum.github.io.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">shen2022multilexsum</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shen, Zejiang and Lo, Kyle and Yu, Lauren and Dahlberg, Nathan and Schlanger, Margo and Downey, Doug}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NeurIPS (Datasets and Benchmarks)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-LexSum: Real-world Summaries of Civil Rights Lawsuits at Multiple Granularities}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=z1d8fUiS8Cr}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/overview-of-the-third-workshop-on-scholarly-document-processing-480.webp 480w,/assets/img/publication_preview/overview-of-the-third-workshop-on-scholarly-document-processing-800.webp 800w,/assets/img/publication_preview/overview-of-the-third-workshop-on-scholarly-document-processing-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/overview-of-the-third-workshop-on-scholarly-document-processing.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="overview-of-the-third-workshop-on-scholarly-document-processing.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cohan-etal-2022-overview" class="col-sm-8"> <div class="title">Overview of the Third Workshop on Scholarly Document Processing</div> <div class="author"> Arman Cohan,¬†Guy Feigenblat,¬†Dayne Freitag,¬†Tirthankar Ghosal,¬†Drahomira Herrmannova,¬†Petr Knoth,¬†<em>Kyle Lo</em>, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Philipp Mayr, Michal Shmueli-Scheuer, Anita Waard, Lucy Lu Wang' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">4 more authors</span> </div> <div class="periodical"> <em>In Scholarly Document Processing (SDP) Workshop</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.sdp-1.1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/overview-of-the-third-workshop-on-scholarly-document-processing.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>With the ever-increasing pace of research and high volume of scholarly communication, scholars face a daunting task. Not only must they keep up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. These needs have motivated an increasing focus on computational methods for enhancing search, summarization, and analysis of scholarly documents. However, the various strands of research on scholarly document processing remain fragmented. To reach out to the broader NLP and AI/ML community, pool distributed efforts in this area, and enable shared access to published research, we held the 3rd Workshop on Scholarly Document Processing (SDP) at COLING as a hybrid event (https://sdproc.org/2022/). The SDP workshop consisted of a research track, three invited talks and five Shared Tasks: 1) MSLR22: Multi-Document Summarization for Literature Reviews, 2) DAGPap22: Detecting automatically generated scientific papers, 3) SV-Ident 2022: Survey Variable Identification in Social Science Publications, 4) SKGG: Scholarly Knowledge Graph Generation, 5) MuP 2022: Multi Perspective Scientific Document Summarization. The program was geared towards NLP, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and providing solutions to open challenges.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cohan-etal-2022-overview</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cohan, Arman and Feigenblat, Guy and Freitag, Dayne and Ghosal, Tirthankar and Herrmannova, Drahomira and Knoth, Petr and Lo, Kyle and Mayr, Philipp and Shmueli-Scheuer, Michal and de Waard, Anita and Wang, Lucy Lu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Scholarly Document Processing (SDP) Workshop}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Overview of the Third Workshop on Scholarly Document Processing}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.sdp-1.1}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/automatic-question-answering-for-multiple-stakeholders-the-epidemic-question-answering-dataset-480.webp 480w,/assets/img/publication_preview/automatic-question-answering-for-multiple-stakeholders-the-epidemic-question-answering-dataset-800.webp 800w,/assets/img/publication_preview/automatic-question-answering-for-multiple-stakeholders-the-epidemic-question-answering-dataset-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/automatic-question-answering-for-multiple-stakeholders-the-epidemic-question-answering-dataset.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="automatic-question-answering-for-multiple-stakeholders-the-epidemic-question-answering-dataset.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Goodwin2022" class="col-sm-8"> <div class="title">Automatic question answering for multiple stakeholders, the epidemic question answering dataset</div> <div class="author"> Travis R. Goodwin,¬†Dina Demner-Fushman,¬†<em>Kyle Lo</em>,¬†Lucy Lu Wang,¬†Hoa T. Dang,¬†and¬†Ian M. Soboroff </div> <div class="periodical"> <em>Scientific Data</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1038/s41597-022-01533-w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://www.nature.com/articles/s41597-022-01533-w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Nature</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/automatic-question-answering-for-multiple-stakeholders-the-epidemic-question-answering-dataset.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>One of the effects of COVID-19 pandemic is a rapidly growing and changing stream of publications to inform clinicians, researchers, policy makers, and patients about the health, socio-economic, and cultural consequences of the pandemic. Managing this information stream manually is not feasible. Automatic Question Answering can quickly bring the most salient points to the user‚Äôs attention. Leveraging a collection of scientific articles, government websites, relevant news articles, curated social media posts, and questions asked by researchers, clinicians, and the general public, we developed a dataset to explore automatic Question Answering for multiple stakeholders. Analysis of questions asked by various stakeholders shows that while information needs of experts and the public may overlap, satisfactory answers to these questions often originate from different information sources or benefit from different approaches to answer generation. We believe that this dataset has the potential to support the development of question answering systems not only for epidemic questions, but for other domains with varying expertise such as legal or finance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Goodwin2022</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Goodwin, Travis R. and Demner-Fushman, Dina and Lo, Kyle and Wang, Lucy Lu and Dang, Hoa T. and Soboroff, Ian M.}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1038/s41597-022-01533-w}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Scientific Data}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automatic question answering for multiple stakeholders, the epidemic question answering dataset}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1038/s41597-022-01533-w}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/multicite-modeling-realistic-citations-requires-moving-beyond-the-single-sentence-single-label-setting-480.webp 480w,/assets/img/publication_preview/multicite-modeling-realistic-citations-requires-moving-beyond-the-single-sentence-single-label-setting-800.webp 800w,/assets/img/publication_preview/multicite-modeling-realistic-citations-requires-moving-beyond-the-single-sentence-single-label-setting-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/multicite-modeling-realistic-citations-requires-moving-beyond-the-single-sentence-single-label-setting.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="multicite-modeling-realistic-citations-requires-moving-beyond-the-single-sentence-single-label-setting.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lauscher-etal-2022-multicite" class="col-sm-8"> <div class="title">MultiCite: Modeling realistic citations requires moving beyond the single-sentence single-label setting</div> <div class="author"> Anne Lauscher,¬†Brandon Ko,¬†Bailey Kuehl,¬†Sophie Johnson,¬†Arman Cohan,¬†David Jurgens,¬†and¬†<em>Kyle Lo</em> </div> <div class="periodical"> <em>In NAACL</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2022.naacl-main.137" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/2022.naacl-main.137" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/multicite-modeling-realistic-citations-requires-moving-beyond-the-single-sentence-single-label-setting.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Citation context analysis (CCA) is an important task in natural language processing that studies how and why scholars discuss each others‚Äô work. Despite decades of study, computational methods for CCA have largely relied on overly-simplistic assumptions of how authors cite, which ignore several important phenomena. For instance, scholarly papers often contain rich discussions of cited work that span multiple sentences and express multiple intents concurrently. Yet, recent work in CCA is often approached as a single-sentence, single-label classification task, and thus many datasets used to develop modern computational approaches fail to capture this interesting discourse. To address this research gap, we highlight three understudied phenomena for CCA and release MULTICITE, a new dataset of 12.6K citation contexts from 1.2K computational linguistics papers that fully models these phenomena. Not only is it the largest collection of expert-annotated citation contexts to-date, MULTICITE contains multi-sentence, multi-label citation contexts annotated through-out entire full paper texts. We demonstrate how MULTICITE can enable the development of new computational methods on three important CCA tasks. We release our code and dataset at https://github.com/allenai/multicite.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lauscher-etal-2022-multicite</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lauscher, Anne and Ko, Brandon and Kuehl, Bailey and Johnson, Sophie and Cohan, Arman and Jurgens, David and Lo, Kyle}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NAACL}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2022.naacl-main.137}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{M}ulti{C}ite: Modeling realistic citations requires moving beyond the single-sentence single-label setting}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.naacl-main.137}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/multivers-improving-scientific-claim-verification-with-weak-supervision-and-full-document-context-480.webp 480w,/assets/img/publication_preview/multivers-improving-scientific-claim-verification-with-weak-supervision-and-full-document-context-800.webp 800w,/assets/img/publication_preview/multivers-improving-scientific-claim-verification-with-weak-supervision-and-full-document-context-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/multivers-improving-scientific-claim-verification-with-weak-supervision-and-full-document-context.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="multivers-improving-scientific-claim-verification-with-weak-supervision-and-full-document-context.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wadden-etal-2022-multivers" class="col-sm-8"> <div class="title">MultiVerS: Improving scientific claim verification with weak supervision and full-document context</div> <div class="author"> David Wadden,¬†<em>Kyle Lo</em>,¬†Lucy Lu Wang,¬†Arman Cohan,¬†Iz Beltagy,¬†and¬†Hannaneh Hajishirzi </div> <div class="periodical"> <em>In Findings of NAACL</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2022.findings-naacl.6" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/2022.findings-naacl.6" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/multivers-improving-scientific-claim-verification-with-weak-supervision-and-full-document-context.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The scientific claim verification task requires an NLP system to label scientific documents which Support or Refute an input claim, and to select evidentiary sentences (or rationales) justifying each predicted label. In this work, we present MultiVerS, which predicts a fact-checking label and identifies rationales in a multitask fashion based on a shared encoding of the claim and full document context. This approach accomplishes two key modeling goals. First, it ensures that all relevant contextual information is incorporated into each labeling decision. Second, it enables the model to learn from instances annotated with a document-level fact-checking label, but lacking sentence-level rationales. This allows MultiVerS to perform weakly-supervised domain adaptation by training on scientific documents labeled using high-precision heuristics. Our approach outperforms two competitive baselines on three scientific claim verification datasets, with particularly strong performance in zero / few-shot domain adaptation experiments. Our code and data are available at https://github.com/dwadden/multivers.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wadden-etal-2022-multivers</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wadden, David and Lo, Kyle and Wang, Lucy Lu and Cohan, Arman and Beltagy, Iz and Hajishirzi, Hannaneh}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of NAACL}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2022.findings-naacl.6}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{M}ulti{V}er{S}: Improving scientific claim verification with weak supervision and full-document context}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.findings-naacl.6}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/data-governance-in-the-age-of-large-scale-data-driven-language-technology-480.webp 480w,/assets/img/publication_preview/data-governance-in-the-age-of-large-scale-data-driven-language-technology-800.webp 800w,/assets/img/publication_preview/data-governance-in-the-age-of-large-scale-data-driven-language-technology-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/data-governance-in-the-age-of-large-scale-data-driven-language-technology.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="data-governance-in-the-age-of-large-scale-data-driven-language-technology.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3531146.3534637" class="col-sm-8"> <div class="title">Data Governance in the Age of Large-Scale Data-Driven Language Technology</div> <div class="author"> Yacine Jernite,¬†Huu Nguyen,¬†Stella Biderman,¬†Anna Rogers,¬†Maraim Masoud,¬†Valentin Danchev,¬†Samson Tan, and <span class="more-authors" title="click to view 13 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '13 more authors' ? 'Alexandra Sasha Luccioni, Nishant Subramani, Isaac Johnson, Gerard Dupont, Jesse Dodge, Kyle Lo, Zeerak Talat, Dragomir Radev, Aaron Gokaslan, Somaieh Nikpoor, Peter Henderson, Rishi Bommasani, Margaret Mitchell' : '13 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">13 more authors</span> </div> <div class="periodical"> <em>In FAccT</em>, Seoul, Republic of Korea, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3531146.3534637" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2206.03216" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://dl.acm.org/doi/abs/10.1145/3531146.3534637" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/data-governance-in-the-age-of-large-scale-data-driven-language-technology.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3531146.3534637</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jernite, Yacine and Nguyen, Huu and Biderman, Stella and Rogers, Anna and Masoud, Maraim and Danchev, Valentin and Tan, Samson and Luccioni, Alexandra Sasha and Subramani, Nishant and Johnson, Isaac and Dupont, Gerard and Dodge, Jesse and Lo, Kyle and Talat, Zeerak and Radev, Dragomir and Gokaslan, Aaron and Nikpoor, Somaieh and Henderson, Peter and Bommasani, Rishi and Mitchell, Margaret}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{FAccT}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3531146.3534637}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Data Governance in the Age of Large-Scale Data-Driven Language Technology}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3531146.3534637}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/bigscience-roots-corpus-a-1-6tb-composite-multilingual-dataset-480.webp 480w,/assets/img/publication_preview/bigscience-roots-corpus-a-1-6tb-composite-multilingual-dataset-800.webp 800w,/assets/img/publication_preview/bigscience-roots-corpus-a-1-6tb-composite-multilingual-dataset-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/bigscience-roots-corpus-a-1-6tb-composite-multilingual-dataset.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bigscience-roots-corpus-a-1-6tb-composite-multilingual-dataset.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bigsciencerootscorpus" class="col-sm-8"> <div class="title">The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset</div> <div class="author"> Hugo Lauren√ßon,¬†Lucile Saulnier,¬†Thomas Wang,¬†Christopher Akiki,¬†Albert Villanova Moral,¬†Teven Le Scao,¬†Leandro Von Werra, and <span class="more-authors" title="click to view 47 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '47 more authors' ? 'Chenghao Mou, Eduardo Gonz√°lez Ponferrada, Huu Nguyen, J√∂rg Frohberg, Mario ≈†a≈°ko, Quentin Lhoest, Angelina McMillan-Major, G√©rard Dupont, Stella Biderman, Anna Rogers, Loubna Ben allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Romero Mu√±oz, Jian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid Almubarak, Vu Minh Chien, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Ifeoluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, Yacine Jernite' : '47 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">47 more authors</span> </div> <div class="periodical"> <em>In NeurIPS (Datasets and Benchmarks)</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=UoEw6KigkUn" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">OpenReview</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/bigscience-roots-corpus-a-1-6tb-composite-multilingual-dataset.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bigsciencerootscorpus</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and del Moral, Albert Villanova and Scao, Teven Le and Werra, Leandro Von and Mou, Chenghao and Ponferrada, Eduardo Gonz{\'a}lez and Nguyen, Huu and Frohberg, J{\"o}rg and {\v{S}}a{\v{s}}ko, Mario and Lhoest, Quentin and McMillan-Major, Angelina and Dupont, G{\'e}rard and Biderman, Stella and Rogers, Anna and allal, Loubna Ben and Toni, Francesco De and Pistilli, Giada and Nguyen, Olivier and Nikpoor, Somaieh and Masoud, Maraim and Colombo, Pierre and de la Rosa, Javier and Villegas, Paulo and Thrush, Tristan and Longpre, Shayne and Nagel, Sebastian and Weber, Leon and Mu{\~n}oz, Manuel Romero and Zhu, Jian and Strien, Daniel Van and Alyafeai, Zaid and Almubarak, Khalid and Chien, Vu Minh and Gonzalez-Dios, Itziar and Soroa, Aitor and Lo, Kyle and Dey, Manan and Suarez, Pedro Ortiz and Gokaslan, Aaron and Bose, Shamik and Adelani, David Ifeoluwa and Phan, Long and Tran, Hieu and Yu, Ian and Pai, Suhas and Chim, Jenny and Lepercq, Violette and Ilic, Suzana and Mitchell, Margaret and Luccioni, Sasha and Jernite, Yacine}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NeurIPS (Datasets and Benchmarks)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The BigScience {ROOTS} Corpus: A 1.6{TB} Composite Multilingual Dataset}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=UoEw6KigkUn}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/accord-a-multi-document-approach-to-generating-diverse-descriptions-of-scientific-concepts-480.webp 480w,/assets/img/publication_preview/accord-a-multi-document-approach-to-generating-diverse-descriptions-of-scientific-concepts-800.webp 800w,/assets/img/publication_preview/accord-a-multi-document-approach-to-generating-diverse-descriptions-of-scientific-concepts-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/accord-a-multi-document-approach-to-generating-diverse-descriptions-of-scientific-concepts.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="accord-a-multi-document-approach-to-generating-diverse-descriptions-of-scientific-concepts.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Murthy2022ACCoRDAM" class="col-sm-8"> <div class="title">ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions of Scientific Concepts</div> <div class="author"> Sonia K. Murthy,¬†<em>Kyle Lo</em>,¬†Daniel King,¬†Chandra Bhagavatula,¬†Bailey Kuehl,¬†Sophie Johnson,¬†Jon Borchardt, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Daniel S. Weld, Tom Hope, Doug Downey' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">3 more authors</span> </div> <div class="periodical"> <em>ArXiv</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2205.06982" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/accord-a-multi-document-approach-to-generating-diverse-descriptions-of-scientific-concepts.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Systems that can automatically define unfamiliar terms hold the promise of improving the accessibility of scientific texts, especially for readers who may lack prerequisite background knowledge. However, current systems assume a single "best" description per concept, which fails to account for the many potentially useful ways a concept can be described. We present ACCoRD, an end-to-end system tackling the novel task of generating sets of descriptions of scientific concepts. Our system takes advantage of the myriad ways a concept is mentioned across the scientific literature to produce distinct, diverse descriptions of target scientific concepts in terms of different reference concepts. To support research on the task, we release an expert-annotated resource, the ACCoRD corpus, which includes 1,275 labeled contexts and 1,787 hand-authored concept descriptions. We conduct a user study demonstrating that (1) users prefer descriptions produced by our end-to-end system, and (2) users prefer multiple descriptions to a single "best" description.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Murthy2022ACCoRDAM</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Murthy, Sonia K. and Lo, Kyle and King, Daniel and Bhagavatula, Chandra and Kuehl, Bailey and Johnson, Sophie and Borchardt, Jon and Weld, Daniel S. and Hope, Tom and Downey, Doug}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions of Scientific Concepts}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2205.06982}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vila-improving-structured-content-extraction-from-scientific-pdfs-using-visual-layout-groups-480.webp 480w,/assets/img/publication_preview/vila-improving-structured-content-extraction-from-scientific-pdfs-using-visual-layout-groups-800.webp 800w,/assets/img/publication_preview/vila-improving-structured-content-extraction-from-scientific-pdfs-using-visual-layout-groups-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/vila-improving-structured-content-extraction-from-scientific-pdfs-using-visual-layout-groups.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vila-improving-structured-content-extraction-from-scientific-pdfs-using-visual-layout-groups.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="shen-etal-2022-vila" class="col-sm-8"> <div class="title">VILA: Improving Structured Content Extraction from Scientific PDFs Using Visual Layout Groups</div> <div class="author"> Zejiang Shen,¬†<em>Kyle Lo</em>,¬†Lucy Lu Wang,¬†Bailey Kuehl,¬†Daniel S. Weld,¬†and¬†Doug Downey </div> <div class="periodical"> <em>Transactions of ACL (TACL)</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1162/tacl_a_00466" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/2022.tacl-1.22" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/vila-improving-structured-content-extraction-from-scientific-pdfs-using-visual-layout-groups.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Accurately extracting structured content from PDFs is a critical first step for NLP over scientific papers. Recent work has improved extraction accuracy by incorporating elementary layout information, for example, each token‚Äôs 2D position on the page, into language model pretraining. We introduce new methods that explicitly model VIsual LAyout (VILA) groups, that is, text lines or text blocks, to further improve performance. In our I-VILA approach, we show that simply inserting special tokens denoting layout group boundaries into model inputs can lead to a 1.9% Macro F1 improvement in token classification. In the H-VILA approach, we show that hierarchical encoding of layout-groups can result in up to 47% inference time reduction with less than 0.8% Macro F1 loss. Unlike prior layout-aware approaches, our methods do not require expensive additional pretraining, only fine-tuning, which we show can reduce training cost by up to 95%. Experiments are conducted on a newly curated evaluation suite, S2-VLUE, that unifies existing automatically labeled datasets and includes a new dataset of manual annotations covering diverse papers from 19 scientific disciplines. Pre-trained weights, benchmark datasets, and source code are available at https://github.com/allenai/VILA.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">shen-etal-2022-vila</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shen, Zejiang and Lo, Kyle and Wang, Lucy Lu and Kuehl, Bailey and Weld, Daniel S. and Downey, Doug}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1162/tacl_a_00466}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions of ACL (TACL)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{VILA}: Improving Structured Content Extraction from Scientific {PDF}s Using Visual Layout Groups}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.tacl-1.22}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/generating-scientific-claims-for-zero-shot-scientific-fact-checking-480.webp 480w,/assets/img/publication_preview/generating-scientific-claims-for-zero-shot-scientific-fact-checking-800.webp 800w,/assets/img/publication_preview/generating-scientific-claims-for-zero-shot-scientific-fact-checking-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/generating-scientific-claims-for-zero-shot-scientific-fact-checking.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="generating-scientific-claims-for-zero-shot-scientific-fact-checking.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wright-etal-2022-generating" class="col-sm-8"> <div class="title">Generating Scientific Claims for Zero-Shot Scientific Fact Checking</div> <div class="author"> Dustin Wright,¬†David Wadden,¬†<em>Kyle Lo</em>,¬†Bailey Kuehl,¬†Arman Cohan,¬†Isabelle Augenstein,¬†and¬†Lucy Lu Wang </div> <div class="periodical"> <em>In ACL</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2022.acl-long.175" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/2022.acl-long.175" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/generating-scientific-claims-for-zero-shot-scientific-fact-checking.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Automated scientific fact checking is difficult due to the complexity of scientific language and a lack of significant amounts of training data, as annotation requires domain expertise. To address this challenge, we propose scientific claim generation, the task of generating one or more atomic and verifiable claims from scientific sentences, and demonstrate its usefulness in zero-shot fact checking for biomedical claims. We propose CLAIMGEN-BART, a new supervised method for generating claims supported by the literature, as well as KBIN, a novel method for generating claim negations. Additionally, we adapt an existing unsupervised entity-centric method of claim generation to biomedical claims, which we call CLAIMGEN-ENTITY. Experiments on zero-shot fact checking demonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART, coupled with KBIN, achieve up to 90% performance of fully supervised models trained on manually annotated claims and evidence. A rigorous evaluation study demonstrates significant improvement in generated claim and negation quality over existing baselines</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wright-etal-2022-generating</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wright, Dustin and Wadden, David and Lo, Kyle and Kuehl, Bailey and Cohan, Arman and Augenstein, Isabelle and Wang, Lucy Lu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACL}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2022.acl-long.175}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generating Scientific Claims for Zero-Shot Scientific Fact Checking}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.acl-long.175}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/exploring-the-role-of-local-and-global-explanations-in-recommender-systems-480.webp 480w,/assets/img/publication_preview/exploring-the-role-of-local-and-global-explanations-in-recommender-systems-800.webp 800w,/assets/img/publication_preview/exploring-the-role-of-local-and-global-explanations-in-recommender-systems-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/exploring-the-role-of-local-and-global-explanations-in-recommender-systems.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="exploring-the-role-of-local-and-global-explanations-in-recommender-systems.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3491101.3519795" class="col-sm-8"> <div class="title">Exploring the Role of Local and Global Explanations in Recommender Systems</div> <div class="author"> Marissa Radensky,¬†Doug Downey,¬†<em>Kyle Lo</em>,¬†Zoran Popovic,¬†and¬†Daniel S Weld </div> <div class="periodical"> <em>In CHI (Extended Abstracts)</em>, New Orleans, LA, USA, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3491101.3519795" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2109.13301" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://dl.acm.org/doi/abs/10.1145/3491101.3519795" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/exploring-the-role-of-local-and-global-explanations-in-recommender-systems.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Explanations are well-known to improve recommender systems‚Äô transparency. These explanations may be local, explaining individual recommendations, or global, explaining the recommender model overall. Despite their widespread use, there has been little investigation into the relative benefits of the two explanation approaches. We conducted a 30-participant exploratory study and a 30-participant controlled user study with a research-paper recommender to analyze how providing local, global, or both explanations influences user understanding of system behavior. Our results provide evidence suggesting that both are more helpful than either alone for explaining how to improve recommendations, yet both appeared less helpful than global alone for efficiently identifying false positive and negative recommendations. However, we note that the two explanation approaches may be better compared in a higher-stakes or more opaque domain.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3491101.3519795</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Radensky, Marissa and Downey, Doug and Lo, Kyle and Popovic, Zoran and Weld, Daniel S}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CHI (Extended Abstracts)}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3491101.3519795}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring the Role of Local and Global Explanations in Recommender Systems}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3491101.3519795}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/infrastructure-for-rapid-open-knowledge-network-development-480.webp 480w,/assets/img/publication_preview/infrastructure-for-rapid-open-knowledge-network-development-800.webp 800w,/assets/img/publication_preview/infrastructure-for-rapid-open-knowledge-network-development-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/infrastructure-for-rapid-open-knowledge-network-development.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="infrastructure-for-rapid-open-knowledge-network-development.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1002/aaai.12038" class="col-sm-8"> <div class="title">Infrastructure for rapid open knowledge network development</div> <div class="author"> Michael Cafarella,¬†Michael Anderson,¬†Iz Beltagy,¬†Arie Cattan,¬†Sarah Chasins,¬†Ido Dagan,¬†Doug Downey, and <span class="more-authors" title="click to view 19 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '19 more authors' ? 'Oren Etzioni, Sergey Feldman, Tian Gao, Tom Hope, Kexin Huang, Sophie Johnson, Daniel King, Kyle Lo, Yuze Lou, Matthew Shapiro, Dinghao Shen, Shivashankar Subramanian, Lucy Lu Wang, Yuning Wang, Yitong Wang, Daniel S. Weld, Jenny Vo-Phamhi, Anna Zeng, Jiayun Zou' : '19 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">19 more authors</span> </div> <div class="periodical"> <em>AI Magazine</em>, Mar 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1002/aaai.12038" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://ojs.aaai.org/index.php/aimagazine/article/view/19126" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">AAAI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/infrastructure-for-rapid-open-knowledge-network-development.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Abstract The past decade has witnessed a growth in the use of knowledge graph technologies for advanced data search, data integration, and query-answering applications. The leading example of a public, general-purpose open knowledge network (aka knowledge graph) is Wikidata, which has demonstrated remarkable advances in quality and coverage over this time. Proprietary knowledge graphs drive some of the leading applications of the day including, for example, Google Search, Alexa, Siri, and Cortana. Open Knowledge Networks are exciting: they promise the power of structured database-like queries with the potential for the wide coverage that is today only provided by the Web. With the current state of the art, building, using, and scaling large knowledge networks can still be frustratingly slow. This article describes a National Science Foundation Convergence Accelerator project to build a set of Knowledge Network Programming Infrastructure systems to address this¬†issue.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1002/aaai.12038</span><span class="p">,</span>
  <span class="na">aaai</span> <span class="p">=</span> <span class="s">{aimagazine/article/view/19126}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cafarella, Michael and Anderson, Michael and Beltagy, Iz and Cattan, Arie and Chasins, Sarah and Dagan, Ido and Downey, Doug and Etzioni, Oren and Feldman, Sergey and Gao, Tian and Hope, Tom and Huang, Kexin and Johnson, Sophie and King, Daniel and Lo, Kyle and Lou, Yuze and Shapiro, Matthew and Shen, Dinghao and Subramanian, Shivashankar and Wang, Lucy Lu and Wang, Yuning and Wang, Yitong and Weld, Daniel S. and Vo-Phamhi, Jenny and Zeng, Anna and Zou, Jiayun}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1002/aaai.12038}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://onlinelibrary.wiley.com/doi/pdf/10.1002/aaai.12038}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{AI Magazine}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Infrastructure for rapid open knowledge network development}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12038}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{43}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/flex-unifying-evaluation-for-few-shot-nlp-480.webp 480w,/assets/img/publication_preview/flex-unifying-evaluation-for-few-shot-nlp-800.webp 800w,/assets/img/publication_preview/flex-unifying-evaluation-for-few-shot-nlp-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/flex-unifying-evaluation-for-few-shot-nlp.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="flex-unifying-evaluation-for-few-shot-nlp.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="NEURIPS2021_8493eeac" class="col-sm-8"> <div class="title">FLEX: Unifying Evaluation for Few-Shot NLP</div> <div class="author"> Jonathan Bragg,¬†Arman Cohan,¬†<em>Kyle Lo</em>,¬†and¬†Iz Beltagy </div> <div class="periodical"> <em>In NeurIPS</em>, Dec 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=_WnGcwXLYOE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">OpenReview</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/flex-unifying-evaluation-for-few-shot-nlp.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Few-shot NLP research is highly active, yet conducted in disjoint research threads with evaluation suites that lack challenging-yet-realistic testing setups and fail to employ careful experimental design. Consequently, the community does not know which techniques perform best or even if they outperform simple baselines. In response, we formulate the FLEX Principles, a set of requirements and best practices for unified, rigorous, valid, and cost-sensitive few-shot NLP evaluation. These principles include Sample Size Design, a novel approach to benchmark design that optimizes statistical accuracy and precision while keeping evaluation costs manageable. Following the principles, we release the FLEX benchmark, which includes four few-shot transfer settings, zero-shot evaluation, and a public leaderboard that covers diverse NLP tasks. In addition, we present UniFew, a prompt-based model for few-shot learning that unifies pretraining and finetuning prompt formats, eschewing complex machinery of recent prompt-based approaches in adapting downstream task formats to language model pretraining objectives. We demonstrate that despite simplicity, UniFew achieves results competitive with both popular meta-learning and prompt-based approaches.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NEURIPS2021_8493eeac</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bragg, Jonathan and Cohan, Arman and Lo, Kyle and Beltagy, Iz}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NeurIPS}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FLEX: Unifying Evaluation for Few-Shot NLP}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.neurips.cc/paper/2021/file/8493eeaccb772c0878f99d60a0bd2bb3-Paper.pdf}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/explaining-relationships-between-scientific-documents-480.webp 480w,/assets/img/publication_preview/explaining-relationships-between-scientific-documents-800.webp 800w,/assets/img/publication_preview/explaining-relationships-between-scientific-documents-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/explaining-relationships-between-scientific-documents.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="explaining-relationships-between-scientific-documents.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="luu-etal-2021-explaining" class="col-sm-8"> <div class="title">Explaining Relationships Between Scientific Documents</div> <div class="author"> Kelvin Luu,¬†Xinyi Wu,¬†Rik Koncel-Kedziorski,¬†<em>Kyle Lo</em>,¬†Isabel Cachola,¬†and¬†Noah A. Smith </div> <div class="periodical"> <em>In ACL</em>, Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2021.acl-long.166" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/2021.acl-long.166" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/explaining-relationships-between-scientific-documents.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We address the task of explaining relationships between two scientific documents using natural language text. This task requires modeling the complex content of long technical documents, deducing a relationship between these documents, and expressing the details of that relationship in text. In addition to the theoretical interest of this task, successful solutions can help improve researcher efficiency in search and review. In this paper we establish a dataset of 622K examples from 154K documents. We pretrain a large language model to serve as the foundation for autoregressive approaches to the task. We explore the impact of taking different views on the two documents, including the use of dense representations extracted with scientific IE systems. We provide extensive automatic and human evaluations which show the promise of such models, but make clear challenges for future work.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">luu-etal-2021-explaining</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Luu, Kelvin and Wu, Xinyi and Koncel-Kedziorski, Rik and Lo, Kyle and Cachola, Isabel and Smith, Noah A.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACL}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2021.acl-long.166}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Explaining Relationships Between Scientific Documents}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.acl-long.166}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/overview-of-the-second-workshop-on-scholarly-document-processing-480.webp 480w,/assets/img/publication_preview/overview-of-the-second-workshop-on-scholarly-document-processing-800.webp 800w,/assets/img/publication_preview/overview-of-the-second-workshop-on-scholarly-document-processing-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/overview-of-the-second-workshop-on-scholarly-document-processing.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="overview-of-the-second-workshop-on-scholarly-document-processing.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="beltagy-etal-2021-overview" class="col-sm-8"> <div class="title">Overview of the Second Workshop on Scholarly Document Processing</div> <div class="author"> Iz Beltagy,¬†Arman Cohan,¬†Guy Feigenblat,¬†Dayne Freitag,¬†Tirthankar Ghosal,¬†Keith Hall,¬†Drahomira Herrmannova, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Petr Knoth, Kyle Lo, Philipp Mayr, Robert Patton, Michal Shmueli-Scheuer, Anita Waard, Kuansan Wang, Lucy Lu Wang' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">8 more authors</span> </div> <div class="periodical"> <em>In Scholarly Document Processing (SDP) Workshop</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2021.sdp-1.22" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/overview-of-the-second-workshop-on-scholarly-document-processing.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>With the ever-increasing pace of research and high volume of scholarly communication, scholars face a daunting task. Not only must they keep up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. These needs have motivated an increasing focus on computational methods for enhancing search, summarization, and analysis of scholarly documents. However, the various strands of research on scholarly document processing remain fragmented. To reach out to the broader NLP and AI/ML community, pool distributed efforts in this area, and enable shared access to published research, we held the 2nd Workshop on Scholarly Document Processing (SDP) at NAACL 2021 as a virtual event (https://sdproc.org/2021/). The SDP workshop consisted of a research track, three invited talks, and three Shared Tasks (LongSumm 2021, SCIVER, and 3C). The program was geared towards the application of NLP, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and providing solutions to open challenges.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">beltagy-etal-2021-overview</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Beltagy, Iz and Cohan, Arman and Feigenblat, Guy and Freitag, Dayne and Ghosal, Tirthankar and Hall, Keith and Herrmannova, Drahomira and Knoth, Petr and Lo, Kyle and Mayr, Philipp and Patton, Robert and Shmueli-Scheuer, Michal and de Waard, Anita and Wang, Kuansan and Wang, Lucy Lu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Scholarly Document Processing (SDP) Workshop}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Overview of the Second Workshop on Scholarly Document Processing}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.sdp-1.22}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/a-dataset-of-information-seeking-questions-and-answers-anchored-in-research-papers-480.webp 480w,/assets/img/publication_preview/a-dataset-of-information-seeking-questions-and-answers-anchored-in-research-papers-800.webp 800w,/assets/img/publication_preview/a-dataset-of-information-seeking-questions-and-answers-anchored-in-research-papers-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/a-dataset-of-information-seeking-questions-and-answers-anchored-in-research-papers.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="a-dataset-of-information-seeking-questions-and-answers-anchored-in-research-papers.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dasigi-etal-2021-dataset" class="col-sm-8"> <div class="title">A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers</div> <div class="author"> Pradeep Dasigi,¬†<em>Kyle Lo</em>,¬†Iz Beltagy,¬†Arman Cohan,¬†Noah A. Smith,¬†and¬†Matt Gardner </div> <div class="periodical"> <em>In NAACL</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2021.naacl-main.365" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/2021.naacl-main.365" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/a-dataset-of-information-seeking-questions-and-answers-anchored-in-research-papers.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present Qasper, a dataset of 5049 questions over 1585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">dasigi-etal-2021-dataset</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A. and Gardner, Matt}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NAACL}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2021.naacl-main.365}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.naacl-main.365}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/overview-and-insights-from-the-sciver-shared-task-on-scientific-claim-verification-480.webp 480w,/assets/img/publication_preview/overview-and-insights-from-the-sciver-shared-task-on-scientific-claim-verification-800.webp 800w,/assets/img/publication_preview/overview-and-insights-from-the-sciver-shared-task-on-scientific-claim-verification-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/overview-and-insights-from-the-sciver-shared-task-on-scientific-claim-verification.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="overview-and-insights-from-the-sciver-shared-task-on-scientific-claim-verification.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wadden-lo-2021-overview" class="col-sm-8"> <div class="title">Overview and Insights from the SCIVER shared task on Scientific Claim Verification</div> <div class="author"> David Wadden,¬†and¬†<em>Kyle Lo</em> </div> <div class="periodical"> <em>In Scholarly Document Processing (SDP) Workshop</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2021.sdp-1.16" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/overview-and-insights-from-the-sciver-shared-task-on-scientific-claim-verification.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We present an overview of the SCIVER shared task, presented at the 2nd Scholarly Document Processing (SDP) workshop at NAACL 2021. In this shared task, systems were provided a scientific claim and a corpus of research abstracts, and asked to identify which articles Support or Refute the claim as well as provide evidentiary sentences justifying those labels. 11 teams made a total of 14 submissions to the shared task leaderboard, leading to an improvement of more than +23 F1 on the primary task evaluation metric. In addition to surveying the participating systems, we provide several insights into modeling approaches to support continued progress and future research on the important and challenging task of scientific claim verification.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wadden-lo-2021-overview</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wadden, David and Lo, Kyle}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Scholarly Document Processing (SDP) Workshop}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Overview and Insights from the {SCIVER} shared task on Scientific Claim Verification}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.sdp-1.16}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/augmenting-scientific-papers-with-just-in-time-position-sensitive-definitions-of-terms-and-symbols-480.webp 480w,/assets/img/publication_preview/augmenting-scientific-papers-with-just-in-time-position-sensitive-definitions-of-terms-and-symbols-800.webp 800w,/assets/img/publication_preview/augmenting-scientific-papers-with-just-in-time-position-sensitive-definitions-of-terms-and-symbols-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/augmenting-scientific-papers-with-just-in-time-position-sensitive-definitions-of-terms-and-symbols.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="augmenting-scientific-papers-with-just-in-time-position-sensitive-definitions-of-terms-and-symbols.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3411764.3445648" class="col-sm-8"> <div class="title">Augmenting Scientific Papers with Just-in-Time, Position-Sensitive Definitions of Terms and Symbols</div> <div class="author"> Andrew Head,¬†<em>Kyle Lo</em>,¬†Dongyeop Kang,¬†Raymond Fok,¬†Sam Skjonsberg,¬†Daniel S. Weld,¬†and¬†Marti A. Hearst </div> <div class="periodical"> <em>In CHI</em>, Yokohama, Japan, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3411764.3445648" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2009.14237" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://dl.acm.org/doi/abs/10.1145/3411764.3445648" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/augmenting-scientific-papers-with-just-in-time-position-sensitive-definitions-of-terms-and-symbols.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Despite the central importance of research papers to scientific progress, they can be difficult to read. Comprehension is often stymied when the information needed to understand a passage resides somewhere else‚Äîin another section, or in another paper. In this work, we envision how interfaces can bring definitions of technical terms and symbols to readers when and where they need them most. We introduce ScholarPhi, an augmented reading interface with four novel features: (1) tooltips that surface position-sensitive definitions from elsewhere in a paper, (2) a filter over the paper that ‚Äúdeclutters‚Äù it to reveal how the term or symbol is used across the paper, (3) automatic equation diagrams that expose multiple definitions in parallel, and (4) an automatically generated glossary of important terms and symbols. A usability study showed that the tool helps researchers of all experience levels read papers. Furthermore, researchers were eager to have ScholarPhi‚Äôs definitions available to support their everyday reading.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3411764.3445648</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Head, Andrew and Lo, Kyle and Kang, Dongyeop and Fok, Raymond and Skjonsberg, Sam and Weld, Daniel S. and Hearst, Marti A.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CHI}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3411764.3445648}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{18}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Augmenting Scientific Papers with Just-in-Time, Position-Sensitive Definitions of Terms and Symbols}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3411764.3445648}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/discourse-understanding-and-factual-consistency-in-abstractive-summarization-480.webp 480w,/assets/img/publication_preview/discourse-understanding-and-factual-consistency-in-abstractive-summarization-800.webp 800w,/assets/img/publication_preview/discourse-understanding-and-factual-consistency-in-abstractive-summarization-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/discourse-understanding-and-factual-consistency-in-abstractive-summarization.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="discourse-understanding-and-factual-consistency-in-abstractive-summarization.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gabriel-etal-2021-discourse" class="col-sm-8"> <div class="title">Discourse Understanding and Factual Consistency in Abstractive Summarization</div> <div class="author"> Saadia Gabriel,¬†Antoine Bosselut,¬†Jeff Da,¬†Ari Holtzman,¬†Jan Buys,¬†<em>Kyle Lo</em>,¬†Asli Celikyilmaz, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Yejin Choi' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">1 more author</span> </div> <div class="periodical"> <em>In EACL</em>, Apr 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2021.eacl-main.34" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/2021.eacl-main.34" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/discourse-understanding-and-factual-consistency-in-abstractive-summarization.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We introduce a general framework for abstractive summarization with factual consistency and distinct modeling of the narrative flow in an output summary. Our work addresses current limitations of models for abstractive summarization that often hallucinate information or generate summaries with coherence issues. To generate abstractive summaries with factual consistency and narrative flow, we propose Cooperative Generator-Discriminator Networks (Co-opNet), a novel transformer-based framework where the generator works with a discriminator architecture to compose coherent long-form summaries. We explore four different discriminator objectives which each capture a different aspect of coherence, including whether salient spans of generated abstracts are hallucinated or appear in the input context, and the likelihood of sentence adjacency in generated abstracts. We measure the ability of Co-opNet to learn these objectives with arXiv scientific papers, using the abstracts as a proxy for gold long-form scientific article summaries. Empirical results from automatic and human evaluations demonstrate that Co-opNet learns to summarize with considerably improved global coherence compared to competitive baselines.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gabriel-etal-2021-discourse</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gabriel, Saadia and Bosselut, Antoine and Da, Jeff and Holtzman, Ari and Buys, Jan and Lo, Kyle and Celikyilmaz, Asli and Choi, Yejin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{EACL}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2021.eacl-main.34}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Discourse Understanding and Factual Consistency in Abstractive Summarization}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.eacl-main.34}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/searching-for-scientific-evidence-in-a-pandemic-an-overview-of-trec-covid-480.webp 480w,/assets/img/publication_preview/searching-for-scientific-evidence-in-a-pandemic-an-overview-of-trec-covid-800.webp 800w,/assets/img/publication_preview/searching-for-scientific-evidence-in-a-pandemic-an-overview-of-trec-covid-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/searching-for-scientific-evidence-in-a-pandemic-an-overview-of-trec-covid.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="searching-for-scientific-evidence-in-a-pandemic-an-overview-of-trec-covid.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ROBERTS2021103865" class="col-sm-8"> <div class="title">Searching for scientific evidence in a pandemic: An overview of TREC-COVID</div> <div class="author"> Kirk Roberts,¬†Tasmeer Alam,¬†Steven Bedrick,¬†Dina Demner-Fushman,¬†<em>Kyle Lo</em>,¬†Ian Soboroff,¬†Ellen Voorhees, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Lucy Lu Wang, William R. Hersh' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">2 more authors</span> </div> <div class="periodical"> <em>Journal of Biomedical Informatics</em>, Apr 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1016/j.jbi.2021.103865" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2104.09632" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.sciencedirect.com/science/article/pii/S1532046421001945" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Science Direct</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/searching-for-scientific-evidence-in-a-pandemic-an-overview-of-trec-covid.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We present an overview of the TREC-COVID Challenge, an information retrieval (IR) shared task to evaluate search on scientific literature related to COVID-19. The goals of TREC-COVID include the construction of a pandemic search test collection and the evaluation of IR methods for COVID-19. The challenge was conducted over five rounds from April to July 2020, with participation from 92 unique teams and 556 individual submissions. A total of 50 topics (sets of related queries) were used in the evaluation, starting at 30 topics for Round 1 and adding 5 new topics per round to target emerging topics at that state of the still-emerging pandemic. This paper provides a comprehensive overview of the structure and results of TREC-COVID. Specifically, the paper provides details on the background, task structure, topic structure, corpus, participation, pooling, assessment, judgments, results, top-performing systems, lessons learned, and benchmark datasets.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ROBERTS2021103865</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Roberts, Kirk and Alam, Tasmeer and Bedrick, Steven and Demner-Fushman, Dina and Lo, Kyle and Soboroff, Ian and Voorhees, Ellen and Wang, Lucy Lu and Hersh, William R.}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.jbi.2021.103865}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Biomedical Informatics}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Searching for scientific evidence in a pandemic: An overview of TREC-COVID}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S1532046421001945}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{121}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/trec-covid-constructing-a-pandemic-information-retrieval-test-collection-480.webp 480w,/assets/img/publication_preview/trec-covid-constructing-a-pandemic-information-retrieval-test-collection-800.webp 800w,/assets/img/publication_preview/trec-covid-constructing-a-pandemic-information-retrieval-test-collection-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/trec-covid-constructing-a-pandemic-information-retrieval-test-collection.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="trec-covid-constructing-a-pandemic-information-retrieval-test-collection.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3451964.3451965" class="col-sm-8"> <div class="title">TREC-COVID: Constructing a Pandemic Information Retrieval Test Collection</div> <div class="author"> Ellen Voorhees,¬†Tasmeer Alam,¬†Steven Bedrick,¬†Dina Demner-Fushman,¬†William R. Hersh,¬†<em>Kyle Lo</em>,¬†Kirk Roberts, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Ian Soboroff, Lucy Lu Wang' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">2 more authors</span> </div> <div class="periodical"> <em>SIGIR Forum</em>, Feb 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3451964.3451965" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2005.04474" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://dl.acm.org/doi/abs/10.1145/3451964.3451965" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/trec-covid-constructing-a-pandemic-information-retrieval-test-collection.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>TREC-COVID is a community evaluation designed to build a test collection that captures the information needs of biomedical researchers using the scientific literature during a pandemic. One of the key characteristics of pandemic search is the accelerated rate of change: the topics of interest evolve as the pandemic progresses and the scientific literature in the area explodes. The COVID-19 pandemic provides an opportunity to capture this progression as it happens. TREC-COVID, in creating a test collection around COVID-19 literature, is building infrastructure to support new research and technologies in pandemic search.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1145/3451964.3451965</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Voorhees, Ellen and Alam, Tasmeer and Bedrick, Steven and Demner-Fushman, Dina and Hersh, William R. and Lo, Kyle and Roberts, Kirk and Soboroff, Ian and Wang, Lucy Lu}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3451964.3451965}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIGIR Forum}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TREC-COVID: Constructing a Pandemic Information Retrieval Test Collection}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3451964.3451965}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{54}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/text-mining-approaches-for-dealing-with-the-rapidly-expanding-literature-on-covid-19-480.webp 480w,/assets/img/publication_preview/text-mining-approaches-for-dealing-with-the-rapidly-expanding-literature-on-covid-19-800.webp 800w,/assets/img/publication_preview/text-mining-approaches-for-dealing-with-the-rapidly-expanding-literature-on-covid-19-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/text-mining-approaches-for-dealing-with-the-rapidly-expanding-literature-on-covid-19.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="text-mining-approaches-for-dealing-with-the-rapidly-expanding-literature-on-covid-19.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1093/bib/bbaa296" class="col-sm-8"> <div class="title">Text mining approaches for dealing with the rapidly expanding literature on COVID-19</div> <div class="author"> Lucy Lu Wang,¬†and¬†<em>Kyle Lo</em> </div> <div class="periodical"> <em>Briefings in Bioinformatics</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1093/bib/bbaa296" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://openreview.net/forum?id=exBoxQwNnY2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">OpenReview</a> <a href="https://academic.oup.com/bib/article/22/2/781/6024738" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Oxford University Press</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/text-mining-approaches-for-dealing-with-the-rapidly-expanding-literature-on-covid-19.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>More than 50 000 papers have been published about COVID-19 since the beginning of 2020 and several hundred new papers continue to be published every day. This incredible rate of scientific productivity leads to information overload, making it difficult for researchers, clinicians and public health officials to keep up with the latest findings. Automated text mining techniques for searching, reading and summarizing papers are helpful for addressing information overload. In this review, we describe the many resources that have been introduced to support text mining applications over the COVID-19 literature; specifically, we discuss the corpora, modeling resources, systems and shared tasks that have been introduced for COVID-19. We compile a list of 39 systems that provide functionality such as search, discovery, visualization and summarization over the COVID-19 literature. For each system, we provide a qualitative description and assessment of the system‚Äôs performance, unique data or user interface features and modeling decisions. Many systems focus on search and discovery, though several systems provide novel features, such as the ability to summarize findings over multiple documents or linking between scientific articles and clinical trials. We also describe the public corpora, models and shared tasks that have been introduced to help reduce repeated effort among community members; some of these resources (especially shared tasks) can provide a basis for comparing the performance of different systems. Finally, we summarize promising results and open challenges for text mining the COVID-19 literature.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1093/bib/bbaa296</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Lucy Lu and Lo, Kyle}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1093/bib/bbaa296}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://academic.oup.com/bib/article-pdf/22/2/781/36654452/bbaa296.pdf}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Briefings in Bioinformatics}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Text mining approaches for dealing with the rapidly expanding literature on COVID-19}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1093/bib/bbaa296}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mitigating-biases-in-cord-19-for-analyzing-covid-19-literature-480.webp 480w,/assets/img/publication_preview/mitigating-biases-in-cord-19-for-analyzing-covid-19-literature-800.webp 800w,/assets/img/publication_preview/mitigating-biases-in-cord-19-for-analyzing-covid-19-literature-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/mitigating-biases-in-cord-19-for-analyzing-covid-19-literature.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mitigating-biases-in-cord-19-for-analyzing-covid-19-literature.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.3389/frma.2020.596624" class="col-sm-8"> <div class="title">Mitigating Biases in CORD-19 for Analyzing COVID-19 Literature</div> <div class="author"> Anshul Kanakia,¬†Kuansan Wang,¬†Yuxiao Dong,¬†Boya Xie,¬†<em>Kyle Lo</em>,¬†Zhihong Shen,¬†Lucy Lu Wang, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Chiyuan Huang, Darrin Eide, Sebastian Kohlmeier, Chieh-Han Wu' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">4 more authors</span> </div> <div class="periodical"> <em>Frontiers in Research Metrics and Analytics</em>, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3389/frma.2020.596624" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8025972" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PubMed Central</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/mitigating-biases-in-cord-19-for-analyzing-covid-19-literature.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>On the behest of the Office of Science and Technology Policy in the White House, six institutions, including ours, have created an open research dataset called COVID-19 Research Dataset (CORD-19) to facilitate the development of question-answering systems that can assist researchers in finding relevant research on COVID-19. As of May 27, 2020, CORD-19 includes more than 100,000 open access publications from major publishers and PubMed as well as preprint articles deposited into medRxiv, bioRxiv, and arXiv. Recent years, however, have also seen question-answering and other machine learning systems exhibit harmful behaviors to humans due to biases in the training data. It is imperative and only ethical for modern scientists to be vigilant in inspecting and be prepared to mitigate the potential biases when working with any datasets. This article describes a framework to examine biases in scientific document collections like CORD-19 by comparing their properties with those derived from the citation behaviors of the entire scientific community. In total, three expanded sets are created for the analyses: 1) the enclosure set CORD-19E composed of CORD-19 articles and their references and citations, mirroring the methodology used in the renowned ‚ÄúA Century of Physics‚Äù analysis; 2) the full closure graph CORD-19C that recursively includes references starting with CORD-19; and 3) the inflection closure CORD-19I, that is, a much smaller subset of CORD-19C but already appropriate for statistical analysis based on the theory of the scale-free nature of the citation network. Taken together, all these expanded datasets show much smoother trends when used to analyze global COVID-19 research. The results suggest that while CORD-19 exhibits a strong tilt toward recent and topically focused articles, the knowledge being explored to attack the pandemic encompasses a much longer time span and is very interdisciplinary. A question-answering system with such expanded scope of knowledge may perform better in understanding the literature and answering related questions. However, while CORD-19 appears to have topical coverage biases compared to the expanded sets, the collaboration patterns, especially in terms of team sizes and geographical distributions, are captured very well already in CORD-19 as the raw statistics and trends agree with those from larger datasets.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.3389/frma.2020.596624</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kanakia, Anshul and Wang, Kuansan and Dong, Yuxiao and Xie, Boya and Lo, Kyle and Shen, Zhihong and Wang, Lucy Lu and Huang, Chiyuan and Eide, Darrin and Kohlmeier, Sebastian and Wu, Chieh-Han}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3389/frma.2020.596624}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Frontiers in Research Metrics and Analytics}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mitigating Biases in CORD-19 for Analyzing COVID-19 Literature}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.frontiersin.org/articles/10.3389/frma.2020.596624}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tldr-extreme-summarization-of-scientific-documents-480.webp 480w,/assets/img/publication_preview/tldr-extreme-summarization-of-scientific-documents-800.webp 800w,/assets/img/publication_preview/tldr-extreme-summarization-of-scientific-documents-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/tldr-extreme-summarization-of-scientific-documents.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tldr-extreme-summarization-of-scientific-documents.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cachola-etal-2020-tldr" class="col-sm-8"> <div class="title">TLDR: Extreme Summarization of Scientific Documents</div> <div class="author"> Isabel Cachola,¬†<em>Kyle Lo</em>,¬†Arman Cohan,¬†and¬†Daniel Weld </div> <div class="periodical"> <em>In Findings of EMNLP</em>, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2020.findings-emnlp.428" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/2020.findings-emnlp.428" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/tldr-extreme-summarization-of-scientific-documents.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We introduce TLDR generation, a new form of extreme summarization, for scientific papers. TLDR generation involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. To facilitate study on this task, we introduce SCITLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SCITLDR contains both author-written and expert-derived TLDRs, where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden. We propose CATTS, a simple yet effective learning strategy for generating TLDRs that exploits titles as an auxiliary training signal. CATTS improves upon strong baselines under both automated metrics and human evaluations. Data and code are publicly available at https://github.com/allenai/scitldr.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cachola-etal-2020-tldr</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cachola, Isabel and Lo, Kyle and Cohan, Arman and Weld, Daniel}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of EMNLP}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2020.findings-emnlp.428}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{TLDR}: Extreme Summarization of Scientific Documents}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.findings-emnlp.428}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/document-level-definition-detection-in-scholarly-documents-existing-models-error-analyses-and-future-directions-480.webp 480w,/assets/img/publication_preview/document-level-definition-detection-in-scholarly-documents-existing-models-error-analyses-and-future-directions-800.webp 800w,/assets/img/publication_preview/document-level-definition-detection-in-scholarly-documents-existing-models-error-analyses-and-future-directions-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/document-level-definition-detection-in-scholarly-documents-existing-models-error-analyses-and-future-directions.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="document-level-definition-detection-in-scholarly-documents-existing-models-error-analyses-and-future-directions.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kang-etal-2020-document" class="col-sm-8"> <div class="title">Document-Level Definition Detection in Scholarly Documents: Existing Models, Error Analyses, and Future Directions</div> <div class="author"> Dongyeop Kang,¬†Andrew Head,¬†Risham Sidhu,¬†<em>Kyle Lo</em>,¬†Daniel Weld,¬†and¬†Marti A. Hearst </div> <div class="periodical"> <em>In Scholarly Document Processing (SDP) Workshop</em>, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2020.sdp-1.22" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/2020.sdp-1.22" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/document-level-definition-detection-in-scholarly-documents-existing-models-error-analyses-and-future-directions.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The task of definition detection is important for scholarly papers, because papers often make use of technical terminology that may be unfamiliar to readers. Despite prior work on definition detection, current approaches are far from being accurate enough to use in realworld applications. In this paper, we first perform in-depth error analysis of the current best performing definition detection system and discover major causes of errors. Based on this analysis, we develop a new definition detection system, HEDDEx, that utilizes syntactic features, transformer encoders, and heuristic filters, and evaluate it on a standard sentence-level benchmark. Because current benchmarks evaluate randomly sampled sentences, we propose an alternative evaluation that assesses every sentence within a document. This allows for evaluating recall in addition to precision. HEDDEx outperforms the leading system on both the sentence-level and the document-level tasks, by 12.7 F1 points and 14.4 F1 points, respectively. We note that performance on the high-recall document-level task is much lower than in the standard evaluation approach, due to the necessity of incorporation of document structure as features. We discuss remaining challenges in document-level definition detection, ideas for improvements, and potential issues for the development of reading aid applications.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kang-etal-2020-document</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kang, Dongyeop and Head, Andrew and Sidhu, Risham and Lo, Kyle and Weld, Daniel and Hearst, Marti A.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Scholarly Document Processing (SDP) Workshop}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2020.sdp-1.22}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Document-Level Definition Detection in Scholarly Documents: Existing Models, Error Analyses, and Future Directions}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.sdp-1.22}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fact-or-fiction-verifying-scientific-claims-480.webp 480w,/assets/img/publication_preview/fact-or-fiction-verifying-scientific-claims-800.webp 800w,/assets/img/publication_preview/fact-or-fiction-verifying-scientific-claims-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/fact-or-fiction-verifying-scientific-claims.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fact-or-fiction-verifying-scientific-claims.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wadden-etal-2020-fact" class="col-sm-8"> <div class="title">Fact or Fiction: Verifying Scientific Claims</div> <div class="author"> David Wadden,¬†Shanchuan Lin,¬†<em>Kyle Lo</em>,¬†Lucy Lu Wang,¬†Madeleine Zuylen,¬†Arman Cohan,¬†and¬†Hannaneh Hajishirzi </div> <div class="periodical"> <em>In EMNLP</em>, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2020.emnlp-main.609" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/2020.emnlp-main.609" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/fact-or-fiction-verifying-scientific-claims.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https://github.com/allenai/scifact. A leaderboard and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wadden-etal-2020-fact</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wadden, David and Lin, Shanchuan and Lo, Kyle and Wang, Lucy Lu and van Zuylen, Madeleine and Cohan, Arman and Hajishirzi, Hannaneh}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{EMNLP}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2020.emnlp-main.609}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fact or Fiction: Verifying Scientific Claims}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.emnlp-main.609}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/trec-covid-rationale-and-structure-of-an-information-retrieval-shared-task-for-covid-19-480.webp 480w,/assets/img/publication_preview/trec-covid-rationale-and-structure-of-an-information-retrieval-shared-task-for-covid-19-800.webp 800w,/assets/img/publication_preview/trec-covid-rationale-and-structure-of-an-information-retrieval-shared-task-for-covid-19-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/trec-covid-rationale-and-structure-of-an-information-retrieval-shared-task-for-covid-19.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="trec-covid-rationale-and-structure-of-an-information-retrieval-shared-task-for-covid-19.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1093/jamia/ocaa091" class="col-sm-8"> <div class="title">TREC-COVID: rationale and structure of an information retrieval shared task for COVID-19</div> <div class="author"> Kirk Roberts,¬†Tasmeer Alam,¬†Steven Bedrick,¬†Dina Demner-Fushman,¬†<em>Kyle Lo</em>,¬†Ian Soboroff,¬†Ellen Voorhees, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Lucy Lu Wang, William R Hersh' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">2 more authors</span> </div> <div class="periodical"> <em>Journal of the American Medical Informatics Association</em>, Jul 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1093/jamia/ocaa091" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7239098" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PubMed Central</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/trec-covid-rationale-and-structure-of-an-information-retrieval-shared-task-for-covid-19.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>TREC-COVID is an information retrieval (IR) shared task initiated to support clinicians and clinical research during the COVID-19 pandemic. IR for pandemics breaks many normal assumptions, which can be seen by examining 9 important basic IR research questions related to pandemic situations. TREC-COVID differs from traditional IR shared task evaluations with special considerations for the expected users, IR modality considerations, topic development, participant requirements, assessment process, relevance criteria, evaluation metrics, iteration process, projected timeline, and the implications of data use as a post-task test collection. This article describes how all these were addressed for the particular requirements of developing IR systems under a pandemic situation. Finally, initial participation numbers are also provided, which demonstrate the tremendous interest the IR community has in this effort.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1093/jamia/ocaa091</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Roberts, Kirk and Alam, Tasmeer and Bedrick, Steven and Demner-Fushman, Dina and Lo, Kyle and Soboroff, Ian and Voorhees, Ellen and Wang, Lucy Lu and Hersh, William R}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1093/jamia/ocaa091}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of the American Medical Informatics Association}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{TREC-COVID: rationale and structure of an information retrieval shared task for COVID-19}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1093/jamia/ocaa091}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{27}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dont-stop-pretraining-adapt-language-models-to-domains-and-tasks-480.webp 480w,/assets/img/publication_preview/dont-stop-pretraining-adapt-language-models-to-domains-and-tasks-800.webp 800w,/assets/img/publication_preview/dont-stop-pretraining-adapt-language-models-to-domains-and-tasks-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/dont-stop-pretraining-adapt-language-models-to-domains-and-tasks.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dont-stop-pretraining-adapt-language-models-to-domains-and-tasks.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gururangan-etal-2020-dont" class="col-sm-8"> <div class="title">Don‚Äôt Stop Pretraining: Adapt Language Models to Domains and Tasks</div> <div class="author"> Suchin Gururangan,¬†Ana Marasoviƒá,¬†Swabha Swayamdipta,¬†<em>Kyle Lo</em>,¬†Iz Beltagy,¬†Doug Downey,¬†and¬†Noah A. Smith </div> <div class="periodical"> <em>In ACL</em>, Jul 2020 </div> <div class="periodical"> </div> <div class="links"> <div>üèÜ <strong>Honorable Mention for Best Paper</strong> üèÜ</div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2020.acl-main.740" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/2020.acl-main.740" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/dont-stop-pretraining-adapt-language-models-to-domains-and-tasks.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Honorable Mention for Best Paper</p> </div> <div class="abstract hidden"> <p>Language models pretrained on text from a wide variety of sources form the foundation of today‚Äôs NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task‚Äôs unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gururangan-etal-2020-dont</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACL}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2020.acl-main.740}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.acl-main.740}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/s2orc-the-semantic-scholar-open-research-corpus-480.webp 480w,/assets/img/publication_preview/s2orc-the-semantic-scholar-open-research-corpus-800.webp 800w,/assets/img/publication_preview/s2orc-the-semantic-scholar-open-research-corpus-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/s2orc-the-semantic-scholar-open-research-corpus.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="s2orc-the-semantic-scholar-open-research-corpus.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lo-etal-2020-s2orc" class="col-sm-8"> <div class="title">S2ORC: The Semantic Scholar Open Research Corpus</div> <div class="author"> <em>Kyle Lo</em>,¬†Lucy Lu Wang,¬†Mark Neumann,¬†Rodney Kinney,¬†and¬†Daniel Weld </div> <div class="periodical"> <em>In ACL</em>, Jul 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2020.acl-main.447" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/2020.acl-main.447" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/s2orc-the-semantic-scholar-open-research-corpus.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lo-etal-2020-s2orc</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lo, Kyle and Wang, Lucy Lu and Neumann, Mark and Kinney, Rodney and Weld, Daniel}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACL}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2020.acl-main.447}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{S}2{ORC}: The Semantic Scholar Open Research Corpus}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.acl-main.447}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cord-19-the-covid-19-open-research-dataset-480.webp 480w,/assets/img/publication_preview/cord-19-the-covid-19-open-research-dataset-800.webp 800w,/assets/img/publication_preview/cord-19-the-covid-19-open-research-dataset-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/cord-19-the-covid-19-open-research-dataset.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cord-19-the-covid-19-open-research-dataset.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang-etal-2020-cord" class="col-sm-8"> <div class="title">CORD-19: The COVID-19 Open Research Dataset</div> <div class="author"> Lucy Lu Wang,¬†<em>Kyle Lo</em>,¬†Yoganand Chandrasekhar,¬†Russell Reas,¬†Jiangjiang Yang,¬†Doug Burdick,¬†Darrin Eide, and <span class="more-authors" title="click to view 21 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '21 more authors' ? 'Kathryn Funk, Yannis Katsis, Rodney Michael Kinney, Yunyao Li, Ziyang Liu, William Merrill, Paul Mooney, Dewey A. Murdick, Devvret Rishi, Jerry Sheehan, Zhihong Shen, Brandon Stilson, Alex D. Wade, Kuansan Wang, Nancy Xin Ru Wang, Christopher Wilhelm, Boya Xie, Douglas M. Raymond, Daniel S. Weld, Oren Etzioni, Sebastian Kohlmeier' : '21 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">21 more authors</span> </div> <div class="periodical"> <em>In NLP for COVID-19 Workshop</em>, Jul 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2020.nlpcovid19-acl.1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/cord-19-the-covid-19-open-research-dataset.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang-etal-2020-cord</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Lucy Lu and Lo, Kyle and Chandrasekhar, Yoganand and Reas, Russell and Yang, Jiangjiang and Burdick, Doug and Eide, Darrin and Funk, Kathryn and Katsis, Yannis and Kinney, Rodney Michael and Li, Yunyao and Liu, Ziyang and Merrill, William and Mooney, Paul and Murdick, Dewey A. and Rishi, Devvret and Sheehan, Jerry and Shen, Zhihong and Stilson, Brandon and Wade, Alex D. and Wang, Kuansan and Wang, Nancy Xin Ru and Wilhelm, Christopher and Xie, Boya and Raymond, Douglas M. and Weld, Daniel S. and Etzioni, Oren and Kohlmeier, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NLP for COVID-19 Workshop}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{CORD-19}: The {COVID-19} Open Research Dataset}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.nlpcovid19-acl.1}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/scibert-a-pretrained-language-model-for-scientific-text-480.webp 480w,/assets/img/publication_preview/scibert-a-pretrained-language-model-for-scientific-text-800.webp 800w,/assets/img/publication_preview/scibert-a-pretrained-language-model-for-scientific-text-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/scibert-a-pretrained-language-model-for-scientific-text.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="scibert-a-pretrained-language-model-for-scientific-text.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="beltagy-etal-2019-scibert" class="col-sm-8"> <div class="title">SciBERT: A Pretrained Language Model for Scientific Text</div> <div class="author"> Iz Beltagy,¬†<em>Kyle Lo</em>,¬†and¬†Arman Cohan </div> <div class="periodical"> <em>In EMNLP</em>, Nov 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/D19-1371" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/D19-1371" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/scibert-a-pretrained-language-model-for-scientific-text.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">beltagy-etal-2019-scibert</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Beltagy, Iz and Lo, Kyle and Cohan, Arman}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{EMNLP}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/D19-1371}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{S}ci{BERT}: A Pretrained Language Model for Scientific Text}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/D19-1371}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/quantifying-sex-bias-in-clinical-studies-at-scale-with-automated-data-extraction-480.webp 480w,/assets/img/publication_preview/quantifying-sex-bias-in-clinical-studies-at-scale-with-automated-data-extraction-800.webp 800w,/assets/img/publication_preview/quantifying-sex-bias-in-clinical-studies-at-scale-with-automated-data-extraction-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/quantifying-sex-bias-in-clinical-studies-at-scale-with-automated-data-extraction.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="quantifying-sex-bias-in-clinical-studies-at-scale-with-automated-data-extraction.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1001/jamanetworkopen.2019.6700" class="col-sm-8"> <div class="title">Quantifying Sex Bias in Clinical Studies at Scale With Automated Data Extraction</div> <div class="author"> Sergey Feldman,¬†Waleed Ammar,¬†<em>Kyle Lo</em>,¬†Elly Trepman,¬†Madeleine Zuylen,¬†and¬†Oren Etzioni </div> <div class="periodical"> <em>JAMA Network Open</em>, Jul 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1001/jamanetworkopen.2019.6700" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6613296" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PubMed Central</a> <a href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2737103" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">JAMA</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/quantifying-sex-bias-in-clinical-studies-at-scale-with-automated-data-extraction.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Analyses of female representation in clinical studies have been limited in scope and scale.To perform a large-scale analysis of global enrollment sex bias in clinical studies.In this cross-sectional study, clinical studies from published articles from PubMed from 1966 to 2018 and records from Aggregate Analysis of ClinicalTrials.gov from 1999 to 2018 were identified. Global disease prevalence was determined for male and female patients in 11 disease categories from the Global Burden of Disease database: cardiovascular, diabetes, digestive, hepatitis (types A, B, C, and E), HIV/AIDS, kidney (chronic), mental, musculoskeletal, neoplasms, neurological, and respiratory (chronic). Machine reading algorithms were developed that extracted sex data from tables in articles and records on December 31, 2018, at an artificial intelligence research institute. Male and female participants in 43‚ÄØ135 articles (792‚ÄØ004‚ÄØ915 participants) and 13‚ÄØ165 records (12‚ÄØ977‚ÄØ103 participants) were included.Sex bias was defined as the difference between the fraction of female participants in study participants minus prevalence fraction of female participants for each disease category. A total of 1000 bootstrap estimates of sex bias were computed by resampling individual studies with replacement. Sex bias was reported as mean and 95\% bootstrap confidence intervals from articles and records in each disease category over time (before or during 1993 to 2018), with studies or participants as the measurement unit.There were 792‚ÄØ004‚ÄØ915 participants, including 390‚ÄØ470‚ÄØ834 female participants (49\%), in articles and 12‚ÄØ977‚ÄØ103 participants, including 6‚ÄØ351‚ÄØ619 female participants (49\%), in records. With studies as measurement unit, substantial female underrepresentation (sex bias‚Äâ‚â§‚Äâ‚àí0.05) was observed in 7 of 11 disease categories, especially HIV/AIDS (mean for articles, ‚àí0.17 [95\% CI, ‚àí0.18 to ‚àí0.16]), chronic kidney diseases (mean, ‚àí0.17 [95\% CI, ‚àí0.17 to ‚àí0.16]), and cardiovascular diseases (mean, ‚àí0.14 [95\% CI, ‚àí0.14 to ‚àí0.13]). Sex bias in articles for all categories combined was unchanged over time with studies as measurement unit (range, ‚àí0.15 [95\% CI, ‚àí0.16 to ‚àí0.13] to ‚àí0.10 [95\% CI, ‚àí0.14 to ‚àí0.06]), but improved from before or during 1993 (mean, ‚àí0.11 [95\% CI, ‚àí0.16 to ‚àí0.05]) to 2014 to 2018 (mean, ‚àí0.05 [95\% CI, ‚àí0.09 to ‚àí0.02]) with participants as the measurement unit. Larger study size was associated with greater female representation.Automated extraction of the number of participants in clinical reports provides an effective alternative to manual analysis of demographic bias. Despite legal and policy initiatives to increase female representation, sex bias against female participants in clinical studies persists. Studies with more participants have greater female representation. Differences between sex bias estimates with studies vs participants as measurement unit, and between articles vs records, suggest that sex bias with both measures and data sources should be reported.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1001/jamanetworkopen.2019.6700</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Feldman, Sergey and Ammar, Waleed and Lo, Kyle and Trepman, Elly and van Zuylen, Madeleine and Etzioni, Oren}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1001/jamanetworkopen.2019.6700}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2737103/feldman\_2019\_oi\_190268.pdf}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{JAMA Network Open}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Quantifying Sex Bias in Clinical Studies at Scale With Automated Data Extraction}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1001/jamanetworkopen.2019.6700}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/combining-distant-and-direct-supervision-for-neural-relation-extraction-480.webp 480w,/assets/img/publication_preview/combining-distant-and-direct-supervision-for-neural-relation-extraction-800.webp 800w,/assets/img/publication_preview/combining-distant-and-direct-supervision-for-neural-relation-extraction-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/combining-distant-and-direct-supervision-for-neural-relation-extraction.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="combining-distant-and-direct-supervision-for-neural-relation-extraction.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="beltagy-etal-2019-combining" class="col-sm-8"> <div class="title">Combining Distant and Direct Supervision for Neural Relation Extraction</div> <div class="author"> Iz Beltagy,¬†<em>Kyle Lo</em>,¬†and¬†Waleed Ammar </div> <div class="periodical"> <em>In NAACL</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/N19-1184" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/N19-1184" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/combining-distant-and-direct-supervision-for-neural-relation-extraction.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In relation extraction with distant supervision, noisy labels make it difficult to train quality models. Previous neural models addressed this problem using an attention mechanism that attends to sentences that are likely to express the relations. We improve such models by combining the distant supervision data with an additional directly-supervised data, which we use as supervision for the attention weights. We find that joint training on both types of supervision leads to a better model because it improves the model‚Äôs ability to identify noisy sentences. In addition, we find that sigmoidal attention weights with max pooling achieves better performance over the commonly used weighted average attention in this setup. Our proposed method achieves a new state-of-the-art result on the widely used FB-NYT dataset.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">beltagy-etal-2019-combining</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Beltagy, Iz and Lo, Kyle and Ammar, Waleed}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NAACL}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/N19-1184}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Combining Distant and Direct Supervision for Neural Relation Extraction}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/N19-1184}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ontology-alignment-in-the-biomedical-domain-using-entity-definitions-and-context-480.webp 480w,/assets/img/publication_preview/ontology-alignment-in-the-biomedical-domain-using-entity-definitions-and-context-800.webp 800w,/assets/img/publication_preview/ontology-alignment-in-the-biomedical-domain-using-entity-definitions-and-context-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/ontology-alignment-in-the-biomedical-domain-using-entity-definitions-and-context.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ontology-alignment-in-the-biomedical-domain-using-entity-definitions-and-context.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang-etal-2018-ontology" class="col-sm-8"> <div class="title">Ontology alignment in the biomedical domain using entity definitions and context</div> <div class="author"> Lucy Lu Wang,¬†Chandra Bhagavatula,¬†Mark Neumann,¬†<em>Kyle Lo</em>,¬†Chris Wilhelm,¬†and¬†Waleed Ammar </div> <div class="periodical"> <em>In BioNLP Workshop</em>, Jul 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/W18-2306" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/W18-2306" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/ontology-alignment-in-the-biomedical-domain-using-entity-definitions-and-context.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Ontology alignment is the task of identifying semantically equivalent entities from two given ontologies. Different ontologies have different representations of the same entity, resulting in a need to de-duplicate entities when merging ontologies. We propose a method for enriching entities in an ontology with external definition and context information, and use this additional information for ontology alignment. We develop a neural architecture capable of encoding the additional information when available, and show that the addition of external data results in an F1-score of 0.69 on the Ontology Alignment Evaluation Initiative (OAEI) largebio SNOMED-NCI subtask, comparable with the entity-level matchers in a SOTA system.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang-etal-2018-ontology</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Lucy Lu and Bhagavatula, Chandra and Neumann, Mark and Lo, Kyle and Wilhelm, Chris and Ammar, Waleed}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{BioNLP Workshop}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/W18-2306}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Ontology alignment in the biomedical domain using entity definitions and context}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/W18-2306}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/construction-of-the-literature-graph-in-semantic-scholar-480.webp 480w,/assets/img/publication_preview/construction-of-the-literature-graph-in-semantic-scholar-800.webp 800w,/assets/img/publication_preview/construction-of-the-literature-graph-in-semantic-scholar-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/construction-of-the-literature-graph-in-semantic-scholar.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="construction-of-the-literature-graph-in-semantic-scholar.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ammar-etal-2018-construction" class="col-sm-8"> <div class="title">Construction of the Literature Graph in Semantic Scholar</div> <div class="author"> Waleed Ammar,¬†Dirk Groeneveld,¬†Chandra Bhagavatula,¬†Iz Beltagy,¬†Miles Crawford,¬†Doug Downey,¬†Jason Dunkelberger, and <span class="more-authors" title="click to view 16 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '16 more authors' ? 'Ahmed Elgohary, Sergey Feldman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew Peters, Joanna Power, Sam Skjonsberg, Lucy Lu Wang, Chris Wilhelm, Zheng Yuan, Madeleine Zuylen, Oren Etzioni' : '16 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">16 more authors</span> </div> <div class="periodical"> <em>In NAACL</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/N18-3011" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://aclanthology.org/N18-3011" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/construction-of-the-literature-graph-in-semantic-scholar.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We describe a deployed scalable system for organizing published scientific literature into a heterogeneous graph to facilitate algorithmic manipulation and discovery. The resulting literature graph consists of more than 280M nodes, representing papers, authors, entities and various interactions between them (e.g., authorships, citations, entity mentions). We reduce literature graph construction into familiar NLP tasks (e.g., entity extraction and linking), point out research challenges due to differences from standard formulations of these tasks, and report empirical results for each task. The methods described in this paper are used to enable semantic features in \urlwww.semanticscholar.org.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ammar-etal-2018-construction</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ammar, Waleed and Groeneveld, Dirk and Bhagavatula, Chandra and Beltagy, Iz and Crawford, Miles and Downey, Doug and Dunkelberger, Jason and Elgohary, Ahmed and Feldman, Sergey and Ha, Vu and Kinney, Rodney and Kohlmeier, Sebastian and Lo, Kyle and Murray, Tyler and Ooi, Hsu-Han and Peters, Matthew and Power, Joanna and Skjonsberg, Sam and Wang, Lucy Lu and Wilhelm, Chris and Yuan, Zheng and van Zuylen, Madeleine and Etzioni, Oren}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NAACL}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/N18-3011}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Construction of the Literature Graph in Semantic Scholar}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/N18-3011}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Kyle Lo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. # Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: November 07, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>