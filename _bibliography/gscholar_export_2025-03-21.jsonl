{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SciBERT: A Pretrained Language Model for Scientific Text", "pub_year": 2019, "citation": "EMNLP 2019, 2019", "author": "Iz Beltagy and Kyle Lo and Arman Cohan", "conference": "EMNLP 2019", "abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:W7OEmFMy1HYC", "num_citations": 4309, "citedby_url": "/scholar?hl=en&cites=7377999893003631695,8948928912554461294,2530255820854385096,16208263907124402041,15868115782550457562,11363894853860708432,10641624047062794448,12170462852891451975,7385411267403323657,1045678360199031527", "cites_id": ["7377999893003631695", "8948928912554461294", "2530255820854385096", "16208263907124402041", "15868115782550457562", "11363894853860708432", "10641624047062794448", "12170462852891451975", "7385411267403323657", "1045678360199031527"], "pub_url": "https://arxiv.org/abs/1903.10676", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:T_xZ98_rY2YJ:scholar.google.com/", "cites_per_year": {"2019": 78, "2020": 347, "2021": 696, "2022": 858, "2023": 1029, "2024": 1095, "2025": 184}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "pub_year": 2020, "citation": "ACL 2020 (\ud83c\udfc6 Honorable Mention for Best Paper \ud83c\udfc6 ), 2020", "author": "Suchin Gururangan and Ana Marasovi\u0107 and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A Smith", "conference": "ACL 2020 (\ud83c\udfc6 Honorable Mention for Best Paper \ud83c\udfc6 )", "abstract": "Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:_FxGoFyzp5QC", "num_citations": 2596, "citedby_url": "/scholar?hl=en&cites=12296599237256723847,3296878568424287597,5362649960191349588,8004125482506149725,13975711283835631488,438998881975811067,7974665943775189436,14937423243806422153,9362196928868621776,3432915160153638793", "cites_id": ["12296599237256723847", "3296878568424287597", "5362649960191349588", "8004125482506149725", "13975711283835631488", "438998881975811067", "7974665943775189436", "14937423243806422153", "9362196928868621776", "3432915160153638793"], "pub_url": "https://arxiv.org/abs/2004.10964", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:h70GKttLpqoJ:scholar.google.com/", "cites_per_year": {"2020": 132, "2021": 417, "2022": 590, "2023": 664, "2024": 677, "2025": 109}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Bloom: A 176b-parameter open-access multilingual language model", "pub_year": 2023, "citation": "", "author": "Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ili\u0107 and Daniel Hesslow and Roman Castagn\u00e9 and Alexandra Sasha Luccioni and Fran\u00e7ois Yvon and Matthias Gall\u00e9 and Jonathan Tow and Alexander M Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Beno\u00eet Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Lauren\u00e7on and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris Emezue and Christopher Klamm and Colin Leong and Daniel van Strien and David Ifeoluwa Adelani and Dragomir Radev and Eduardo Gonz\u00e1lez Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Bar Natan and Francesco De Toni and G\u00e9rard Dupont and Germ\u00e1n Kruszewski and Giada Pistilli and Hady Elsahar and Hamza Benyamina and Hieu Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier De la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and J\u00f6rg Frohberg and Joseph Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro Von Werra and Leon Weber and Long Phan and Loubna Ben Allal and Ludovic Tanguy and Manan Dey and Manuel Romero Mu\u00f1oz and Maraim Masoud and Mar\u00eda Grandury and Mario \u0160a\u0161ko and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Mohammad A Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto Luis L\u00f3pez and Rui Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma and Shayne Longpre and Somaieh Nikpoor and Stanislav Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Elizabeth Salesky and Sabrina J Mielke and Wilson Y Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S Al-Shaibani", "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:M3NEmzRMIkIC", "num_citations": 1824, "citedby_url": "/scholar?hl=en&cites=13592851130575865074,8671612866653391361,4189196902121773568", "cites_id": ["13592851130575865074", "8671612866653391361", "4189196902121773568"], "pub_url": "https://inria.hal.science/hal-03850124/", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:8sS90hWCo7wJ:scholar.google.com/", "cites_per_year": {"2022": 31, "2023": 805, "2024": 840, "2025": 143}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "CORD-19: The Covid-19 Open Research Dataset", "pub_year": 2020, "citation": "arXiv preprint arXiv:2004.10706, 2020", "author": "Lucy Lu Wang and Kyle Lo and Yoganand Chandrasekhar and Russell Reas and Jiangjiang Yang and Darrin Eide and Kathryn Funk and Rodney Kinney and Ziyang Liu and William Merrill and Paul Mooney and Dewey Murdick and Devvret Rishi and Jerry Sheehan and Zhihong Shen and Brandon Stilson and Alex D Wade and Kuansan Wang and Chris Wilhelm and Boya Xie and Douglas Raymond and Daniel S Weld and Oren Etzioni and Sebastian Kohlmeier", "journal": "arXiv preprint arXiv:2004.10706", "abstract": "The Covid-19 Open Research Dataset (CORD-19) is a growing1 resource of scientific papers on Covid-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded2 over 200K times and has served as the basis of many Covid-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for Covid-19."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:LkGwnXOMwfcC", "num_citations": 1061, "citedby_url": "/scholar?hl=en&cites=14379169419409422584,10654639097966638285,3418208377074584981,18236393307117157986,6652646185330643844,13446011727263640418,2824674034117033481,6745556213959816591,4814810034739280728", "cites_id": ["14379169419409422584", "10654639097966638285", "3418208377074584981", "18236393307117157986", "6652646185330643844", "13446011727263640418", "2824674034117033481", "6745556213959816591", "4814810034739280728"], "pub_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7251955/", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:-PgOIWcSjccJ:scholar.google.com/", "cites_per_year": {"2020": 144, "2021": 309, "2022": 237, "2023": 197, "2024": 145, "2025": 20}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "S2orc: The semantic scholar open research corpus", "pub_year": 2020, "citation": "Proceedings of ACL, 2020", "author": "Kyle Lo and Lucy Lu Wang and Mark Neumann and Rodney Kinney and Daniel S Weld", "journal": "Proceedings of ACL", "abstract": "We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:hqOjcs7Dif8C", "num_citations": 669, "citedby_url": "/scholar?hl=en&cites=11978464475399626925,14639928947051144188,17349459575173380681", "cites_id": ["11978464475399626925", "14639928947051144188", "17349459575173380681"], "pub_url": "https://arxiv.org/abs/1911.02782", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:raDUyvkNPKYJ:scholar.google.com/", "cites_per_year": {"2020": 37, "2021": 92, "2022": 112, "2023": 172, "2024": 211, "2025": 40}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Construction of the literature graph in semantic scholar", "pub_year": 2018, "citation": "arXiv preprint arXiv:1805.02262, 2018", "author": "Waleed Ammar and Dirk Groeneveld and Chandra Bhagavatula and Iz Beltagy and Miles Crawford and Doug Downey and Jason Dunkelberger and Ahmed Elgohary and Sergey Feldman and Vu Ha and Rodney Kinney and Sebastian Kohlmeier and Kyle Lo and Tyler Murray and Hsu-Han Ooi and Matthew Peters and Joanna Power and Sam Skjonsberg and Lucy Lu Wang and Chris Wilhelm and Zheng Yuan and Madeleine Van Zuylen and Oren Etzioni", "journal": "arXiv preprint arXiv:1805.02262", "abstract": "We describe a deployed scalable system for organizing published scientific literature into a heterogeneous graph to facilitate algorithmic manipulation and discovery. The resulting literature graph consists of more than 280M nodes, representing papers, authors, entities and various interactions between them (e.g., authorships, citations, entity mentions). We reduce literature graph construction into familiar NLP tasks (e.g., entity extraction and linking), point out research challenges due to differences from standard formulations of these tasks, and report empirical results for each task. The methods described in this paper are used to enable semantic features in www.semanticscholar.org"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:u-x6o8ySG0sC", "num_citations": 526, "citedby_url": "/scholar?hl=en&cites=5500969515339734950,9455462207916057127,17262639780631182458", "cites_id": ["5500969515339734950", "9455462207916057127", "17262639780631182458"], "pub_url": "https://arxiv.org/abs/1805.02262", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:pt8T-qBeV0wJ:scholar.google.com/", "cites_per_year": {"2018": 8, "2019": 33, "2020": 82, "2021": 91, "2022": 108, "2023": 109, "2024": 79, "2025": 12}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Fact or Fiction: Verifying Scientific Claims", "pub_year": 2020, "citation": "arXiv preprint arXiv:2004.14974, 2020", "author": "David Wadden and Kyle Lo and Lucy Lu Wang and Shanchuan Lin and Madeleine van Zuylen and Arman Cohan and Hannaneh Hajishirzi", "journal": "arXiv preprint arXiv:2004.14974", "abstract": "We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https://github.com/allenai/scifact. A leaderboard and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:Se3iqnhoufwC", "num_citations": 501, "citedby_url": "/scholar?hl=en&cites=8581256575017048563,576027366400720136,3258694246237062289", "cites_id": ["8581256575017048563", "576027366400720136", "3258694246237062289"], "pub_url": "https://arxiv.org/abs/2004.14974", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:81lguGO_FncJ:scholar.google.com/", "cites_per_year": {"2020": 8, "2021": 74, "2022": 80, "2023": 133, "2024": 181, "2025": 23}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "A dataset of information-seeking questions and answers anchored in research papers", "pub_year": 2021, "citation": "arXiv preprint arXiv:2105.03011, 2021", "author": "Pradeep Dasigi and Kyle Lo and Iz Beltagy and Arman Cohan and Noah A Smith and Matt Gardner", "journal": "arXiv preprint arXiv:2105.03011", "abstract": "Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:4TOpqqG69KYC", "num_citations": 283, "citedby_url": "/scholar?hl=en&cites=6307051234121361325,17638386895004520426", "cites_id": ["6307051234121361325", "17638386895004520426"], "pub_url": "https://arxiv.org/abs/2105.03011", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:rSf-Za4lh1cJ:scholar.google.com/", "cites_per_year": {"2021": 12, "2022": 29, "2023": 57, "2024": 148, "2025": 36}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "TLDR: Extreme summarization of scientific documents", "pub_year": 2020, "citation": "arXiv preprint arXiv:2004.15011, 2020", "author": "Isabel Cachola and Kyle Lo and Arman Cohan and Daniel S Weld", "journal": "arXiv preprint arXiv:2004.15011", "abstract": "We introduce TLDR generation, a new form of extreme summarization, for scientific papers. TLDR generation involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. To facilitate study on this task, we introduce SciTLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SciTLDR contains both author-written and expert-derived TLDRs, where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden. We propose CATTS, a simple yet effective learning strategy for generating TLDRs that exploits titles as an auxiliary training signal. CATTS improves upon strong baselines under both automated metrics and human evaluations. Data and code are publicly available at https://github.com/allenai/scitldr."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:UebtZRa9Y70C", "num_citations": 259, "citedby_url": "/scholar?hl=en&cites=17509837705190203838,5818415530738880232", "cites_id": ["17509837705190203838", "5818415530738880232"], "pub_url": "https://arxiv.org/abs/2004.15011", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:vmGaLJ9w__IJ:scholar.google.com/", "cites_per_year": {"2020": 4, "2021": 37, "2022": 65, "2023": 70, "2024": 72, "2025": 10}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "TREC-COVID: constructing a pandemic information retrieval test collection", "pub_year": 2021, "citation": "ACM SIGIR Forum 54 (1), 1-12, 2021", "author": "Ellen Voorhees and Tasmeer Alam and Steven Bedrick and Dina Demner-Fushman and William R Hersh and Kyle Lo and Kirk Roberts and Ian Soboroff and Lucy Lu Wang", "journal": "ACM SIGIR Forum", "volume": "54", "number": "1", "pages": "1-12", "publisher": "ACM", "abstract": "TREC-COVID is a community evaluation designed to build a test collection that captures the information needs of biomedical researchers using the scientific literature during a pandemic. One of the key characteristics of pandemic search is the accelerated rate of change: the topics of interest evolve as the pandemic progresses and the scientific literature in the area explodes. The COVID-19 pandemic provides an opportunity to capture this progression as it happens. TREC-COVID, in creating a test collection around COVID-19 literature, is building infrastructure to support new research and technologies in pandemic search."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:0EnyYjriUFMC", "num_citations": 258, "citedby_url": "/scholar?hl=en&cites=4891210870996621710", "cites_id": ["4891210870996621710"], "pub_url": "https://dl.acm.org/doi/abs/10.1145/3451964.3451965", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:jllIV18S4UMJ:scholar.google.com/", "cites_per_year": {"2020": 21, "2021": 51, "2022": 44, "2023": 58, "2024": 69, "2025": 14}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Olmo: Accelerating the science of language models", "pub_year": 2024, "citation": "ACL 2024 (\ud83c\udfc6 Best Paper Award \ud83c\udfc6 ), 2024", "author": "Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and Ananya Harsh Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A Smith and Hannaneh Hajishirzi", "journal": "ACL 2024 (\ud83c\udfc6 Best Paper Award \ud83c\udfc6 )", "abstract": "Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:2P1L_qKh6hAC", "num_citations": 202, "citedby_url": "/scholar?hl=en&cites=9815776543380863930,13240433636519999712,4991821231120907909,16203937061170404633,17106330609328052471", "cites_id": ["9815776543380863930", "13240433636519999712", "4991821231120907909", "16203937061170404633", "17106330609328052471"], "pub_url": "https://arxiv.org/abs/2402.00838", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:uvvTjsKkOIgJ:scholar.google.com/", "cites_per_year": {"2023": 5, "2024": 147, "2025": 49}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "The BigScience ROOTS Corpus: A 1.6 TB Composite Multilingual Dataset", "citation": "Thirty-sixth Conference on Neural Information Processing Systems Datasets \u2026, 0", "author": "Hugo Lauren\u00e7on and Lucile Saulnier and Thomas Wang and Christopher Akiki and Albert Villanova del Moral and Teven Le Scao and Leandro Von Werra and Chenghao Mou and Eduardo Gonz\u00e1lez Ponferrada and Huu Nguyen and J\u00f6rg Frohberg and Mario \u0160a\u0161ko and Quentin Lhoest and Angelina McMillan-Major and G\u00e9rard Dupont and Stella Biderman and Anna Rogers and Francesco De Toni and Giada Pistilli and Olivier Nguyen and Somaieh Nikpoor and Maraim Masoud and Pierre Colombo and Javier de la Rosa and Paulo Villegas and Tristan Thrush and Shayne Longpre and Sebastian Nagel and Leon Weber and Manuel Romero Mu\u00f1oz and Jian Zhu and Daniel Van Strien and Zaid Alyafeai and Khalid Almubarak and Vu Minh Chien and Itziar Gonzalez-Dios and Aitor Soroa and Kyle Lo and Manan Dey and Pedro Ortiz Suarez and Aaron Gokaslan and Shamik Bose and David Ifeoluwa Adelani and Long Phan and Hieu Tran and Ian Yu and Suhas Pai and Jenny Chim and Violette Lepercq and Suzana Ilic and Margaret Mitchell and Sasha Luccioni and Yacine Jernite", "conference": "Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:-f6ydRqryjwC", "num_citations": 202, "citedby_url": "/scholar?hl=en&cites=3183055554973462544", "cites_id": ["3183055554973462544"], "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:EPCGpil8LCwJ:scholar.google.com/", "cites_per_year": {"2022": 2, "2023": 65, "2024": 102, "2025": 28}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Quantifying sex bias in clinical studies at scale with automated data extraction", "pub_year": 2019, "citation": "JAMA network open 2 (7), e196700-e196700, 2019", "author": "Sergey Feldman and Waleed Ammar and Kyle Lo and Elly Trepman and Madeleine van Zuylen and Oren Etzioni", "journal": "JAMA network open", "volume": "2", "number": "7", "pages": "e196700-e196700", "publisher": "American Medical Association", "abstract": "Analyses of female representation in clinical studies have been limited in scope and scale.To perform a large-scale analysis of global enrollment sex bias in clinical studies.In this cross-sectional study, clinical studies from published articles from PubMed from 1966 to 2018 and records from Aggregate Analysis of ClinicalTrials.gov from 1999 to 2018 were identified. Global disease prevalence was determined for male and female patients in 11 disease categories from the Global Burden of Disease database: cardiovascular, diabetes, digestive, hepatitis (types A, B, C, and E), HIV/AIDS, kidney (chronic), mental, musculoskeletal, neoplasms, neurological, and respiratory (chronic). Machine reading algorithms were developed that extracted sex data from tables in articles and records on December 31, 2018, at an artificial intelligence research institute. Male and \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:UeHWp8X0CEIC", "num_citations": 163, "citedby_url": "/scholar?hl=en&cites=7616052052393035044", "cites_id": ["7616052052393035044"], "pub_url": "https://jamanetwork.com/journals/jamanetworkopen/article-abstract/2737103", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:JIXFdf2msWkJ:scholar.google.com/", "cites_per_year": {"2019": 2, "2020": 17, "2021": 37, "2022": 44, "2023": 26, "2024": 28, "2025": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research", "pub_year": 2024, "citation": "ACL 2024 (\ud83c\udfc6 Best Resource Award \ud83c\udfc6 ), 2024", "author": "Luca Soldaini and Rodney Kinney and Akshita Bhagia and Dustin Schwenk and David Atkinson and Russell Authur and Ben Bogin and Khyathi Chandu and Jennifer Dumas and Yanai Elazar and Valentin Hofmann and Ananya Harsh Jha and Sachin Kumar and Li Lucy and Xinxi Lyu and Nathan Lambert and Ian Magnusson and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E Peters and Abhilasha Ravichander and Kyle Richardson and Zejiang Shen and Emma Strubell and Nishant Subramani and Oyvind Tafjord and Pete Walsh and Luke Zettlemoyer and Noah A Smith and Hannaneh Hajishirzi and Iz Beltagy and Dirk Groeneveld and Jesse Dodge and Kyle Lo", "journal": "ACL 2024 (\ud83c\udfc6 Best Resource Award \ud83c\udfc6 )", "abstract": "Information about pretraining corpora used to train the current best-performing language models is seldom discussed: commercial models rarely detail their data, and even open models are often released without accompanying training data or recipes to reproduce them. As a result, it is challenging to conduct and advance scientific research on language modeling, such as understanding how training data impacts model capabilities and limitations. To facilitate scientific research on language model pretraining, we curate and release Dolma, a three-trillion-token English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. We extensively document Dolma, including its design principles, details about its construction, and a summary of its contents. We present analyses and experimental results on intermediate states of Dolma to share what we have learned about important data curation practices. Finally, we open-source our data curation toolkit to enable reproduction of our work as well as support further research in large-scale data curation."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:ldfaerwXgEUC", "num_citations": 158, "citedby_url": "/scholar?hl=en&cites=9366363748568682906,12294864443547789504,6686692148493394804,16475477106165594654,16346848030558272138", "cites_id": ["9366363748568682906", "12294864443547789504", "6686692148493394804", "16475477106165594654", "16346848030558272138"], "pub_url": "https://arxiv.org/abs/2402.00159", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:mlGIRjQC_IEJ:scholar.google.com/", "cites_per_year": {"2023": 6, "2024": 117, "2025": 34}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Text mining approaches for dealing with the rapidly expanding literature on COVID-19", "pub_year": 2021, "citation": "Briefings in Bioinformatics 22 (2), 781-799, 2021", "author": "Lucy Lu Wang and Kyle Lo", "volume": "22", "number": "2", "pages": "781-799", "publisher": "Oxford University Press", "abstract": "More than 50 000 papers have been published about COVID-19 since the beginning of 2020 and several hundred new papers continue to be published every day. This incredible rate of scientific productivity leads to information overload, making it difficult for researchers, clinicians and public health officials to keep up with the latest findings. Automated text mining techniques for searching, reading and summarizing papers are helpful for addressing information overload. In this review, we describe the many resources that have been introduced to support text mining applications over the COVID-19 literature; specifically, we discuss the corpora, modeling resources, systems and shared tasks that have been introduced for COVID-19. We compile a list of 39 systems that provide functionality such as search, discovery, visualization and summarization over the COVID-19 literature. For each system, we provide a \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:KlAtU1dfN6UC", "num_citations": 150, "citedby_url": "/scholar?hl=en&cites=17332744040624520058", "cites_id": ["17332744040624520058"], "pub_url": "https://academic.oup.com/bib/article-abstract/22/2/781/6024738", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ensxRN9GivAJ:scholar.google.com/", "cites_per_year": {"2021": 34, "2022": 55, "2023": 36, "2024": 23, "2025": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "TREC-COVID: Rationale and Structure of an Information Retrieval Shared Task for COVID-19", "pub_year": 2020, "citation": "Journal of the American Medical Informatics Association, 2020", "author": "Kirk Roberts and Tasmeer Alam and Steven Bedrick and Dina Demner-Fushman and Kyle Lo and Ian Soboroff and Ellen Voorhees and Lucy Lu Wang and William R Hersh", "journal": "Journal of the American Medical Informatics Association", "abstract": "TREC-COVID is an information retrieval (IR) shared task initiated to support clinicians and clinical research during the COVID-19 pandemic. IR for pandemics breaks many normal assumptions, which can be seen by examining 9 important basic IR research questions related to pandemic situations. TREC-COVID differs from traditional IR shared task evaluations with special considerations for the expected users, IR modality considerations, topic development, participant requirements, assessment process, relevance criteria, evaluation metrics, iteration process, projected timeline, and the implications of data use as a post-task test collection. This article describes how all these were addressed for the particular requirements of developing IR systems under a pandemic situation. Finally, initial participation numbers are also provided, which demonstrate the tremendous interest the IR community has in this effort."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:roLk4NBRz8UC", "num_citations": 146, "citedby_url": "/scholar?hl=en&cites=3097470903742290941,4809929076757418229,4816456341056109482", "cites_id": ["3097470903742290941", "4809929076757418229", "4816456341056109482"], "pub_url": "https://academic.oup.com/jamia/article-abstract/27/9/1431/5828938", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:_duqN2Bt_CoJ:scholar.google.com/", "cites_per_year": {"2020": 26, "2021": 50, "2022": 33, "2023": 20, "2024": 15, "2025": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Harnessing the power of smart and connected health to tackle COVID-19: IoT, AI, robotics, and blockchain for a better world", "pub_year": 2021, "citation": "IEEE Internet of Things Journal 8 (16), 12826-12846, 2021", "author": "Farshad Firouzi and Bahar Farahani and Mahmoud Daneshmand and Kathy Grise and Jaeseung Song and Roberto Saracco and Lucy Lu Wang and Kyle Lo and Plamen Angelov and Eduardo Soares and Po-Shen Loh and Zeynab Talebpour and Reza Moradi and Mohsen Goodarzi and Haleh Ashraf and Mohammad Talebpour and Alireza Talebpour and Luca Romeo and Rupam Das and Hadi Heidari and Dana Pasquale and James Moody and Chris Woods and Erich S Huang and Payam Barnaghi and Majid Sarrafzadeh and Ron Li and Kristen L Beck and Olexandr Isayev and Nakmyoung Sung and Alan Luo", "journal": "IEEE Internet of Things Journal", "volume": "8", "number": "16", "pages": "12826-12846", "publisher": "IEEE", "abstract": "As COVID-19 hounds the world, the common cause of finding a swift solution to manage the pandemic has brought together researchers, institutions, governments, and society at large. The Internet of Things (IoT), artificial intelligence (AI)\u2014including machine learning (ML) and Big Data analytics\u2014as well as Robotics and Blockchain, are the four decisive areas of technological innovation that have been ingenuity harnessed to fight this pandemic and future ones. While these highly interrelated smart and connected health technologies cannot resolve the pandemic overnight and may not be the only answer to the crisis, they can provide greater insight into the disease and support frontline efforts to prevent and control the pandemic. This article provides a blend of discussions on the contribution of these digital technologies, propose several complementary and multidisciplinary techniques to combat COVID-19, offer \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:M3ejUd6NZC8C", "num_citations": 139, "citedby_url": "/scholar?hl=en&cites=7327816362691721478", "cites_id": ["7327816362691721478"], "pub_url": "https://ieeexplore.ieee.org/abstract/document/9406879/", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:BvmITSiisWUJ:scholar.google.com/", "cites_per_year": {"2021": 7, "2022": 28, "2023": 51, "2024": 44, "2025": 7}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "The semantic scholar open data platform", "pub_year": 2023, "citation": "arXiv preprint arXiv:2301.10140, 2023", "author": "Rodney Kinney and Chloe Anastasiades and Russell Authur and Iz Beltagy and Jonathan Bragg and Alexandra Buraczynski and Isabel Cachola and Stefan Candra and Yoganand Chandrasekhar and Arman Cohan and Miles Crawford and Doug Downey and Jason Dunkelberger and Oren Etzioni and Rob Evans and Sergey Feldman and Joseph Gorney and David Graham and Fangzhou Hu and Regan Huff and Daniel King and Sebastian Kohlmeier and Bailey Kuehl and Michael Langan and Daniel Lin and Haokun Liu and Kyle Lo and Jaron Lochner and Kelsey MacMillan and Tyler Murray and Chris Newell and Smita Rao and Shaurya Rohatgi and Paul Sayre and Zejiang Shen and Amanpreet Singh and Luca Soldaini and Shivashankar Subramanian and Amber Tanaka and Alex D Wade and Linda Wagner and Lucy Lu Wang and Chris Wilhelm and Caroline Wu and Jiangjiang Yang and Angele Zamarron and Madeleine Van Zuylen and Daniel S Weld", "journal": "arXiv preprint arXiv:2301.10140", "abstract": "The volume of scientific output is creating an urgent need for automated tools to help scientists keep up with developments in their field. Semantic Scholar (S2) is an open data platform and website aimed at accelerating science by helping scholars discover and understand scientific literature. We combine public and proprietary data sources using state-of-the-art techniques for scholarly PDF content extraction and automatic knowledge graph construction to build the Semantic Scholar Academic Graph, the largest open scientific literature graph to-date, with 200M+ papers, 80M+ authors, 550M+ paper-authorship edges, and 2.4B+ citation edges. The graph includes advanced semantic features such as structurally parsed text, natural language summaries, and vector embeddings. In this paper, we describe the components of the S2 data processing pipeline and the associated APIs offered by the platform. We will update this living document to reflect changes as we add new data offerings and improve existing services."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:4JMBOYKVnBMC", "num_citations": 128, "citedby_url": "/scholar?hl=en&cites=2104554159089044413,16220830404127647609,17152319468327566535,9078594800465746916", "cites_id": ["2104554159089044413", "16220830404127647609", "17152319468327566535", "9078594800465746916"], "pub_url": "https://arxiv.org/abs/2301.10140", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:vcsSd9vgNB0J:scholar.google.com/", "cites_per_year": {"2022": 1, "2023": 23, "2024": 91, "2025": 13}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Flex: Unifying evaluation for few-shot nlp", "pub_year": 2021, "citation": "Advances in neural information processing systems 34, 15787-15800, 2021", "author": "Jonathan Bragg and Arman Cohan and Kyle Lo and Iz Beltagy", "journal": "Advances in neural information processing systems", "volume": "34", "pages": "15787-15800", "abstract": "Few-shot NLP research is highly active, yet conducted in disjoint research threads with evaluation suites that lack challenging-yet-realistic testing setups and fail to employ careful experimental design. Consequently, the community does not know which techniques perform best or even if they outperform simple baselines. In response, we formulate the FLEX Principles, a set of requirements and best practices for unified, rigorous, valid, and cost-sensitive few-shot NLP evaluation. These principles include Sample Size Design, a novel approach to benchmark design that optimizes statistical accuracy and precision while keeping evaluation costs manageable. Following the principles, we release the FLEX benchmark, which includes four few-shot transfer settings, zero-shot evaluation, and a public leaderboard that covers diverse NLP tasks. In addition, we present UniFew, a prompt-based model for few-shot learning that unifies pretraining and finetuning prompt formats, eschewing complex machinery of recent prompt-based approaches in adapting downstream task formats to language model pretraining objectives. We demonstrate that despite simplicity, UniFew achieves results competitive with both popular meta-learning and prompt-based approaches."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:Wp0gIr-vW9MC", "num_citations": 122, "citedby_url": "/scholar?hl=en&cites=16247210157005775094", "cites_id": ["16247210157005775094"], "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/8493eeaccb772c0878f99d60a0bd2bb3-Abstract.html", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:9qByb46veeEJ:scholar.google.com/", "cites_per_year": {"2021": 12, "2022": 33, "2023": 36, "2024": 33, "2025": 7}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Booookscore: A systematic exploration of book-length summarization in the era of llms", "pub_year": 2023, "citation": "arXiv preprint arXiv:2310.00785, 2023", "author": "Yapei Chang and Kyle Lo and Tanya Goyal and Mohit Iyyer", "journal": "arXiv preprint arXiv:2310.00785", "abstract": "Summarizing book-length documents (>100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BooookScore, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving $15K USD and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than those generated by open-source models. While LLaMA 2 falls \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:vV6vV6tmYwMC", "num_citations": 109, "citedby_url": "/scholar?hl=en&cites=17968620361685249119,845767145766245791", "cites_id": ["17968620361685249119", "845767145766245791"], "pub_url": "https://arxiv.org/abs/2310.00785", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:X6wZYgRdXfkJ:scholar.google.com/", "cites_per_year": {"2022": 1, "2023": 3, "2024": 85, "2025": 20}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Augmenting scientific papers with just-in-time, position-sensitive definitions of terms and symbols", "pub_year": 2021, "citation": "Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems \u2026, 2021", "author": "Andrew Head and Kyle Lo and Dongyeop Kang and Raymond Fok and Sam Skjonsberg and Daniel S Weld and Marti A Hearst", "pages": "1-18", "abstract": "Despite the central importance of research papers to scientific progress, they can be difficult to read. Comprehension is often stymied when the information needed to understand a passage resides somewhere else\u2014in another section, or in another paper. In this work, we envision how interfaces can bring definitions of technical terms and symbols to readers when and where they need them most. We introduce ScholarPhi, an augmented reading interface with four novel features: (1) tooltips that surface position-sensitive definitions from elsewhere in a paper, (2) a filter over the paper that \u201cdeclutters\u201d it to reveal how the term or symbol is used across the paper, (3) automatic equation diagrams that expose multiple definitions in parallel, and (4) an automatically generated glossary of important terms and symbols. A usability study showed that the tool helps researchers of all experience levels read papers. Furthermore \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:Zph67rFs4hoC", "num_citations": 105, "citedby_url": "/scholar?hl=en&cites=3838473867334658064", "cites_id": ["3838473867334658064"], "pub_url": "https://dl.acm.org/doi/abs/10.1145/3411764.3445648", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ECgaLqX_RDUJ:scholar.google.com/", "cites_per_year": {"2020": 1, "2021": 6, "2022": 23, "2023": 33, "2024": 33, "2025": 7}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models", "pub_year": 2024, "citation": "arXiv preprint arXiv:2409.17146, 2024", "author": "Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and Jiasen Lu and Taira Anderson and Erin Bransom and Kiana Ehsani and Huong Ngo and YenSung Chen and Ajay Patel and Mark Yatskar and Chris Callison-Burch and Andrew Head and Rose Hendrix and Favyen Bastani and Eli VanderBilt and Nathan Lambert and Yvonne Chou and Arnavi Chheda and Jenna Sparks and Sam Skjonsberg and Michael Schmitz and Aaron Sarnat and Byron Bischoff and Pete Walsh and Chris Newell and Piper Wolters and Tanmay Gupta and Kuo-Hao Zeng and Jon Borchardt and Dirk Groeneveld and Jen Dumas and Crystal Nam and Sophie Lebrecht and Caitlin Wittlif and Carissa Schoenick and Oscar Michel and Ranjay Krishna and Luca Weihs and Noah A Smith and Hannaneh Hajishirzi and Ross Girshick and Ali Farhadi and Aniruddha Kembhavi", "journal": "arXiv preprint arXiv:2409.17146", "abstract": "Today's most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, we also introduce a diverse dataset mixture for fine-tuning that includes in-the-wild Q&A and innovative 2D pointing data. The success of our approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human evaluation. We will be releasing all of our model weights, captioning and fine-tuning data, and source code in the near future. Select model weights, inference code, and demo are available at https://molmo.allenai.org."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:f2IySw72cVMC", "num_citations": 83, "citedby_url": "/scholar?hl=en&cites=18139020847063770619,10326719535477763372", "cites_id": ["18139020847063770619", "10326719535477763372"], "pub_url": "https://arxiv.org/abs/2409.17146", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:-yWkElu_uvsJ:scholar.google.com/", "cites_per_year": {"2024": 39, "2025": 44}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Paper Plain: Making Medical Research Papers Approachable to Healthcare Consumers with Natural Language Processing", "pub_year": 2023, "citation": "ACM Transactions on Computer-Human Interaction 30 (5), 1-38, 2023", "author": "Tal August and Lucy Lu Wang and Jonathan Bragg and Marti A Hearst and Andrew Head and Kyle Lo", "journal": "ACM Transactions on Computer-Human Interaction", "volume": "30", "number": "5", "pages": "1-38", "publisher": "ACM", "abstract": "When seeking information not covered in patient-friendly documents, healthcare consumers may turn to the research literature. Reading medical papers, however, can be a challenging experience. To improve access to medical papers, we explore four features enabled by natural language processing: definitions of unfamiliar terms, in-situ plain language section summaries, a collection of key questions that guides readers to answering passages, and plain language summaries of those passages. We embody these features into a prototype system, Paper Plain. We evaluate Paper Plain, finding that participants who used the prototype system had an easier time reading research papers without a loss in paper comprehension compared to those who used a typical PDF reader. Altogether, the study results suggest that guiding readers to relevant passages and providing plain language summaries alongside the \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:ZeXyd9-uunAC", "num_citations": 82, "citedby_url": "/scholar?hl=en&cites=14088093195797512963", "cites_id": ["14088093195797512963"], "pub_url": "https://dl.acm.org/doi/abs/10.1145/3589955", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:A8Mboh72gsMJ:scholar.google.com/", "cites_per_year": {"2022": 3, "2023": 26, "2024": 46, "2025": 6}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MultiVerS: Improving scientific claim verification with weak supervision and full-document context", "pub_year": 2022, "citation": "Findings of the Association for Computational Linguistics: NAACL 2022, 61-76, 2022", "author": "David Wadden and Kyle Lo and Lucy Wang and Arman Cohan and Iz Beltagy and Hannaneh Hajishirzi", "conference": "Findings of the Association for Computational Linguistics: NAACL 2022", "pages": "61-76", "abstract": "The scientific claim verification task requires an NLP system to label scientific documents which Support or Refute an input claim, and to select evidentiary sentences (or rationales) justifying each predicted label. In this work, we present MultiVerS, which predicts a fact-checking label and identifies rationales in a multitask fashion based on a shared encoding of the claim and full document context. This approach accomplishes two key modeling goals. First, it ensures that all relevant contextual information is incorporated into each labeling decision. Second, it enables the model to learn from instances annotated with a document-level fact-checking label, but lacking sentence-level rationales. This allows MultiVerS to perform weakly-supervised domain adaptation by training on scientific documents labeled using high-precision heuristics. Our approach outperforms two competitive baselines on three scientific claim verification datasets, with particularly strong performance in zero / few-shot domain adaptation experiments. Our code and data are available at https://github.com/dwadden/multivers."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:dhFuZR0502QC", "num_citations": 79, "citedby_url": "/scholar?hl=en&cites=17727831189844028857,12477125042920707305", "cites_id": ["17727831189844028857", "12477125042920707305"], "pub_url": "https://arxiv.org/abs/2112.01640", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:uW2-hIroBfYJ:scholar.google.com/", "cites_per_year": {"2022": 14, "2023": 25, "2024": 33, "2025": 7}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Data governance in the age of large-scale data-driven language technology", "pub_year": 2022, "citation": "Proceedings of the 2022 ACM Conference on Fairness, Accountability, and \u2026, 2022", "author": "Yacine Jernite and Huu Nguyen and Stella Biderman and Anna Rogers and Maraim Masoud and Valentin Danchev and Samson Tan and Alexandra Sasha Luccioni and Nishant Subramani and Isaac Johnson and Gerard Dupont and Jesse Dodge and Kyle Lo and Zeerak Talat and Dragomir Radev and Aaron Gokaslan and Somaieh Nikpoor and Peter Henderson and Rishi Bommasani and Margaret Mitchell", "pages": "2206-2222", "abstract": "The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work. "}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:mB3voiENLucC", "num_citations": 77, "citedby_url": "/scholar?hl=en&cites=12565045004624813169", "cites_id": ["12565045004624813169"], "pub_url": "https://dl.acm.org/doi/abs/10.1145/3531146.3534637", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:cVx199sBYK4J:scholar.google.com/", "cites_per_year": {"2022": 9, "2023": 31, "2024": 35, "2025": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization", "pub_year": 2023, "citation": "EACL 2023 (\ud83c\udfc6 Outstanding Paper Award \ud83c\udfc6 ), 2023", "author": "Kalpesh Krishna and Erin Bransom and Bailey Kuehl and Mohit Iyyer and Pradeep Dasigi and Arman Cohan and Kyle Lo", "conference": "EACL 2023 (\ud83c\udfc6 Outstanding Paper Award \ud83c\udfc6 )", "abstract": "While human evaluation remains best practice for accurately judging the faithfulness of automatically-generated summaries, few solutions exist to address the increased difficulty and workload when evaluating long-form summaries. Through a survey of 162 papers on long-form summarization, we first shed light on current human evaluation practices surrounding long-form summaries. We find that 73% of these papers do not perform any human evaluation on model-generated summaries, while other works face new difficulties that manifest when dealing with long documents (e.g., low inter-annotator agreement). Motivated by our survey, we present LongEval, a set of guidelines for human evaluation of faithfulness in long-form summaries that addresses the following challenges: (1) How can we achieve high inter-annotator agreement on faithfulness scores? (2) How can we minimize annotator workload while maintaining accurate faithfulness scores? and (3) Do humans benefit from automated alignment between summary and source snippets? We deploy LongEval in annotation studies on two long-form summarization datasets in different domains (SQuALITY and PubMed), and we find that switching to a finer granularity of judgment (e.g., clause-level) reduces inter-annotator variance in faithfulness scores (e.g., std-dev from 18.5 to 6.8). We also show that scores from a partial annotation of fine-grained units highly correlates with scores from a full annotation workload (0.89 Kendall's tau using 50% judgments). We release our human judgments, annotation templates, and our software as a Python library for future research."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:iH-uZ7U-co4C", "num_citations": 76, "citedby_url": "/scholar?hl=en&cites=5194963748596851481", "cites_id": ["5194963748596851481"], "pub_url": "https://arxiv.org/abs/2301.13298", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:GaMlWAE4GEgJ:scholar.google.com/", "cites_per_year": {"2023": 17, "2024": 51, "2025": 8}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Explaining Relationships Between Scientific Documents", "pub_year": 2021, "citation": "Proceedings of the 59th Annual Meeting of the Association for Computational \u2026, 2021", "author": "Kelvin Luu and Xinyi Wu and Rik Koncel-Kedziorski and Kyle Lo and Isabel Cachola and Noah A Smith", "conference": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)", "pages": "2130-2144", "abstract": "We address the task of explaining relationships between two scientific documents using natural language text. This task requires modeling the complex content of long technical documents, deducing a relationship between these documents, and expressing the details of that relationship in text. In addition to the theoretical interest of this task, successful solutions can help improve researcher efficiency in search and review. In this paper we establish a dataset of 622K examples from 154K documents. We pretrain a large language model to serve as the foundation for autoregressive approaches to the task. We explore the impact of taking different views on the two documents, including the use of dense representations extracted with scientific IE systems. We provide extensive automatic and human evaluations which show the promise of such models, but make clear challenges for future work."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:9ZlFYXVOiuMC", "num_citations": 71, "citedby_url": "/scholar?hl=en&cites=16399981651571984030,9238973615662532836", "cites_id": ["16399981651571984030", "9238973615662532836"], "pub_url": "https://arxiv.org/abs/2002.00317", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:nkbf2GtwmOMJ:scholar.google.com/", "cites_per_year": {"2020": 2, "2021": 9, "2022": 21, "2023": 19, "2024": 16, "2025": 4}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Ontology alignment in the biomedical domain using entity definitions and context", "pub_year": 2018, "citation": "arXiv preprint arXiv:1806.07976, 2018", "author": "Lucy Lu Wang and Chandra Bhagavatula and Mark Neumann and Kyle Lo and Chris Wilhelm and Waleed Ammar", "journal": "arXiv preprint arXiv:1806.07976", "abstract": "Ontology alignment is the task of identifying semantically equivalent entities from two given ontologies. Different ontologies have different representations of the same entity, resulting in a need to de-duplicate entities when merging ontologies. We propose a method for enriching entities in an ontology with external definition and context information, and use this additional information for ontology alignment. We develop a neural architecture capable of encoding the additional information when available, and show that the addition of external data results in an F1-score of 0.69 on the Ontology Alignment Evaluation Initiative (OAEI) largebio SNOMED-NCI subtask, comparable with the entity-level matchers in a SOTA system."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:d1gkVwhDpl0C", "num_citations": 69, "citedby_url": "/scholar?hl=en&cites=13491887646267112922,6276386827614620450", "cites_id": ["13491887646267112922", "6276386827614620450"], "pub_url": "https://arxiv.org/abs/1806.07976", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:2n07yVTQPLsJ:scholar.google.com/", "cites_per_year": {"2019": 7, "2020": 13, "2021": 11, "2022": 12, "2023": 15, "2024": 9, "2025": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SciFact-open: Towards open-domain scientific claim verification", "pub_year": 2022, "citation": "arXiv preprint arXiv:2210.13777, 2022", "author": "David Wadden and Kyle Lo and Bailey Kuehl and Arman Cohan and Iz Beltagy and Lucy Lu Wang and Hannaneh Hajishirzi", "journal": "arXiv preprint arXiv:2210.13777", "abstract": "While research on scientific claim verification has led to the development of powerful systems that appear to approach human performance, these approaches have yet to be tested in a realistic setting against large corpora of scientific literature. Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. Drawing upon pooling techniques from information retrieval, we collect evidence for scientific claims by pooling and annotating the top predictions of four state-of-the-art scientific claim verification models. We find that systems developed on smaller corpora struggle to generalize to SciFact-Open, exhibiting performance drops of at least 15 F1. In addition, analysis of the evidence in SciFact-Open reveals interesting phenomena likely to appear when claim verification systems are deployed in practice, e.g., cases where the evidence supports only a special case of the claim. Our dataset is available at https://github.com/dwadden/scifact-open."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:j3f4tGmQtD8C", "num_citations": 61, "citedby_url": "/scholar?hl=en&cites=2212935191531147534,17392949561774679693", "cites_id": ["2212935191531147534", "17392949561774679693"], "pub_url": "https://arxiv.org/abs/2210.13777", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:DtEsAtXstR4J:scholar.google.com/", "cites_per_year": {"2023": 16, "2024": 34, "2025": 9}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Generating scientific claims for zero-shot scientific fact checking", "pub_year": 2022, "citation": "arXiv preprint arXiv:2203.12990, 2022", "author": "Dustin Wright and David Wadden and Kyle Lo and Bailey Kuehl and Arman Cohan and Isabelle Augenstein and Lucy Lu Wang", "journal": "arXiv preprint arXiv:2203.12990", "abstract": "Automated scientific fact checking is difficult due to the complexity of scientific language and a lack of significant amounts of training data, as annotation requires domain expertise. To address this challenge, we propose scientific claim generation, the task of generating one or more atomic and verifiable claims from scientific sentences, and demonstrate its usefulness in zero-shot fact checking for biomedical claims. We propose CLAIMGEN-BART, a new supervised method for generating claims supported by the literature, as well as KBIN, a novel method for generating claim negations. Additionally, we adapt an existing unsupervised entity-centric method of claim generation to biomedical claims, which we call CLAIMGEN-ENTITY. Experiments on zero-shot fact checking demonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART, coupled with KBIN, achieve up to 90% performance of fully supervised models trained on manually annotated claims and evidence. A rigorous evaluation study demonstrates significant improvement in generated claim and negation quality over existing baselines"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:HDshCWvjkbEC", "num_citations": 60, "citedby_url": "/scholar?hl=en&cites=14808292054991015129", "cites_id": ["14808292054991015129"], "pub_url": "https://arxiv.org/abs/2203.12990", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:2Yz6kyqfgc0J:scholar.google.com/", "cites_per_year": {"2021": 1, "2022": 6, "2023": 19, "2024": 31, "2025": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Datacomp-lm: In search of the next generation of training sets for language models", "pub_year": 2024, "citation": "Advances in Neural Information Processing Systems 37, 14200-14282, 2024", "author": "Jeffrey Li and Alex Fang and Georgios Smyrnis and Maor Ivgi and Matt Jordan and Samir Yitzhak Gadre and Hritik Bansal and Etash Guha and Sedrick Scott Keh and Kushal Arora and Saurabh Garg and Rui Xin and Niklas Muennighoff and Reinhard Heckel and Jean Mercat and Mayee Chen and Suchin Gururangan and Mitchell Wortsman and Alon Albalak and Yonatan Bitton and Marianna Nezhurina and Amro Abbas and Cheng-Yu Hsieh and Dhruba Ghosh and Josh Gardner and Maciej Kilian and Hanlin Zhang and Rulin Shao and Sarah Pratt and Sunny Sanyal and Gabriel Ilharco and Giannis Daras and Kalyani Marathe and Aaron Gokaslan and Jieyu Zhang and Khyathi Chandu and Thao Nguyen and Igor Vasiljevic and Sham Kakade and Shuran Song and Sujay Sanghavi and Fartash Faghri and Sewoong Oh and Luke Zettlemoyer and Kyle Lo and Alaaeldin El-Nouby and Hadi Pouransari and Alexander Toshev and Stephanie Wang and Dirk Groeneveld and Luca Soldaini and Pang Wei W Koh and Jenia Jitsev and Thomas Kollar and Alex Dimakis and Yair Carmon and Achal Dave and Ludwig Schmidt and Vaishaal Shankar", "journal": "Advances in Neural Information Processing Systems", "volume": "37", "pages": "14200-14282", "abstract": "We introduce DataComp for Language Models, a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing atmodel scales ranging from 412M to 7B parameters. As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set. The resulting dataset, DCLM-Baseline, enables training a 7B parameter language model from scratch to 63% 5-shot accuracy on MMLU with 2T training tokens. Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-Baseline represents a 6 percentage point improvement on MMLU while being trained with half the compute. Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation. We release the\\dclm benchmark, framework, models, and datasets at https://www. datacomp. ai/dclm/"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:fPk4N6BV_jEC", "num_citations": 58, "citedby_url": "/scholar?hl=en&cites=14691729232576505865", "cites_id": ["14691729232576505865"], "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/19e4ea30dded58259665db375885e412-Abstract-Datasets_and_Benchmarks_Track.html", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:Cejxm-WB48sJ:scholar.google.com/", "cites_per_year": {"2024": 37, "2025": 20}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Multi-lexsum: Real-world summaries of civil rights lawsuits at multiple granularities", "pub_year": 2022, "citation": "Advances in Neural Information Processing Systems 35, 13158-13173, 2022", "author": "Zejiang Shen and Kyle Lo and Lauren Yu and Nathan Dahlberg and Margo Schlanger and Doug Downey", "journal": "Advances in Neural Information Processing Systems", "volume": "35", "pages": "13158-13173", "abstract": "With the advent of large language models, methods for abstractive summarization have made great strides, creating potential for use in applications to aid knowledge workers processing unwieldy document collections. One such setting is the Civil Rights Litigation Clearinghouse (CRLC, https://clearinghouse. net), which posts information about large-scale civil rights lawsuits, serving lawyers, scholars, and the general public. Today, summarization in the CRLC requires extensive training of lawyers and law students who spend hours per case understanding multiple relevant documents in order to produce high-quality summaries of key events and outcomes. Motivated by this ongoing real-world summarization effort, we introduce Multi-LexSum, a collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing. Multi-LexSum presents a challenging multi-document summarization task given the length of the source documents, often exceeding two hundred pages per case. Furthermore, Multi-LexSum is distinct from other datasets in its multiple target summaries, each at a different granularity (ranging from one-sentence\" extreme\" summaries to multi-paragraph narrations of over five hundred words). We present extensive analysis demonstrating that despite the high-quality summaries in the training data (adhering to strict content and style guidelines), state-of-the-art summarization models perform poorly on this task. We release Multi-LexSum for further summarization research and to facilitate the development of applications to assist in the CRLC's mission at https://multilexsum. github. io."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:7PzlFSSx8tAC", "num_citations": 58, "citedby_url": "/scholar?hl=en&cites=5062559571179489712,16804314209216215130", "cites_id": ["5062559571179489712", "16804314209216215130"], "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/552ef803bef9368c29e53c167de34b55-Abstract-Datasets_and_Benchmarks.html", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:sElofhvTQUYJ:scholar.google.com/", "cites_per_year": {"2022": 13, "2023": 20, "2024": 17, "2025": 8}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MultiCite: Modeling realistic citations requires moving beyond the single-sentence single-label setting", "pub_year": 2021, "citation": "arXiv preprint arXiv:2107.00414, 2021", "author": "Anne Lauscher and Brandon Ko and Bailey Kuehl and Sophie Johnson and David Jurgens and Arman Cohan and Kyle Lo", "journal": "arXiv preprint arXiv:2107.00414", "abstract": "Citation context analysis (CCA) is an important task in natural language processing that studies how and why scholars discuss each others' work. Despite decades of study, traditional frameworks for CCA have largely relied on overly-simplistic assumptions of how authors cite, which ignore several important phenomena. For instance, scholarly papers often contain rich discussions of cited work that span multiple sentences and express multiple intents concurrently. Yet, CCA is typically approached as a single-sentence, single-label classification task, and thus existing datasets fail to capture this interesting discourse. In our work, we address this research gap by proposing a novel framework for CCA as a document-level context extraction and labeling task. We release MultiCite, a new dataset of 12,653 citation contexts from over 1,200 computational linguistics papers. Not only is it the largest collection of expert-annotated citation contexts to-date, MultiCite contains multi-sentence, multi-label citation contexts within full paper texts. Finally, we demonstrate how our dataset, while still usable for training classic CCA models, also supports the development of new types of models for CCA beyond fixed-width text classification. We release our code and dataset at https://github.com/allenai/multicite."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:aqlVkmm33-oC", "num_citations": 48, "citedby_url": "/scholar?hl=en&cites=2318471823385723395", "cites_id": ["2318471823385723395"], "pub_url": "https://arxiv.org/abs/2107.00414", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:A4Ko7tbdLCAJ:scholar.google.com/", "cites_per_year": {"2021": 1, "2022": 12, "2023": 20, "2024": 12, "2025": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "CiteSee: Augmenting Citations in Scientific Papers with Persistent and Personalized Historical Context", "pub_year": 2023, "citation": "CHI 2023 (\ud83c\udfc6 Best Paper Award \ud83c\udfc6 ), 2023", "author": "Joseph Chee Chang and Amy X Zhang and Jonathan Bragg and Andrew Head and Kyle Lo and Doug Downey and Daniel S Weld", "journal": "CHI 2023 (\ud83c\udfc6 Best Paper Award \ud83c\udfc6 )", "abstract": " When reading a scholarly article, inline citations help researchers contextualize the current article and discover relevant prior work. However, it can be challenging to prioritize and make sense of the hundreds of citations encountered during literature reviews. This paper introduces CiteSee, a paper reading tool that leverages a user\u2019s publishing, reading, and saving activities to provide personalized visual augmentations and context around citations. First, CiteSee connects the current paper to familiar contexts by surfacing known citations a user had cited or opened. Second, CiteSee helps users prioritize their exploration by highlighting relevant but unknown citations based on saving and reading history. We conducted a lab study that suggests CiteSee is significantly more effective for paper discovery than three baselines. A field deployment study shows CiteSee helps participants keep track of their explorations \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:RHpTSmoSYBkC", "num_citations": 44, "citedby_url": "/scholar?hl=en&cites=792499569457548616", "cites_id": ["792499569457548616"], "pub_url": "https://dl.acm.org/doi/abs/10.1145/3544548.3580847", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:SM0q2iiG_woJ:scholar.google.com/", "cites_per_year": {"2023": 12, "2024": 29, "2025": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "VILA: Improving structured content extraction from scientific PDFs using visual layout groups", "pub_year": 2022, "citation": "Transactions of the Association for Computational Linguistics 10, 376-392, 2022", "author": "Zejiang Shen and Kyle Lo and Lucy Lu Wang and Bailey Kuehl and Daniel S Weld and Doug Downey", "journal": "Transactions of the Association for Computational Linguistics", "volume": "10", "pages": "376-392", "publisher": "MIT Press", "abstract": "Accurately extracting structured content from PDFs is a critical first step for NLP over scientific papers. Recent work has improved extraction accuracy by incorporating elementary layout information, for example, each token\u2019s 2D position on the page, into language model pretraining. We introduce new methods that explicitly model VIsual LAyout (VILA) groups, that is, text lines or text blocks, to further improve performance. In our I-VILA approach, we show that simply inserting special tokens denoting layout group boundaries into model inputs can lead to a 1.9% Macro F1 improvement in token classification. In the H-VILA approach, we show that hierarchical encoding of layout-groups can result in up to 47% inference time reduction with less than 0.8% Macro F1 loss. Unlike prior layout-aware approaches, our methods do not require expensive additional pretraining, only fine-tuning, which we show can reduce \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:IWHjjKOFINEC", "num_citations": 44, "citedby_url": "/scholar?hl=en&cites=8559478060655220690,5859933587483610821", "cites_id": ["8559478060655220690", "5859933587483610821"], "pub_url": "https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00466/110438", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:0pcaNPJfyXYJ:scholar.google.com/", "cites_per_year": {"2021": 1, "2022": 6, "2023": 18, "2024": 18, "2025": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Scim: Intelligent skimming support for scientific papers", "pub_year": 2023, "citation": "Proceedings of the 28th International Conference on Intelligent User \u2026, 2023", "author": "Raymond Fok and Hita Kambhamettu and Luca Soldaini and Jonathan Bragg and Kyle Lo and Marti Hearst and Andrew Head and Daniel S Weld", "pages": "476-490", "abstract": "Scholars need to keep up with an exponentially increasing flood of scientific papers. To aid this challenge, we introduce Scim, a novel intelligent interface that helps experienced researchers skim \u2013 or rapidly review \u2013 a paper to attain a cursory understanding of its contents. Scim supports the skimming process by highlighting salient paper contents in order to direct a reader\u2019s attention. The system\u2019s highlights are faceted by content type, evenly distributed across a paper, and have a density configurable by readers at both the global and local level. We evaluate Scim with both an in-lab usability study and a longitudinal diary study, revealing how its highlights facilitate the more efficient construction of a conceptualization of a paper. We conclude by discussing design considerations and tensions for the design of future intelligent skimming tools. "}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:bEWYMUwI8FkC", "num_citations": 43, "citedby_url": "/scholar?hl=en&cites=6553069583172565327,5500407485025447174", "cites_id": ["6553069583172565327", "5500407485025447174"], "pub_url": "https://dl.acm.org/doi/abs/10.1145/3581641.3584034", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:T_lq9RAu8VoJ:scholar.google.com/", "cites_per_year": {"2022": 3, "2023": 13, "2024": 27}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Searching for scientific evidence in a pandemic: An overview of TREC-COVID", "pub_year": 2021, "citation": "Journal of Biomedical Informatics 121, 103865, 2021", "author": "Kirk Roberts and Tasmeer Alam and Steven Bedrick and Dina Demner-Fushman and Kyle Lo and Ian Soboroff and Ellen Voorhees and Lucy Lu Wang and William R Hersh", "volume": "121", "pages": "103865", "publisher": "Academic Press", "abstract": "We present an overview of the TREC-COVID Challenge, an information retrieval (IR) shared task to evaluate search on scientific literature related to COVID-19. The goals of TREC-COVID include the construction of a pandemic search test collection and the evaluation of IR methods for COVID-19. The challenge was conducted over five rounds from April to July 2020, with participation from 92 unique teams and 556 individual submissions. A total of 50 topics (sets of related queries) were used in the evaluation, starting at 30 topics for Round 1 and adding 5 new topics per round to target emerging topics at that state of the still-emerging pandemic. This paper provides a comprehensive overview of the structure and results of TREC-COVID. Specifically, the paper provides details on the background, task structure, topic structure, corpus, participation, pooling, assessment, judgments, results, top-performing systems \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:qxL8FJ1GzNcC", "num_citations": 41, "citedby_url": "/scholar?hl=en&cites=10923488131665974770", "cites_id": ["10923488131665974770"], "pub_url": "https://www.sciencedirect.com/science/article/pii/S1532046421001945", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:8lUlpJQGmJcJ:scholar.google.com/", "cites_per_year": {"2020": 1, "2021": 5, "2022": 8, "2023": 6, "2024": 18, "2025": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Combining Distant and Direct Supervision for Neural Relation Extraction", "pub_year": 2019, "citation": "Proceedings of the 2019 Conference of the North American Chapter of the \u2026, 2019", "author": "Iz Beltagy and Kyle Lo and Waleed Ammar", "conference": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)", "pages": "1858-1867", "abstract": "In relation extraction with distant supervision, noisy labels make it difficult to train quality models. Previous neural models addressed this problem using an attention mechanism that attends to sentences that are likely to express the relations. We improve such models by combining the distant supervision data with an additional directly-supervised data, which we use as supervision for the attention weights. We find that joint training on both types of supervision leads to a better model because it improves the model's ability to identify noisy sentences. In addition, we find that sigmoidal attention weights with max pooling achieves better performance over the commonly used weighted average attention in this setup. Our proposed method achieves a new state-of-the-art result on the widely used FB-NYT dataset."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:qjMakFHDy7sC", "num_citations": 41, "citedby_url": "/scholar?hl=en&cites=17147872236474473626,11411750052991620055", "cites_id": ["17147872236474473626", "11411750052991620055"], "pub_url": "https://arxiv.org/abs/1810.12956", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:mhzYHPJ6-e0J:scholar.google.com/", "cites_per_year": {"2019": 4, "2020": 20, "2021": 8, "2022": 5, "2023": 3, "2024": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Discourse Understanding and Factual Consistency in Abstractive Summarization", "pub_year": 2021, "citation": "Proceedings of the 16th Conference of the European Chapter of the \u2026, 2021", "author": "Saadia Gabriel and Antoine Bosselut and Jeff Da and Ari Holtzman and Jan Buys and Kyle Lo and Asli Celikyilmaz and Yejin Choi", "conference": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume", "pages": "435-447", "abstract": "We introduce a general framework for abstractive summarization with factual consistency and distinct modeling of the narrative flow in an output summary. Our work addresses current limitations of models for abstractive summarization that often hallucinate information or generate summaries with coherence issues. To generate abstractive summaries with factual consistency and narrative flow, we propose Cooperative Generator -- Discriminator Networks (Co-opNet), a novel transformer-based framework where a generator works with a discriminator architecture to compose coherent long-form summaries. We explore four different discriminator objectives which each capture a different aspect of coherence, including whether salient spans of generated abstracts are hallucinated or appear in the input context, and the likelihood of sentence adjacency in generated abstracts. We measure the ability of Co-opNet to learn these objectives with arXiv scientific papers, using the abstracts as a proxy for gold long-form scientific article summaries. Empirical results from automatic and human evaluations demonstrate that Co-opNet learns to summarize with considerably improved global coherence compared to competitive baselines."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:ULOm3_A8WrAC", "num_citations": 40, "citedby_url": "/scholar?hl=en&cites=7991106706017804040,4699061227185991634", "cites_id": ["7991106706017804040", "4699061227185991634"], "pub_url": "https://arxiv.org/abs/1907.01272", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:CN9u3jUd5m4J:scholar.google.com/", "cites_per_year": {"2019": 2, "2020": 5, "2021": 4, "2022": 14, "2023": 8, "2024": 7}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "peS2o (Pretraining Efficiently on S2ORC) Dataset", "pub_year": 2023, "citation": "Allen Institute for AI, Tech. Rep, 2023", "author": "Luca Soldaini and Kyle Lo", "journal": "Allen Institute for AI, Tech. Rep"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:xtRiw3GOFMkC", "num_citations": 35, "citedby_url": "/scholar?hl=en&cites=2312374705071487035", "cites_id": ["2312374705071487035"], "pub_url": "https://scholar.google.com/scholar?cluster=2312374705071487035&hl=en&oi=scholarr", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:O3hi84o0FyAJ:scholar.google.com/", "cites_per_year": {"2023": 5, "2024": 25, "2025": 5}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Followir: Evaluating and teaching information retrieval models to follow instructions", "pub_year": 2024, "citation": "arXiv preprint arXiv:2403.15246, 2024", "author": "Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini", "journal": "arXiv preprint arXiv:2403.15246", "abstract": "Modern Language Models (LMs) are capable of following long and complex instructions that enable a large and diverse set of user requests. While Information Retrieval (IR) models use these LMs as the backbone of their architectures, virtually none of them allow users to provide detailed instructions alongside queries, thus limiting their ability to satisfy complex information needs. In this work, we study the use of instructions in IR systems. First, we introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR repurposes detailed instructions -- also known as narratives -- developed for professional assessors to evaluate retrieval systems. In particular, we build our benchmark from three collections curated for shared tasks at the Text REtrieval Conference (TREC). These collections contains hundreds to thousands of labeled documents per query, making them suitable for our exploration. Through this process, we can measure how well IR models follow instructions, through a new pairwise evaluation framework. Our results indicate that existing retrieval models fail to correctly use instructions, using them for basic keywords and struggling to understand long-form information. However, we show that it is possible for IR models to learn to follow complex instructions: our new FollowIR-7B model has significant improvements after fine-tuning on our training set."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:g5m5HwL7SMYC", "num_citations": 31, "citedby_url": "/scholar?hl=en&cites=4796907996655810015", "cites_id": ["4796907996655810015"], "pub_url": "https://arxiv.org/abs/2403.15246", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:33XmaGgKkkIJ:scholar.google.com/", "cites_per_year": {"2024": 18, "2025": 12}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Document-level definition detection in scholarly documents: Existing models, error analyses, and future directions", "pub_year": 2020, "citation": "arXiv preprint arXiv:2010.05129, 2020", "author": "Dongyeop Kang and Andrew Head and Risham Sidhu and Kyle Lo and Daniel S Weld and Marti A Hearst", "journal": "arXiv preprint arXiv:2010.05129", "abstract": "The task of definition detection is important for scholarly papers, because papers often make use of technical terminology that may be unfamiliar to readers. Despite prior work on definition detection, current approaches are far from being accurate enough to use in real-world applications. In this paper, we first perform in-depth error analysis of the current best performing definition detection system and discover major causes of errors. Based on this analysis, we develop a new definition detection system, HEDDEx, that utilizes syntactic features, transformer encoders, and heuristic filters, and evaluate it on a standard sentence-level benchmark. Because current benchmarks evaluate randomly sampled sentences, we propose an alternative evaluation that assesses every sentence within a document. This allows for evaluating recall in addition to precision. HEDDEx outperforms the leading system on both the sentence-level and the document-level tasks, by 12.7 F1 points and 14.4 F1 points, respectively. We note that performance on the high-recall document-level task is much lower than in the standard evaluation approach, due to the necessity of incorporation of document structure as features. We discuss remaining challenges in document-level definition detection, ideas for improvements, and potential issues for the development of reading aid applications."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:3fE2CSJIrl8C", "num_citations": 31, "citedby_url": "/scholar?hl=en&cites=14321563456329688429", "cites_id": ["14321563456329688429"], "pub_url": "https://arxiv.org/abs/2010.05129", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:bSW_qhVqwMYJ:scholar.google.com/", "cites_per_year": {"2020": 1, "2021": 8, "2022": 5, "2023": 8, "2024": 8, "2025": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "FABLES: Evaluating faithfulness and content selection in book-length summarization", "pub_year": 2024, "citation": "arXiv preprint arXiv:2404.01261, 2024", "author": "Yekyung Kim and Yapei Chang and Marzena Karpinska and Aparna Garimella and Varun Manjunatha and Kyle Lo and Tanya Goyal and Mohit Iyyer", "journal": "arXiv preprint arXiv:2404.01261", "abstract": "While long-context large language models (LLMs) can technically summarize book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:SeFeTyx0c_EC", "num_citations": 28, "citedby_url": "/scholar?hl=en&cites=5002484658813919587,18240944643778498574", "cites_id": ["5002484658813919587", "18240944643778498574"], "pub_url": "https://arxiv.org/abs/2404.01261", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:Y-Hx-kplbEUJ:scholar.google.com/", "cites_per_year": {"2024": 20, "2025": 8}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "2 OLMo 2 Furious", "pub_year": 2024, "citation": "arXiv preprint arXiv:2501.00656, 2024", "author": "Team OLMo and Pete Walsh and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Shane Arora and Akshita Bhagia and Yuling Gu and Shengyi Huang and Matt Jordan and Nathan Lambert and Dustin Schwenk and Oyvind Tafjord and Taira Anderson and David Atkinson and Faeze Brahman and Christopher Clark and Pradeep Dasigi and Nouha Dziri and Michal Guerquin and Hamish Ivison and Pang Wei Koh and Jiacheng Liu and Saumya Malik and William Merrill and Lester James V Miranda and Jacob Morrison and Tyler Murray and Crystal Nam and Valentina Pyatkin and Aman Rangapur and Michael Schmitz and Sam Skjonsberg and David Wadden and Christopher Wilhelm and Michael Wilson and Luke Zettlemoyer and Ali Farhadi and Noah A Smith and Hannaneh Hajishirzi", "journal": "arXiv preprint arXiv:2501.00656", "abstract": "We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from T\\\"ulu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:b0M2c_1WBrUC", "num_citations": 24, "citedby_url": "/scholar?hl=en&cites=2928412902057166176,17764035793925069907", "cites_id": ["2928412902057166176", "17764035793925069907"], "pub_url": "https://arxiv.org/abs/2501.00656", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:YMXJ3gTQoygJ:scholar.google.com/", "cites_per_year": {"2025": 24}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "The Semantic Reader Project", "pub_year": 2024, "citation": "Communications of the ACM 67 (10), 50-61, 2024", "author": "Kyle Lo and Joseph Chee Chang and Andrew Head and Jonathan Bragg and Amy X Zhang and Cassidy Trier and Chloe Anastasiades and Tal August and Russell Authur and Danielle Bragg and Erin Bransom and Isabel Cachola and Stefan Candra and Yoganand Chandrasekhar and Yen-Sung Chen and Evie Yu-Yen Cheng and Yvonne Chou and Doug Downey and Rob Evans and Raymond Fok and Fangzhou Hu and Regan Huff and Dongyeop Kang and Tae Soo Kim and Rodney Kinney and Aniket Kittur and Hyeonsu B Kang and Egor Klevak and Bailey Kuehl and Michael J Langan and Matt Latzke and Jaron Lochner and Kelsey MacMillan and Eric Marsh and Tyler Murray and Aakanksha Naik and Ngoc-Uyen Nguyen and Srishti Palani and Soya Park and Caroline Paulic and Napol Rachatasumrit and Smita Rao and Paul Sayre and Zejiang Shen and Pao Siangliulue and Luca Soldaini and Huy Tran and Madeleine van Zuylen and Lucy Lu Wang and Christopher Wilhelm and Caroline Wu and Jiangjiang Yang and Angele Zamarron and Marti A Hearst and Daniel S Weld", "journal": "Communications of the ACM", "volume": "67", "number": "10", "pages": "50-61", "publisher": "ACM", "abstract": "Scholarly publications are key to the transfer of knowledge from scholars to others. However, research papers are information-dense, and as the volume of the scientific literature grows, the greater the need for new technology to support scholars. In contrast to the process of finding papers, which has been transformed by Internet technology, the experience of reading research papers has changed little in decades. For instance, the PDF format for sharing papers remains widely used due to its portability but has significant downsides, inter alia, static content and poor accessibility for low-vision readers. This paper explores the question \u201cCan recent advances in AI and HCI power intelligent, interactive, and accessible reading interfaces\u2014even for legacy PDFs?\u201d We describe the Semantic Reader Project, a collaborative effort across multiple institutions to explore automatic creation of dynamic reading interfaces for \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:D03iK_w7-QYC", "num_citations": 24, "citedby_url": "/scholar?hl=en&cites=14641757477475336257,556385340848037399,3307214393194246445", "cites_id": ["14641757477475336257", "556385340848037399", "3307214393194246445"], "pub_url": "https://dl.acm.org/doi/abs/10.1145/3659096", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:QUBWtNn4McsJ:scholar.google.com/", "cites_per_year": {"2023": 8, "2024": 12, "2025": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. Olmoe: Open mixture-of-experts language models", "pub_year": 2024, "citation": "arXiv preprint arXiv:2409.02060 1, 2024", "author": "Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison", "journal": "arXiv preprint arXiv:2409.02060", "volume": "1"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:l7t_Zn2s7bgC", "num_citations": 24, "citedby_url": "/scholar?hl=en&cites=8938374356693701881", "cites_id": ["8938374356693701881"], "pub_url": "https://scholar.google.com/scholar?cluster=8938374356693701881&hl=en&oi=scholarr", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:-VQwsB58C3wJ:scholar.google.com/", "cites_per_year": {"2024": 12, "2025": 12}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Exploring the role of local and global explanations in recommender systems", "pub_year": 2022, "citation": "Chi conference on human factors in computing systems extended abstracts, 1-7, 2022", "author": "Marissa Radensky and Doug Downey and Kyle Lo and Zoran Popovic and Daniel S Weld", "pages": "1-7", "abstract": " Explanations are well-known to improve recommender systems\u2019 transparency. These explanations may be local, explaining individual recommendations, or global, explaining the recommender model overall. Despite their widespread use, there has been little investigation into the relative benefits of the two explanation approaches. We conducted a 30-participant exploratory study and a 30-participant controlled user study with a research-paper recommender to analyze how providing local, global, or both explanations influences user understanding of system behavior. Our results provide evidence suggesting that both are more helpful than either alone for explaining how to improve recommendations, yet both appeared less helpful than global alone for efficiently identifying false positive and negative recommendations. However, we note that the two explanation approaches may be better compared in a higher \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:mVmsd5A6BfQC", "num_citations": 23, "citedby_url": "/scholar?hl=en&cites=4517974356887139581", "cites_id": ["4517974356887139581"], "pub_url": "https://dl.acm.org/doi/abs/10.1145/3491101.3519795", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:_fSUhr0Rsz4J:scholar.google.com/", "cites_per_year": {"2022": 3, "2023": 5, "2024": 14, "2025": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "One thousand and one pairs: A\" novel\" challenge for long-context language models", "pub_year": 2024, "citation": "arXiv preprint arXiv:2406.16264, 2024", "author": "Marzena Karpinska and Katherine Thai and Kyle Lo and Tanya Goyal and Mohit Iyyer", "journal": "arXiv preprint arXiv:2406.16264", "abstract": "Synthetic long-context LLM benchmarks (e.g., \"needle-in-the-haystack\") test only surface-level retrieval capabilities, but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? We address this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, our annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. Our experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that we evaluate: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest accuracy at 55.8%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:zA6iFVUQeVQC", "num_citations": 20, "citedby_url": "/scholar?hl=en&cites=8574083646815710650,4979623433804239820", "cites_id": ["8574083646815710650", "4979623433804239820"], "pub_url": "https://arxiv.org/abs/2406.16264", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:uq3vOqZD_XYJ:scholar.google.com/", "cites_per_year": {"2024": 13, "2025": 7}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "PaperMage: A Unified Toolkit for Processing, Representing, and Manipulating Visually-Rich Scientific Documents", "pub_year": 2023, "citation": "EMNLP System Demonstrations 2023 (\ud83c\udfc6 Best Paper Award \ud83c\udfc6 ), 2023", "author": "Kyle Lo and Zejiang Shen and Benjamin Newman and Joseph Z Chang and Russell Authur and Erin Bransom and Stefan Candra and Yoganand Chandrasekhar and Regan Huff and Bailey Kuehl and Amanpreet Singh and Chris Wilhelm and Angele Zamarron and Marti A Hearst and Daniel S Weld and Doug Downey and Luca Soldaini", "conference": "EMNLP System Demonstrations 2023 (\ud83c\udfc6 Best Paper Award \ud83c\udfc6 )", "abstract": "Despite growing interest in applying natural language processing (NLP) and computer vision (CV) models to the scholarly domain, scientific documents remain challenging to work with. They\u2019re often in difficult-to-use PDF formats, and the ecosystem of models to process them is fragmented and incomplete. We introduce PaperMage, an open-source Python toolkit for analyzing and processing visually-rich, structured scientific documents. PaperMage offers clean and intuitive abstractions for seamlessly representing and manipulating both textual and visual document elements. PaperMage achieves this by integrating disparate state-of-the-art NLP and CV models into a unified framework, and provides turn-key recipes for common scientific document processing use-cases. PaperMage has powered multiple research prototypes of AI applications over scientific documents, along with Semantic Scholar\u2019s large-scale production system for processing millions of PDFs. GitHub: https://github. com/allenai/papermage"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:RYcK_YlVTxYC", "num_citations": 20, "citedby_url": "/scholar?hl=en&cites=13495243293110981053", "cites_id": ["13495243293110981053"], "pub_url": "https://aclanthology.org/2023.emnlp-demo.45/", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:vVFaM0a8SLsJ:scholar.google.com/", "cites_per_year": {"2024": 17, "2025": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Explanation-based tuning of opaque machine learners with application to paper recommendation", "pub_year": 2020, "citation": "arXiv preprint arXiv:2003.04315, 2020", "author": "Benjamin Charles Germain Lee and Kyle Lo and Doug Downey and Daniel S Weld", "journal": "arXiv preprint arXiv:2003.04315", "abstract": "Research in human-centered AI has shown the benefits of machine-learning systems that can explain their predictions. Methods that allow users to tune a model in response to the explanations are similarly useful. While both capabilities are well-developed for transparent learning models (eg, linear models and GA2Ms), and recent techniques (eg, LIME and SHAP) can generate explanations for opaque models, no method currently exists for tuning of opaque models in response to explanations. This paper introduces LIMEADE, a general framework for tuning an arbitrary machine learning model based on an explanation of the model\u2019s prediction. We apply our framework to Semantic Sanity, 1 a neural recommender system for scientific papers and report on a detailed user study, showing that our framework leads to significantly higher perceived user control, trust, and satisfaction."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:_Qo2XoVZTnwC", "num_citations": 20, "citedby_url": "/scholar?hl=en&cites=301228772590681700,1522422439555775855,16523141639716543611,855194888258477106", "cites_id": ["301228772590681700", "1522422439555775855", "16523141639716543611", "855194888258477106"], "pub_url": "https://www.academia.edu/download/85753082/2003.04315v1.pdf", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ZP5kAvktLgQJ:scholar.google.com/", "cites_per_year": {"2021": 3, "2022": 5, "2023": 4, "2024": 6, "2025": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Citation count analysis for papers with preprints", "pub_year": 2018, "citation": "arXiv preprint arXiv:1805.05238, 2018", "author": "Sergey Feldman and Kyle Lo and Waleed Ammar", "journal": "arXiv preprint arXiv:1805.05238", "abstract": "We explore the degree to which papers prepublished on arXiv garner more citations, in an attempt to paint a sharper picture of fairness issues related to prepublishing. A paper's citation count is estimated using a negative-binomial generalized linear model (GLM) while observing a binary variable which indicates whether the paper has been prepublished. We control for author influence (via the authors' h-index at the time of paper writing), publication venue, and overall time that paper has been available on arXiv. Our analysis only includes papers that were eventually accepted for publication at top-tier CS conferences, and were posted on arXiv either before or after the acceptance notification. We observe that papers submitted to arXiv before acceptance have, on average, 65\\% more citations in the following year compared to papers submitted after. We note that this finding is not causal, and discuss possible next steps."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:u5HHmVD_uO8C", "num_citations": 20, "citedby_url": "/scholar?hl=en&cites=4698076825611678440", "cites_id": ["4698076825611678440"], "pub_url": "https://arxiv.org/abs/1805.05238", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:6DIUPvvrMkEJ:scholar.google.com/", "cites_per_year": {"2019": 5, "2020": 5, "2021": 3, "2022": 4, "2023": 0, "2024": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Beyond summarization: Designing ai support for real-world expository writing tasks", "pub_year": 2023, "citation": "arXiv preprint arXiv:2304.02623, 2023", "author": "Zejiang Shen and Tal August and Pao Siangliulue and Kyle Lo and Jonathan Bragg and Jeff Hammerbacher and Doug Downey and Joseph Chee Chang and David Sontag", "journal": "arXiv preprint arXiv:2304.02623", "abstract": "Large language models have introduced exciting new opportunities and challenges in designing and developing new AI-assisted writing support tools. Recent work has shown that leveraging this new technology can transform writing in many scenarios such as ideation during creative writing, editing support, and summarization. However, AI-supported expository writing--including real-world tasks like scholars writing literature reviews or doctors writing progress notes--is relatively understudied. In this position paper, we argue that developing AI supports for expository writing has unique and exciting research challenges and can lead to high real-world impacts. We characterize expository writing as evidence-based and knowledge-generating: it contains summaries of external documents as well as new information or knowledge. It can be seen as the product of authors' sensemaking process over a set of source documents, and the interplay between reading, reflection, and writing opens up new opportunities for designing AI support. We sketch three components for AI support design and discuss considerations for future research."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:O3NaXMp0MMsC", "num_citations": 19, "citedby_url": "/scholar?hl=en&cites=15011065935345715212", "cites_id": ["15011065935345715212"], "pub_url": "https://arxiv.org/abs/2304.02623", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:DIg_qO8EUtAJ:scholar.google.com/", "cites_per_year": {"2022": 1, "2023": 4, "2024": 12, "2025": 2}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "A question answering framework for decontextualizing user-facing snippets from scientific documents", "pub_year": 2023, "citation": "arXiv preprint arXiv:2305.14772, 2023", "author": "Benjamin Newman and Luca Soldaini and Raymond Fok and Arman Cohan and Kyle Lo", "journal": "arXiv preprint arXiv:2305.14772", "abstract": "Many real-world applications (e.g., note taking, search) require extracting a sentence or paragraph from a document and showing that snippet to a human outside of the source document. Yet, users may find snippets difficult to understand as they lack context from the original document. In this work, we use language models to rewrite snippets from scientific documents to be read on their own. First, we define the requirements and challenges for this user-facing decontextualization task, such as clarifying where edits occur and handling references to other documents. Second, we propose a framework that decomposes the task into three stages: question generation, question answering, and rewriting. Using this framework, we collect gold decontextualizations from experienced scientific article readers. We then conduct a range of experiments across state-of-the-art commercial and open-source language models to identify how to best provide missing-but-relevant information to models for our task. Finally, we develop QaDecontext, a simple prompting strategy inspired by our framework that improves over end-to-end prompting. We conclude with analysis that finds, while rewriting is easy, question generation and answering remain challenging for today's models."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:lSLTfruPkqcC", "num_citations": 18, "citedby_url": "/scholar?hl=en&cites=14962338181047274614,6175087826518579729", "cites_id": ["14962338181047274614", "6175087826518579729"], "pub_url": "https://arxiv.org/abs/2305.14772", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:dkyWQk3npM8J:scholar.google.com/", "cites_per_year": {"2023": 5, "2024": 11, "2025": 2}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "When do generative query and document expansions fail? a comprehensive study across methods, retrievers, and datasets", "pub_year": 2023, "citation": "arXiv preprint arXiv:2309.08541, 2023", "author": "Orion Weller and Kyle Lo and David Wadden and Dawn Lawrie and Benjamin Van Durme and Arman Cohan and Luca Soldaini", "journal": "arXiv preprint arXiv:2309.08541", "abstract": "Using large language models (LMs) for query or document expansion can improve generalization in information retrieval. However, it is unknown whether these techniques are universally beneficial or only effective in specific settings, such as for particular retrieval models, dataset domains, or query types. To answer this, we conduct the first comprehensive analysis of LM-based expansion. We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models. We show this trend holds across a set of eleven expansion techniques, twelve datasets with diverse distribution shifts, and twenty-four retrieval models. Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positives). Our results suggest the following recipe: use expansions for weaker models or when the target dataset significantly differs from training corpus in format; otherwise, avoid expansions to keep the relevance signal clear."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:blknAaTinKkC", "num_citations": 17, "citedby_url": "/scholar?hl=en&cites=4641666793342372867,15294649746642700711", "cites_id": ["4641666793342372867", "15294649746642700711"], "pub_url": "https://arxiv.org/abs/2309.08541", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:AwzmHluDakAJ:scholar.google.com/", "cites_per_year": {"2022": 1, "2023": 3, "2024": 12, "2025": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A", "pub_year": 2024, "citation": "Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha \u2026, 2024", "author": "Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and Jiasen Lu and Taira Anderson and Erin Bransom and Kiana Ehsani and Huong Ngo and YenSung Chen and Ajay Patel and Mark Yatskar and Chris Callison-Burch and Andrew Head and Rose Hendrix and Favyen Bastani and Eli VanderBilt and Nathan Lambert and Yvonne Chou and Arnavi Chheda and Jenna Sparks and Sam Skjonsberg and Michael Schmitz and Aaron Sarnat and Byron Bischoff and Pete Walsh and Chris Newell and Piper Wolters and Tanmay Gupta and Kuo-Hao Zeng and Jon Borchardt and Dirk Groeneveld and Jen Dumas and Crystal Nam and Sophie Lebrecht", "journal": "Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models", "volume": "6"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:OU6Ihb5iCvQC", "num_citations": 16, "citedby_url": "/scholar?hl=en&cites=6206746388225645642", "cites_id": ["6206746388225645642"], "pub_url": "https://scholar.google.com/scholar?cluster=6206746388225645642&hl=en&oi=scholarr", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:StwgkfTKIlYJ:scholar.google.com/", "cites_per_year": {"2024": 9, "2025": 7}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Paloma: A benchmark for evaluating language model fit", "pub_year": 2024, "citation": "Advances in Neural Information Processing Systems 37, 64338-64376, 2024", "author": "Ian Magnusson and Akshita Bhagia and Valentin Hofmann and Luca Soldaini and Ananya Harsh Jha and Oyvind Tafjord and Dustin Schwenk and Evan Walsh and Yanai Elazar and Kyle Lo and Dirk Groeneveld and Iz Beltagy and Hanna Hajishirzi and Noah A Smith and Kyle Richardson and Jesse Dodge", "journal": "Advances in Neural Information Processing Systems", "volume": "37", "pages": "64338-64376", "abstract": "Evaluations of language models (LMs) commonly report perplexity on monolithic data held out from training. Implicitly or explicitly, this data is composed of domains\u2014varying distributions of language. We introduce Perplexity Analysis for Language Model Assessment (Paloma), a benchmark to measure LM fit to 546 English and code domains, instead of assuming perplexity on one distribution extrapolates to others. We include two new datasets of the top 100 subreddits (eg, r/depression on Reddit) and programming languages (eg, Java on GitHub), both sources common in contemporary LMs. With our benchmark, we release 6 baseline 1B LMs carefully controlled to provide fair comparisons about which pretraining corpus is best and code for others to apply those controls to their own experiments. Our case studies demonstrate how the fine-grained results from Paloma surface findings such as that models pretrained without data beyond Common Crawl exhibit anomalous gaps in LM fit to many domains or that loss is dominated by the most frequently occurring strings in the vocabulary."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:35N4QoGY0k4C", "num_citations": 15, "citedby_url": "/scholar?hl=en&cites=12289944742459895662,12341219371612362640", "cites_id": ["12289944742459895662", "12341219371612362640"], "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/760b2d94398aa61468aa3bc11506d9ea-Abstract-Datasets_and_Benchmarks_Track.html", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:bm-A26CnjqoJ:scholar.google.com/", "cites_per_year": {"2023": 1, "2024": 13}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Decomposing complex queries for tip-of-the-tongue retrieval", "pub_year": 2023, "citation": "arXiv preprint arXiv:2305.15053, 2023", "author": "Kevin Lin and Kyle Lo and Joseph E Gonzalez and Dan Klein", "journal": "arXiv preprint arXiv:2305.15053", "abstract": "When re-finding items, users who forget or are uncertain about identifying details often rely on creative strategies for expressing their information needs -- complex queries that describe content elements (e.g., book characters or events), information beyond the document text (e.g., descriptions of book covers), or personal context (e.g., when they read a book). This retrieval setting, called tip of the tongue (TOT), is especially challenging for models heavily reliant on lexical and semantic overlap between query and document text. In this work, we introduce a simple yet effective framework for handling such complex queries by decomposing the query into individual clues, routing those as sub-queries to specialized retrievers, and ensembling the results. This approach allows us to take advantage of off-the-shelf retrievers (e.g., CLIP for retrieving images of book covers) or incorporate retriever-specific logic (e.g., date constraints). We show that our framework incorportating query decompositions into retrievers can improve gold book recall up to 7% relative again for Recall@5 on a new collection of 14,441 real-world query-book pairs from an online community for resolving TOT inquiries."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:hMod-77fHWUC", "num_citations": 15, "citedby_url": "/scholar?hl=en&cites=5465505046556657048", "cites_id": ["5465505046556657048"], "pub_url": "https://arxiv.org/abs/2305.15053", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:mEUd4uFf2UsJ:scholar.google.com/", "cites_per_year": {"2023": 3, "2024": 9, "2025": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Olmoe: Open mixture-of-experts language models", "pub_year": 2024, "citation": "arXiv preprint arXiv:2409.02060, 2024", "author": "Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A Smith and Pang Wei Koh and Amanpreet Singh and Hannaneh Hajishirzi", "journal": "arXiv preprint arXiv:2409.02060", "abstract": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:3s1wT3WcHBgC", "num_citations": 13, "citedby_url": "/scholar?hl=en&cites=7085139256371018019,1892632883289593350", "cites_id": ["7085139256371018019", "1892632883289593350"], "pub_url": "https://arxiv.org/abs/2409.02060", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:I3krcJ14U2IJ:scholar.google.com/", "cites_per_year": {"2024": 8, "2025": 5}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Open domain multi-document summarization: A comprehensive study of model brittleness under retrieval", "pub_year": 2022, "citation": "arXiv preprint arXiv:2212.10526, 2022", "author": "John Giorgi and Luca Soldaini and Bo Wang and Gary Bader and Kyle Lo and Lucy Lu Wang and Arman Cohan", "journal": "arXiv preprint arXiv:2212.10526", "abstract": "Multi-document summarization (MDS) assumes a set of topic-related documents are provided as input. In practice, this document set is not always available; it would need to be retrieved given an information need, i.e. a question or topic statement, a setting we dub \"open-domain\" MDS. We study this more challenging setting by formalizing the task and bootstrapping it using existing datasets, retrievers and summarizers. Via extensive automatic and human evaluation, we determine: (1) state-of-the-art summarizers suffer large reductions in performance when applied to open-domain MDS, (2) additional training in the open-domain setting can reduce this sensitivity to imperfect retrieval, and (3) summarizers are insensitive to the retrieval of duplicate documents and the order of retrieved documents, but highly sensitive to other errors, like the retrieval of irrelevant documents. Based on our results, we provide practical guidelines to enable future work on open-domain MDS, e.g. how to choose the number of retrieved documents to summarize. Our results suggest that new retrieval and summarization methods and annotated resources for training and evaluation are necessary for further progress in the open-domain setting."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:J_g5lzvAfSwC", "num_citations": 13, "citedby_url": "/scholar?hl=en&cites=263152454308810961", "cites_id": ["263152454308810961"], "pub_url": "https://arxiv.org/abs/2212.10526", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:0VDZUMPnpgMJ:scholar.google.com/", "cites_per_year": {"2024": 10, "2025": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Accord: A multi-document approach to generating diverse descriptions of scientific concepts", "pub_year": 2022, "citation": "arXiv preprint arXiv:2205.06982, 2022", "author": "Sonia K Murthy and Kyle Lo and Daniel King and Chandra Bhagavatula and Bailey Kuehl and Sophie Johnson and Jonathan Borchardt and Daniel S Weld and Tom Hope and Doug Downey", "journal": "arXiv preprint arXiv:2205.06982", "abstract": "Systems that can automatically define unfamiliar terms hold the promise of improving the accessibility of scientific texts, especially for readers who may lack prerequisite background knowledge. However, current systems assume a single \"best\" description per concept, which fails to account for the many potentially useful ways a concept can be described. We present ACCoRD, an end-to-end system tackling the novel task of generating sets of descriptions of scientific concepts. Our system takes advantage of the myriad ways a concept is mentioned across the scientific literature to produce distinct, diverse descriptions of target scientific concepts in terms of different reference concepts. To support research on the task, we release an expert-annotated resource, the ACCoRD corpus, which includes 1,275 labeled contexts and 1,787 hand-authored concept descriptions. We conduct a user study demonstrating that (1) users prefer descriptions produced by our end-to-end system, and (2) users prefer multiple descriptions to a single \"best\" description."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:QIV2ME_5wuYC", "num_citations": 13, "citedby_url": "/scholar?hl=en&cites=2435626468117814913", "cites_id": ["2435626468117814913"], "pub_url": "https://arxiv.org/abs/2205.06982", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:gZ6zRV4VzSEJ:scholar.google.com/", "cites_per_year": {"2022": 2, "2023": 4, "2024": 6}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Mitigating biases in cord-19 for analyzing covid-19 literature", "pub_year": 2020, "citation": "Frontiers in Research Metrics and Analytics 5, 2020", "author": "Anshul Kanakia and Kuansan Wang and Yuxiao Dong and Boya Xie and Kyle Lo and Zhihong Shen and Lucy Lu Wang and Chiyuan Huang and Darrin Eide and Sebastian Kohlmeier and Chieh-Han Wu", "journal": "Frontiers in Research Metrics and Analytics", "volume": "5", "abstract": "On the behest of the Office of Science and Technology Policy in the White House, six institutions, including ours, have created an open research dataset called COVID-19 Research Dataset (CORD-19) to facilitate the development of question-answering systems that can assist researchers in finding relevant research on COVID-19. As of May 27, 2020, CORD-19 includes more than 100,000 open access publications from major publishers and PubMed as well as preprint articles deposited into medRxiv, bioRxiv, and arXiv. Recent years, however, have also seen question-answering and other machine learning systems exhibit harmful behaviors to humans due to biases in the training data. It is imperative and only ethical for modern scientists to be vigilant in inspecting and be prepared to mitigate the potential biases when working with any datasets. This article describes a framework to examine biases in scientific document collections like CORD-19 by comparing their properties with those derived from the citation behaviors of the entire scientific community. In total, three expanded sets are created for the analyses: 1) the enclosure set CORD-19E composed of CORD-19 articles and their references and citations, mirroring the methodology used in the renowned \u201cA Century of Physics\u201d analysis; 2) the full closure graph CORD-19C that recursively includes references starting with CORD-19; and 3) the inflection closure CORD-19I, that is, a much smaller subset of CORD-19C but already appropriate for statistical analysis based on the theory of the scale-free nature of the citation network. Taken together, all these expanded datasets show much \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:MXK_kJrjxJIC", "num_citations": 12, "citedby_url": "/scholar?hl=en&cites=7004015334833065475", "cites_id": ["7004015334833065475"], "pub_url": "https://www.frontiersin.org/articles/10.3389/frma.2020.596624/full", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:Aw4dktZCM2EJ:scholar.google.com/", "cites_per_year": {"2021": 4, "2022": 5, "2023": 2, "2024": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Openscholar: Synthesizing scientific literature with retrieval-augmented lms", "pub_year": 2024, "citation": "arXiv preprint arXiv:2411.14199, 2024", "author": "Akari Asai and Jacqueline He and Rulin Shao and Weijia Shi and Amanpreet Singh and Joseph Chee Chang and Kyle Lo and Luca Soldaini and Sergey Feldman and Mike D'arcy and David Wadden and Matt Latzke and Minyang Tian and Pan Ji and Shengyan Liu and Hao Tong and Bohao Wu and Yanyu Xiong and Luke Zettlemoyer and Graham Neubig and Dan Weld and Doug Downey and Wen-tau Yih and Pang Wei Koh and Hannaneh Hajishirzi", "journal": "arXiv preprint arXiv:2411.14199", "abstract": "Scientific progress depends on researchers' ability to synthesize the growing body of literature. Can large language models (LMs) assist scientists in this task? We introduce OpenScholar, a specialized retrieval-augmented LM that answers scientific queries by identifying relevant passages from 45 million open-access papers and synthesizing citation-backed responses. To evaluate OpenScholar, we develop ScholarQABench, the first large-scale multi-domain benchmark for literature search, comprising 2,967 expert-written queries and 208 long-form answers across computer science, physics, neuroscience, and biomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and PaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o hallucinates citations 78 to 90% of the time, OpenScholar achieves citation accuracy on par with human experts. OpenScholar's datastore, retriever, and self-feedback inference loop also improves off-the-shelf LMs: for instance, OpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations, experts preferred OpenScholar-8B and OpenScholar-GPT4o responses over expert-written ones 51% and 70% of the time, respectively, compared to GPT4o's 32%. We open-source all of our code, models, datastore, data and a public demo."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:EUQCXRtRnyEC", "num_citations": 10, "citedby_url": "/scholar?hl=en&cites=16873248879131124102", "cites_id": ["16873248879131124102"], "pub_url": "https://arxiv.org/abs/2411.14199", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:hmFeD3XSKeoJ:scholar.google.com/", "cites_per_year": {"2024": 3, "2025": 7}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Overview and insights from the SCIVER shared task on scientific claim verification", "pub_year": 2021, "citation": "arXiv preprint arXiv:2107.08188, 2021", "author": "David Wadden and Kyle Lo", "journal": "arXiv preprint arXiv:2107.08188", "abstract": "We present an overview of the SciVer shared task, presented at the 2nd Scholarly Document Processing (SDP) workshop at NAACL 2021. In this shared task, systems were provided a scientific claim and a corpus of research abstracts, and asked to identify which articles SUPPORT or REFUTE the claim as well as provide evidentiary sentences justifying those labels. 11 teams made a total of 14 submissions to the shared task leaderboard, leading to an improvement of more than +23 F1 on the primary task evaluation metric. In addition to surveying the participating systems, we provide several insights into modeling approaches to support continued progress and future research on the important and challenging task of scientific claim verification."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:4DMP91E08xMC", "num_citations": 10, "citedby_url": "/scholar?hl=en&cites=12022788082142921217", "cites_id": ["12022788082142921217"], "pub_url": "https://arxiv.org/abs/2107.08188", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:AcLtrQ-G2aYJ:scholar.google.com/", "cites_per_year": {"2021": 2, "2022": 2, "2023": 4, "2024": 2}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "The responsible foundation model development cheatsheet: A review of tools & resources", "pub_year": 2024, "citation": "arXiv preprint arXiv:2406.16746, 2024", "author": "Shayne Longpre and Stella Biderman and Alon Albalak and Hailey Schoelkopf and Daniel McDuff and Sayash Kapoor and Kevin Klyman and Kyle Lo and Gabriel Ilharco and Nay San and Maribeth Rauh and Aviya Skowron and Bertie Vidgen and Laura Weidinger and Arvind Narayanan and Victor Sanh and David Adelani and Percy Liang and Rishi Bommasani and Peter Henderson and Sasha Luccioni and Yacine Jernite and Luca Soldaini", "abstract": "Foundation model development attracts a rapidly expanding body of contributors, scientists, and applications. To help shape responsible development practices, we introduce the Foundation Model Development Cheatsheet: a growing collection of 250+ tools and resources spanning text, vision, and speech modalities. We draw on a large body of prior work to survey resources (e.g. software, documentation, frameworks, guides, and practical tools) that support informed data selection, processing, and understanding, precise and limitation-aware artifact documentation, efficient model training, advance awareness of the environmental impact from training, careful model evaluation of capabilities, risks, and claims, as well as responsible model release, licensing and deployment practices. We hope this curated collection of resources helps guide more responsible development. The process of curating this list, enabled us to review the AI development ecosystem, revealing what tools are critically missing, misused, or over-used in existing practices. We find that (i) tools for data sourcing, model evaluation, and monitoring are critically under-serving ethical and real-world needs, (ii) evaluations for model safety, capabilities, and environmental impact all lack reproducibility and transparency, (iii) text and particularly English-centric analyses continue to dominate over multilingual and multi-modal analyses, and (iv) evaluation of systems, rather than just models, is needed so that capabilities and impact are assessed in context."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:cFHS6HbyZ2cC", "num_citations": 9, "citedby_url": "/scholar?hl=en&cites=18319085535215706877,13608187154271284184", "cites_id": ["18319085535215706877", "13608187154271284184"], "pub_url": "https://arxiv.org/abs/2406.16746", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:_WKHUDx3Ov4J:scholar.google.com/", "cites_per_year": {"2024": 6, "2025": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "LLMs as Research Tools: A Large Scale Survey of Researchers' Usage and Perceptions", "pub_year": 2024, "citation": "arXiv preprint arXiv:2411.05025, 2024", "author": "Zhehui Liao and Maria Antoniak and Inyoung Cheong and Evie Yu-Yen Cheng and Ai-Heng Lee and Kyle Lo and Joseph Chee Chang and Amy X Zhang", "journal": "arXiv preprint arXiv:2411.05025", "abstract": "The rise of large language models (LLMs) has led many researchers to consider their usage for scientific work. Some have found benefits using LLMs to augment or automate aspects of their research pipeline, while others have urged caution due to risks and ethical concerns. Yet little work has sought to quantify and characterize how researchers use LLMs and why. We present the first large-scale survey of 816 verified research article authors to understand how the research community leverages and perceives LLMs as research tools. We examine participants' self-reported LLM usage, finding that 81% of researchers have already incorporated LLMs into different aspects of their research workflow. We also find that traditionally disadvantaged groups in academia (non-White, junior, and non-native English speaking researchers) report higher LLM usage and perceived benefits, suggesting potential for improved research equity. However, women, non-binary, and senior researchers have greater ethical concerns, potentially hindering adoption."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:bFI3QPDXJZMC", "num_citations": 8, "citedby_url": "/scholar?hl=en&cites=6417324396648603455", "cites_id": ["6417324396648603455"], "pub_url": "https://arxiv.org/abs/2411.05025", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:Px-wzonqDlkJ:scholar.google.com/", "cites_per_year": {"2024": 3, "2025": 5}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Know Your Audience: The benefits and pitfalls of generating plain language summaries beyond the\" general\" audience", "pub_year": 2024, "citation": "Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems \u2026, 2024", "author": "Tal August and Kyle Lo and Noah A Smith and Katharina Reinecke", "pages": "1-26", "abstract": " Language models (LMs) show promise as tools for communicating science to the general public by simplifying and summarizing complex language. Because models can be prompted to generate text for a specific audience (e.g., college-educated adults), LMs might be used to create multiple versions of plain language summaries for people with different familiarities of scientific topics. However, it is not clear what the benefits and pitfalls of adaptive plain language are. When is simplifying necessary, what are the costs in doing so, and do these costs differ for readers with different background knowledge? Through three within-subjects studies in which we surface summaries for different envisioned audiences to participants of different backgrounds, we found that while simpler text led to the best reading experience for readers with little to no familiarity in a topic, high familiarity readers tended to ignore certain details \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:HoB7MX3m0LUC", "num_citations": 8, "citedby_url": "/scholar?hl=en&cites=5209297333952423720", "cites_id": ["5209297333952423720"], "pub_url": "https://dl.acm.org/doi/abs/10.1145/3613904.3642289", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:KKefP1MkS0gJ:scholar.google.com/", "cites_per_year": {"2024": 4, "2025": 4}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Overview of the 2020 epidemic question answering track", "pub_year": 2020, "citation": "Text Analysis Conference, 2020", "author": "TRAVIS R Goodwin and Dina Demner-Fushman and Kyle Lo and Lucy Lu Wang and WILLIAM R Hersh and HT Dang and Ian M Soboroff", "journal": "Text Analysis Conference", "abstract": "1 OBJECTIVEIn response to the COVID-19 pandemic, we organized a new track for TAC 2020: Epidemic Question Answering (EPIC-QA). This track challenges teams to develop systems capable of automatically answering ad-hoc questions about the disease COVID-19, its causal virus SARS-CoV-2, related coronaviruses, and the recommended response to the pandemic. While COVID-19 has been an impetus for a large body of emergent scientific research and inquiry, the response to COVID-19 raises questions for consumers. The rapid increase in coronavirus literature and evolving guidelines on community response creates a challenging burden not only for the scientific and medical communities but also the general public to stay up-to-date on the latest developments. Consequently, the goal of the track is to evaluate systems on their ability to provide timely and accurate expert-level answers as expected by the scientific and medical communities as well as answers in consumer-friendly language for the general public.While there is overlap in the types of questions asked by different stakeholders, the answers to such questions should vary based on the background knowledge of the user. For example, consider the simple question from the general health domain illustrated in Figure 1, How does Tylenol work? In the eyes of an expert, an answer should indicate that the exact mechanism is unknown, but may elaborate on the pathways involved.[6] By contrast, for a consumer, this information would be unhelpful; instead, a more appropriate answer would provide a general overview of how the class of drugs works rather than the exact \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:NaGl4SEjCO4C", "num_citations": 8, "citedby_url": "/scholar?hl=en&cites=9283289688834686380", "cites_id": ["9283289688834686380"], "pub_url": "https://tac.nist.gov/publications/2020/additional.papers/TAC2020.EPIC-QA.overview.notebook.pdf", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:rL0Jesne1IAJ:scholar.google.com/", "cites_per_year": {"2021": 2, "2022": 5, "2023": 0, "2024": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Sciriff: A resource to enhance language model instruction-following over scientific literature", "pub_year": 2024, "citation": "arXiv preprint arXiv:2406.07835, 2024", "author": "David Wadden and Kejian Shi and Jacob Morrison and Aakanksha Naik and Shruti Singh and Nitzan Barzilay and Kyle Lo and Tom Hope and Luca Soldaini and Shannon Zejiang Shen and Doug Downey and Hannaneh Hajishirzi and Arman Cohan", "journal": "arXiv preprint arXiv:2406.07835", "abstract": "We present SciRIFF (Scientific Resource for Instruction-Following and Finetuning), a dataset of 137K instruction-following demonstrations for 54 tasks covering five essential scientific literature understanding capabilities: information extraction, summarization, question answering, claim verification, and classification. SciRIFF demonstrations are notable for their long input contexts, detailed task specifications, and complex structured outputs. While instruction-following resources are available in specific domains such as clinical medicine and chemistry, SciRIFF is the first dataset focused on extracting and synthesizing information from research literature across a wide range of scientific fields. To demonstrate the utility of SciRIFF, we develop a sample-efficient strategy to adapt a general instruction-following model for science by performing additional finetuning on a mix of general-domain and SciRIFF demonstrations. In evaluations on nine held-out scientific tasks, our model -- called SciTulu -- improves over a strong LLM baseline by 28.1% and 6.5% at the 7B and 70B scales respectively, while maintaining general instruction-following performance within 2% of the baseline. We are optimistic that SciRIFF will facilitate the development and evaluation of LLMs to help researchers navigate the ever-growing body of scientific literature. We release our dataset, model checkpoints, and data processing and evaluation code to enable further research."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:dfsIfKJdRG4C", "num_citations": 7, "citedby_url": "/scholar?hl=en&cites=16596710802798404331", "cites_id": ["16596710802798404331"], "pub_url": "https://arxiv.org/abs/2406.07835", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:6-KvwIpcU-YJ:scholar.google.com/", "cites_per_year": {"2024": 4, "2025": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "The rise of open science: Tracking the evolution and perceived value of data and methods link-sharing practices", "pub_year": 2023, "citation": "arXiv preprint arXiv:2310.03193, 2023", "author": "Hancheng Cao and Jesse Dodge and Kyle Lo and Daniel A McFarland and Lucy Lu Wang", "journal": "arXiv preprint arXiv:2310.03193", "abstract": "In recent years, funding agencies and journals increasingly advocate for open science practices (e.g. data and method sharing) to improve the transparency, access, and reproducibility of science. However, quantifying these practices at scale has proven difficult. In this work, we leverage a large-scale dataset of 1.1M papers from arXiv that are representative of the fields of physics, math, and computer science to analyze the adoption of data and method link-sharing practices over time and their impact on article reception. To identify links to data and methods, we train a neural text classification model to automatically classify URL types based on contextual mentions in papers. We find evidence that the practice of link-sharing to methods and data is spreading as more papers include such URLs over time. Reproducibility efforts may also be spreading because the same links are being increasingly reused across papers (especially in computer science); and these links are increasingly concentrated within fewer web domains (e.g. Github) over time. Lastly, articles that share data and method links receive increased recognition in terms of citation count, with a stronger effect when the shared links are active (rather than defunct). Together, these findings demonstrate the increased spread and perceived value of data and method sharing practices in open science."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:RGFaLdJalmkC", "num_citations": 7, "citedby_url": "/scholar?hl=en&cites=8593928012951126982", "cites_id": ["8593928012951126982"], "pub_url": "https://arxiv.org/abs/2310.03193", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:xgM7xf7DQ3cJ:scholar.google.com/", "cites_per_year": {"2024": 6, "2025": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Overview of the third workshop on scholarly document processing", "pub_year": 2022, "citation": "Proceedings of the Third Workshop on Scholarly Document Processing, 1-6, 2022", "author": "Arman Cohan and Guy Feigenblat and Dayne Freitag and Tirthankar Ghosal and Drahomira Herrmannova and Petr Knoth and Kyle Lo and Philipp Mayr and Michal Shmueli-Scheuer and Anita de Waard and Lucy Lu Wang", "conference": "Proceedings of the Third Workshop on Scholarly Document Processing", "pages": "1-6", "abstract": "With the ever-increasing pace of research and high volume of scholarly communication, scholars face a daunting task. Not only must they keep up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. These needs have motivated an increasing focus on computational methods for enhancing search, summarization, and analysis of scholarly documents. However, the various strands of research on scholarly document processing remain fragmented. To reach out to the broader NLP and AI/ML community, pool distributed efforts in this area, and enable shared access to published research, we held the 3rd Workshop on Scholarly Document Processing (SDP) at COLING as a hybrid event (https://sdproc. org/2022/). The SDP workshop consisted of a research track, three invited talks and five Shared Tasks: 1) MSLR22: Multi-Document Summarization for Literature Reviews, 2) DAGPap22: Detecting automatically generated scientific papers, 3) SV-Ident 2022: Survey Variable Identification in Social Science Publications, 4) SKGG: Scholarly Knowledge Graph Generation, 5) MuP 2022: Multi Perspective Scientific Document Summarization. The program was geared towards NLP, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and providing solutions to open challenges."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:isC4tDSrTZIC", "num_citations": 7, "citedby_url": "/scholar?hl=en&cites=9753835167228998427,4189260747128592370", "cites_id": ["9753835167228998427", "4189260747128592370"], "pub_url": "https://aclanthology.org/2022.sdp-1.1/", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:G7NlFGiVXIcJ:scholar.google.com/", "cites_per_year": {"2023": 3, "2024": 4}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Exploring the challenges of open domain multi-document summarization", "pub_year": 2022, "citation": "ArXiv, abs/2212.10526, 2022", "author": "John Giorgi and Luca Soldaini and Bo Wang and Gary Bader and Kyle Lo and Lucy Lu Wang and Arman Cohan", "journal": "ArXiv, abs/2212.10526", "abstract": "Multi-document summarization (MDS) has traditionally been studied assuming a set of ground-truth topic-related input documents is provided. In practice, the input document set is unlikely to be available a priori and would need to be retrieved based on an information need, a setting we call open-domain MDS. We experiment with current state-of-the-art retrieval and summarization models on several popular MDS datasets extended to the opendomain setting. We find that existing summarizers suffer large reductions in performance when applied as-is to this more realistic task, though training summarizers with retrieved inputs can reduce their sensitivity retrieval errors. To further probe these findings, we conduct perturbation experiments on summarizer inputs to study the impact of different types of document retrieval errors. Based on our results, we provide practical guidelines to help facilitate a shift to open-domain MDS. We release our code and experimental results alongside all data or model artifacts created during our investigation. 1"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:TFP_iSt0sucC", "num_citations": 7, "citedby_url": "/scholar?hl=en&cites=6842274821805570410", "cites_id": ["6842274821805570410"], "pub_url": "https://llwang.net/assets/pdf/2022_giorgi_openmds.pdf", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:avlR0bKk9F4J:scholar.google.com/", "cites_per_year": {"2022": 1, "2023": 1, "2024": 4, "2025": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Overview of the second workshop on scholarly document processing", "pub_year": 2021, "citation": "Oak Ridge National Laboratory (ORNL), Oak Ridge, TN (United States), 2021", "author": "Iz Beltagy and Arman Cohan and Guy Feigenblat and Dayne Freitag and Tirthankar Ghosal and Keith Hall and Dasha Herrmannova and Petr Knoth and Kyle Lo and Philipp Mayr and Robert Patton and Michal Shmueli-Scheuer and Anita de Waard and Kuansan Wang and Lucy Lu Wang", "publisher": "Oak Ridge National Laboratory (ORNL), Oak Ridge, TN (United States)", "abstract": "With the ever-increasing pace of research and high volume of scholarly communication, scholars face a daunting task. Not only must they keep up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. These needs have motivated an increasing focus on computational methods for enhancing search, summarization, and analysis of scholarly documents. However, the various strands of research on scholarly document processing remain fragmented. To reach out to the broader NLP and AI/ML community, pool distributed efforts in this area, and enable shared access to published research, we held the 2nd Workshop on Scholarly Document Processing (SDP) at NAACL 2021 as a virtual event (https://sdproc.org/2021/). The SDP workshop consisted of a research track, three invited talks, and three Shared Tasks (LongSumm 2021, SCIVER, and 3C). The program was geared towards the application of NLP, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and providing solutions to open challenges."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:R3hNpaxXUhUC", "num_citations": 7, "citedby_url": "/scholar?hl=en&cites=15964712197679675889,695568424151593519", "cites_id": ["15964712197679675889", "695568424151593519"], "pub_url": "https://www.osti.gov/biblio/1830126", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:8fkKvCgNjt0J:scholar.google.com/", "cites_per_year": {"2021": 1, "2022": 2, "2023": 1, "2024": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "InfoLossQA: Characterizing and recovering information loss in text simplification", "pub_year": 2024, "citation": "arXiv preprint arXiv:2401.16475, 2024", "author": "Jan Trienes and Sebastian Joseph and J\u00f6rg Schl\u00f6tterer and Christin Seifert and Kyle Lo and Wei Xu and Byron C Wallace and Junyi Jessy Li", "journal": "arXiv preprint arXiv:2401.16475", "abstract": "Text simplification aims to make technical texts more accessible to laypeople but often results in deletion of information and vagueness. This work proposes InfoLossQA, a framework to characterize and recover simplification-induced information loss in form of question-and-answer (QA) pairs. Building on the theory of Question Under Discussion, the QA pairs are designed to help readers deepen their knowledge of a text. We conduct a range of experiments with this framework. First, we collect a dataset of 1,000 linguist-curated QA pairs derived from 104 LLM simplifications of scientific abstracts of medical studies. Our analyses of this data reveal that information loss occurs frequently, and that the QA pairs give a high-level overview of what information was lost. Second, we devise two methods for this task: end-to-end prompting of open-source and commercial language models, and a natural language inference pipeline. With a novel evaluation framework considering the correctness of QA pairs and their linguistic suitability, our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:70eg2SAEIzsC", "num_citations": 5, "citedby_url": "/scholar?hl=en&cites=15367051424990158805", "cites_id": ["15367051424990158805"], "pub_url": "https://arxiv.org/abs/2401.16475", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:1d-yV9q7QtUJ:scholar.google.com/", "cites_per_year": {"2024": 3, "2025": 2}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Efficiency pentathlon: A standardized arena for efficiency evaluation", "pub_year": 2023, "citation": "arXiv preprint arXiv:2307.09701, 2023", "author": "Hao Peng and Qingqing Cao and Jesse Dodge and Matthew E Peters and Jared Fernandez and Tom Sherborne and Kyle Lo and Sam Skjonsberg and Emma Strubell and Darrell Plessas and Iz Beltagy and Evan Pete Walsh and Noah A Smith and Hannaneh Hajishirzi", "journal": "arXiv preprint arXiv:2307.09701", "abstract": "Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:GnPB-g6toBAC", "num_citations": 5, "citedby_url": "/scholar?hl=en&cites=14372256312551936233", "cites_id": ["14372256312551936233"], "pub_url": "https://arxiv.org/abs/2307.09701", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:6RCGAviCdMcJ:scholar.google.com/", "cites_per_year": {"2023": 1, "2024": 4}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Automatic question answering for multiple stakeholders, the epidemic question answering dataset", "pub_year": 2022, "citation": "Scientific Data 9 (1), 432, 2022", "author": "Travis R Goodwin and Dina Demner-Fushman and Kyle Lo and Lucy Lu Wang and Hoa T Dang and Ian M Soboroff", "journal": "Scientific Data", "volume": "9", "number": "1", "pages": "432", "publisher": "Nature Publishing Group UK", "abstract": "One of the effects of COVID-19 pandemic is a rapidly growing and changing stream of publications to inform clinicians, researchers, policy makers, and patients about the health, socio-economic, and cultural consequences of the pandemic. Managing this information stream manually is not feasible. Automatic Question Answering can quickly bring the most salient points to the user\u2019s attention. Leveraging a collection of scientific articles, government websites, relevant news articles, curated social media posts, and questions asked by researchers, clinicians, and the general public, we developed a dataset to explore automatic Question Answering for multiple stakeholders. Analysis of questions asked by various stakeholders shows that while information needs of experts and the public may overlap, satisfactory answers to these questions often originate from different information sources or benefit from different \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:TQgYirikUcIC", "num_citations": 5, "citedby_url": "/scholar?hl=en&cites=14978306393533700490", "cites_id": ["14978306393533700490"], "pub_url": "https://www.nature.com/articles/s41597-022-01533-w", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:itmJc06i3c8J:scholar.google.com/", "cites_per_year": {"2023": 4, "2024": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Contextualized evaluations: Taking the guesswork out of language model evaluations", "pub_year": 2024, "citation": "arXiv preprint arXiv:2411.07237, 2024", "author": "Chaitanya Malaviya and Joseph Chee Chang and Dan Roth and Mohit Iyyer and Mark Yatskar and Kyle Lo", "journal": "arXiv preprint arXiv:2411.07237", "abstract": "Language model users often issue queries that lack specification, where the context under which a query was issued -- such as the user's identity, the query's intent, and the criteria for a response to be useful -- is not explicit. For instance, a good response to a subjective query like \"What book should I read next?\" would depend on the user's preferences, and a good response to an open-ended query like \"How do antibiotics work against bacteria?\" would depend on the user's expertise. This makes evaluation of responses to such queries an ill-posed task, as evaluators may make arbitrary judgments about the response quality. To remedy this, we present contextualized evaluations, a protocol that synthetically constructs context surrounding an underspecified query and provides it during evaluation. We find that the presence of context can 1) alter conclusions drawn from evaluation, even flipping win rates between model pairs, 2) nudge evaluators to make fewer judgments based on surface-level criteria, like style, and 3) provide new insights about model behavior across diverse contexts. Specifically, our procedure uncovers an implicit bias towards WEIRD contexts in models' \"default\" responses and we find that models are not equally sensitive to following different contexts, even when they are provided in prompts."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:_xSYboBqXhAC", "num_citations": 4, "citedby_url": "/scholar?hl=en&cites=10195936697449568109", "cites_id": ["10195936697449568109"], "pub_url": "https://arxiv.org/abs/2411.07237", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:bdfZ62c-f40J:scholar.google.com/", "cites_per_year": {"2024": 1, "2025": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Kiwi: A dataset of knowledge-intensive writing instructions for answering research questions", "pub_year": 2024, "citation": "arXiv preprint arXiv:2403.03866, 2024", "author": "Fangyuan Xu and Kyle Lo and Luca Soldaini and Bailey Kuehl and Eunsol Choi and David Wadden", "journal": "arXiv preprint arXiv:2403.03866", "abstract": "Large language models (LLMs) adapted to follow user instructions are now widely deployed as conversational agents. In this work, we examine one increasingly common instruction-following task: providing writing assistance to compose a long-form answer. To evaluate the capabilities of current LLMs on this task, we construct KIWI, a dataset of knowledge-intensive writing instructions in the scientific domain. Given a research question, an initial model-generated answer and a set of relevant papers, an expert annotator iteratively issues instructions for the model to revise and improve its answer. We collect 1,260 interaction turns from 234 interaction sessions with three state-of-the-art LLMs. Each turn includes a user instruction, a model response, and a human evaluation of the model response. Through a detailed analysis of the collected responses, we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits. Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement. Our findings indicate that KIWI will be a valuable resource to measure progress and improve LLMs' instruction-following capabilities for knowledge intensive writing tasks."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:pqnbT2bcN3wC", "num_citations": 4, "citedby_url": "/scholar?hl=en&cites=12110793402807535142", "cites_id": ["12110793402807535142"], "pub_url": "https://arxiv.org/abs/2403.03866", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:JsZ8Im8uEqgJ:scholar.google.com/", "cites_per_year": {"2024": 4}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Infrastructure for rapid open knowledge network development", "pub_year": 2022, "citation": "AI Magazine 43 (1), 59-68, 2022", "author": "Michael Cafarella and Michael Anderson and Iz Beltagy and Arie Cattan and Sarah Chasins and Ido Dagan and Doug Downey and Oren Etzioni and Sergey Feldman and Tian Gao and Tom Hope and Kexin Huang and Sophie Johnson and Daniel King and Kyle Lo and Yuze Lou and Matthew Shapiro and Dinghao Shen and Shivashankar Subramanian and Lucy Wang and Yuning Wang and Yitong Wang and Daniel Weld and Jenny Vo-Phamhi and Anna Zeng and Jiayun Zou", "journal": "AI Magazine", "volume": "43", "number": "1", "pages": "59-68", "abstract": "The past decade has witnessed a growth in the use of knowledge graph technologies for advanced data search, data integration, and query-answering applications. The leading example of a public, general-purpose open knowledge network (aka knowledge graph) is Wikidata, which has demonstrated remarkable advances in quality and coverage over this time. Proprietary knowledge graphs drive some of the leading applications of the day including, for example, Google Search, Alexa, Siri, and Cortana. Open Knowledge Networks are exciting: they promise the power of structured database-like queries with the potential for the wide coverage that is today only provided by the Web. With the current state of the art, building, using, and scaling large knowledge networks can still be frustratingly slow. This article describes a National Science Foundation Convergence Accelerator project to build a set of Knowledge Network Programming Infrastructure systems to address this issue."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:e5wmG9Sq2KIC", "num_citations": 4, "citedby_url": "/scholar?hl=en&cites=16395387942025166289", "cites_id": ["16395387942025166289"], "pub_url": "https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/19126", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:0f0hf3ceiOMJ:scholar.google.com/", "cites_per_year": {"2022": 1, "2023": 0, "2024": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models", "pub_year": 2024, "citation": "arXiv preprint arXiv:2410.22360, 2024", "author": "Benjamin Newman and Yoonjoo Lee and Aakanksha Naik and Pao Siangliulue and Raymond Fok and Juho Kim and Daniel S Weld and Joseph Chee Chang and Kyle Lo", "journal": "arXiv preprint arXiv:2410.22360", "abstract": "When conducting literature reviews, scientists often create literature review tables - tables whose rows are publications and whose columns constitute a schema, a set of aspects used to compare and contrast the papers. Can we automatically generate these tables using language models (LMs)? In this work, we introduce a framework that leverages LMs to perform this task by decomposing it into separate schema and value generation steps. To enable experimentation, we address two main challenges: First, we overcome a lack of high-quality datasets to benchmark table generation by curating and releasing arxivDIGESTables, a new dataset of 2,228 literature review tables extracted from ArXiv papers that synthesize a total of 7,542 research papers. Second, to support scalable evaluation of model generations against human-authored reference tables, we develop DecontextEval, an automatic evaluation method that aligns elements of tables with the same underlying aspects despite differing surface forms. Given these tools, we evaluate LMs' abilities to reconstruct reference tables, finding this task benefits from additional context to ground the generation (e.g. table captions, in-text references). Finally, through a human evaluation study we find that even when LMs fail to fully reconstruct a reference table, their generated novel aspects can still be useful."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:pyW8ca7W8N0C", "num_citations": 3, "citedby_url": "/scholar?hl=en&cites=12372766301482470548", "cites_id": ["12372766301482470548"], "pub_url": "https://arxiv.org/abs/2410.22360", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:lEzJwGXltKsJ:scholar.google.com/", "cites_per_year": {"2024": 1, "2025": 2}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Back to Basics: A Simple Recipe for Improving Out-of-Domain Retrieval in Dense Encoders", "pub_year": 2023, "citation": "arXiv preprint arXiv:2311.09765, 2023", "author": "Hyunji Lee and Luca Soldaini and Arman Cohan and Minjoon Seo and Kyle Lo", "journal": "arXiv preprint arXiv:2311.09765", "abstract": "Prevailing research practice today often relies on training dense retrievers on existing large datasets such as MSMARCO and then experimenting with ways to improve zero-shot generalization capabilities to unseen domains. While prior work has tackled this challenge through resource-intensive steps such as data augmentation, architectural modifications, increasing model size, or even further base model pretraining, comparatively little investigation has examined whether the training procedures themselves can be improved to yield better generalization capabilities in the resulting models. In this work, we recommend a simple recipe for training dense encoders: Train on MSMARCO with parameter-efficient methods, such as LoRA, and opt for using in-batch negatives unless given well-constructed hard negatives. We validate these recommendations using the BEIR benchmark and find results are persistent across choice of dense encoder and base model size and are complementary to other resource-intensive strategies for out-of-domain generalization such as architectural modifications or additional pretraining. We hope that this thorough and impartial study around various training techniques, which augments other resource-intensive methods, offers practical insights for developing a dense retrieval model that effectively generalizes, even when trained on a single dataset."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:ns9cj8rnVeAC", "num_citations": 3, "citedby_url": "/scholar?hl=en&cites=319636440295616975", "cites_id": ["319636440295616975"], "pub_url": "https://arxiv.org/abs/2311.09765", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:zzEXIaaTbwQJ:scholar.google.com/", "cites_per_year": {"2024": 3}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Scim: Intelligent faceted highlights for interactive, multi-pass skimming of scientific papers, 2022", "pub_year": 2022, "citation": "URL https://arxiv. org/abs/2205.04561 2 (2), 2022", "author": "Raymond Fok and Andrew Head and Jonathan Bragg and Kyle Lo and Marti A Hearst and Daniel S Weld", "journal": "URL https://arxiv. org/abs/2205.04561", "volume": "2", "number": "2"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:abG-DnoFyZgC", "num_citations": 3, "citedby_url": "/scholar?hl=en&cites=207163190529687903", "cites_id": ["207163190529687903"], "pub_url": "https://scholar.google.com/scholar?cluster=207163190529687903&hl=en&oi=scholarr", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:XwEP_tL93wIJ:scholar.google.com/", "cites_per_year": {"2023": 2, "2024": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students' Hand-Drawn Math Images", "pub_year": 2025, "citation": "arXiv preprint arXiv:2501.14877, 2025", "author": "Sami Baral and Li Lucy and Ryan Knight and Alice Ng and Luca Soldaini and Neil T Heffernan and Kyle Lo", "journal": "arXiv preprint arXiv:2501.14877", "abstract": "In real-world settings, vision language models (VLMs) should robustly handle naturalistic, noisy visual content as well as domain-specific language and concepts. For example, K-12 educators using digital learning platforms may need to examine and provide feedback across many images of students' math work. To assess the potential of VLMs to support educators in settings like this one, we introduce DrawEduMath, an English-language dataset of 2,030 images of students' handwritten responses to K-12 math problems. Teachers provided detailed annotations, including free-form descriptions of each image and 11,661 question-answer (QA) pairs. These annotations capture a wealth of pedagogical insights, ranging from students' problem-solving strategies to the composition of their drawings, diagrams, and writing. We evaluate VLMs on teachers' QA pairs, as well as 44,362 synthetic QA pairs derived from teachers' descriptions using language models (LMs). We show that even state-of-the-art VLMs leave much room for improvement on DrawEduMath questions. We also find that synthetic QAs, though imperfect, can yield similar model rankings as teacher-written QAs. We release DrawEduMath to support the evaluation of VLMs' abilities to reason mathematically over images gathered with educational contexts in mind."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:yD5IFk8b50cC", "num_citations": 2, "citedby_url": "/scholar?hl=en&cites=13777857990865557053", "cites_id": ["13777857990865557053"], "pub_url": "https://arxiv.org/abs/2501.14877", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:PZZRL9jINL8J:scholar.google.com/", "cites_per_year": {"2025": 2}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Towards multi-document summarization in the open-domain", "pub_year": 2022, "citation": "", "author": "John Giorgi and Luca Soldaini and Bo Wang and Gary Bader and Kyle Lo and Lucy Lu Wang and Arman Cohan"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:B3FOqHPlNUQC", "num_citations": 2, "citedby_url": "/scholar?hl=en&cites=11516155463887073661", "cites_id": ["11516155463887073661"], "pub_url": "https://scholar.google.com/scholar?cluster=11516155463887073661&hl=en&oi=scholarr", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:fbEpE2Ga0Z8J:scholar.google.com/", "cites_per_year": {"2023": 1, "2024": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "RouterRetriever: Routing over a Mixture of Expert Embedding Models", "pub_year": 2024, "citation": "arXiv preprint arXiv:2409.02685, 2024", "author": "Hyunji Lee and Luca Soldaini and Arman Cohan and Minjoon Seo and Kyle Lo", "journal": "arXiv preprint arXiv:2409.02685", "abstract": "Information retrieval methods often rely on a single embedding model trained on large, general-domain datasets like MSMARCO. While this approach can produce a retriever with reasonable overall performance, they often underperform models trained on domain-specific data when testing on their respective domains. Prior work in information retrieval has tackled this through multi-task training, but the idea of routing over a mixture of domain-specific expert retrievers remains unexplored despite the popularity of such ideas in language model generation research. In this work, we introduce RouterRetriever, a retrieval model that leverages a mixture of domain-specific experts by using a routing mechanism to select the most appropriate expert for each query. RouterRetriever is lightweight and allows easy addition or removal of experts without additional training. Evaluation on the BEIR benchmark demonstrates that RouterRetriever outperforms both models trained on MSMARCO (+2.1 absolute nDCG@10) and multi-task models (+3.2). This is achieved by employing our routing mechanism, which surpasses other routing techniques (+1.8 on average) commonly used in language modeling. Furthermore, the benefit generalizes well to other datasets, even in the absence of a specific expert on the dataset. RouterRetriever is the first work to demonstrate the advantages of routing over a mixture of domain-specific expert embedding models as an alternative to a single, general-purpose embedding model, especially when retrieving from diverse, specialized domains."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:rO6llkc54NcC", "num_citations": 1, "citedby_url": "/scholar?hl=en&cites=2632202342097026510", "cites_id": ["2632202342097026510"], "pub_url": "https://arxiv.org/abs/2409.02685", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:zv2kiRV2hyQJ:scholar.google.com/", "cites_per_year": {"2024": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MathFish: Evaluating Language Model Math Reasoning via Grounding in Educational Curricula", "pub_year": 2024, "citation": "arXiv preprint arXiv:2408.04226, 2024", "author": "Li Lucy and Tal August and Rose E Wang and Luca Soldaini and Courtney Allison and Kyle Lo", "journal": "arXiv preprint arXiv:2408.04226", "abstract": "To ensure that math curriculum is grade-appropriate and aligns with critical skills or concepts in accordance with educational standards, pedagogical experts can spend months carefully reviewing published math problems. Drawing inspiration from this process, our work presents a novel angle for evaluating language models' (LMs) mathematical abilities, by investigating whether they can discern skills and concepts enabled by math content. We contribute two datasets: one consisting of 385 fine-grained descriptions of K-12 math skills and concepts, or standards, from Achieve the Core (ATC), and another of 9.9K math problems labeled with these standards (MathFish). We develop two tasks for evaluating LMs' abilities to assess math problems: (1) verifying whether a problem aligns with a given standard, and (2) tagging a problem with all aligned standards. Working with experienced teachers, we find that LMs struggle to tag and verify standards linked to problems, and instead predict labels that are close to ground truth, but differ in subtle ways. We also show that LMs often generate problems that do not fully align with standards described in prompts, suggesting the need for careful scrutiny on use cases involving LMs for generating curricular materials. Finally, we categorize problems in GSM8k using math standards, allowing us to better understand why some problems are more difficult to solve for models than others."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:a0OBvERweLwC", "num_citations": 1, "citedby_url": "/scholar?hl=en&cites=557852472798341871", "cites_id": ["557852472798341871"], "pub_url": "https://arxiv.org/abs/2408.04226", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:7-o3Tt7jvQcJ:scholar.google.com/", "cites_per_year": {"2025": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Are layout-infused language models robust to layout distribution shifts? a case study with scientific documents", "pub_year": 2023, "citation": "arXiv preprint arXiv:2306.01058, 2023", "author": "Catherine Chen and Zejiang Shen and Dan Klein and Gabriel Stanovsky and Doug Downey and Kyle Lo", "journal": "arXiv preprint arXiv:2306.01058", "abstract": "Recent work has shown that infusing layout features into language models (LMs) improves processing of visually-rich documents such as scientific papers. Layout-infused LMs are often evaluated on documents with familiar layout features (e.g., papers from the same publisher), but in practice models encounter documents with unfamiliar distributions of layout features, such as new combinations of text sizes and styles, or new spatial configurations of textual elements. In this work we test whether layout-infused LMs are robust to layout distribution shifts. As a case study we use the task of scientific document structure recovery, segmenting a scientific paper into its structural categories (e.g., \"title\", \"caption\", \"reference\"). To emulate distribution shifts that occur in practice we re-partition the GROTOAP2 dataset. We find that under layout distribution shifts model performance degrades by up to 20 F1. Simple training strategies, such as increasing training diversity, can reduce this degradation by over 35% relative F1; however, models fail to reach in-distribution performance in any tested out-of-distribution conditions. This work highlights the need to consider layout distribution shifts during model evaluation, and presents a methodology for conducting such evaluations."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:BqipwSGYUEgC", "num_citations": 1, "citedby_url": "/scholar?hl=en&cites=10608497555100420750", "cites_id": ["10608497555100420750"], "pub_url": "https://arxiv.org/abs/2306.01058", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:jkpc9FH0OJMJ:scholar.google.com/", "cites_per_year": {"2023": 1}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models", "pub_year": 2025, "citation": "arXiv preprint arXiv:2502.18443, 2025", "author": "Jake Poznanski and Jon Borchardt and Jason Dunkelberger and Regan Huff and Daniel Lin and Aman Rangapur and Christopher Wilhelm and Kyle Lo and Luca Soldaini", "journal": "arXiv preprint arXiv:2502.18443", "abstract": "PDF documents have the potential to provide trillions of novel, high-quality tokens for training language models. However, these documents come in a diversity of types with differing formats and visual layouts that pose a challenge when attempting to extract and faithfully represent the underlying content for language model use. We present olmOCR, an open-source Python toolkit for processing PDFs into clean, linearized plain text in natural reading order while preserving structured content like sections, tables, lists, equations, and more. Our toolkit runs a fine-tuned 7B vision language model (VLM) trained on a sample of 260,000 pages from over 100,000 crawled PDFs with diverse properties, including graphics, handwritten text and poor quality scans. olmOCR is optimized for large-scale batch processing, able to scale flexibly to different hardware setups and convert a million PDF pages for only $190 USD. We release all components of olmOCR including VLM weights, data and training code, as well as inference code built on serving frameworks including vLLM and SGLang."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:PELIpwtuRlgC", "num_citations": 0, "pub_url": "https://arxiv.org/abs/2502.18443", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:uyVICmVumwAJ:scholar.google.com/", "cites_per_year": {}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Automatic Detection of Research Values from Scientific Abstracts Across Computer Science Subfields", "pub_year": 2025, "citation": "arXiv preprint arXiv:2502.16390, 2025", "author": "Hang Jiang and Tal August and Luca Soldaini and Kyle Lo and Maria Antoniak", "journal": "arXiv preprint arXiv:2502.16390", "abstract": "The field of Computer science (CS) has rapidly evolved over the past few decades, providing computational tools and methodologies to various fields and forming new interdisciplinary communities. This growth in CS has significantly impacted institutional practices and relevant research communities. Therefore, it is crucial to explore what specific \\textbf{research values}, known as \\textbf{basic and fundamental beliefs that guide or motivate research attitudes or actions}, CS-related research communities promote. Prior research has manually analyzed research values from a small sample of machine learning papers \\cite{facct}. No prior work has studied the automatic detection of research values in CS from large-scale scientific texts across different research subfields. This paper introduces a detailed annotation scheme featuring \\textbf{ten research values} that guide CS-related research. Based on the scheme, we build value classifiers to scale up the analysis and present a systematic study over 226,600 paper abstracts from 32 CS-related subfields and 86 popular publishing venues over ten years."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:_B80troHkn4C", "num_citations": 0, "pub_url": "https://arxiv.org/abs/2502.16390", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:qXS7NuWA3p8J:scholar.google.com/", "cites_per_year": {}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Organize the Web: Constructing Domains Enhances Pre-Training Data Curation", "pub_year": 2025, "citation": "arXiv preprint arXiv:2502.10341, 2025", "author": "Alexander Wettig and Kyle Lo and Sewon Min and Hannaneh Hajishirzi and Danqi Chen and Luca Soldaini", "journal": "arXiv preprint arXiv:2502.10341", "abstract": "Modern language models are trained on large, unstructured datasets consisting of trillions of tokens and obtained by crawling the web. The unstructured nature makes it difficult to reason about their contents and develop systematic approaches to data curation. In this paper, we unpack monolithic web corpora by developing taxonomies of their contents and organizing them into domains. We introduce WebOrganizer, a framework for organizing web pages in terms of both their topic and format. Using these two complementary notions of domains, we automatically annotate pre-training data by distilling annotations from a large language model into efficient classifiers. This allows us to study how data from different domains should be mixed to improve models on downstream tasks, and we show that we can combine insights about effective topics and formats to further boost performance. We demonstrate that our domain mixing also improves existing methods that select data based on quality. Furthermore, we study and compare how quality-based methods will implicitly change the domain mixture. Overall, our work demonstrates that constructing and mixing domains provides a valuable complement to quality-based data curation methods, opening new avenues for effective and insightful pre-training data curation."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:AXPGKjj_ei8C", "num_citations": 0, "pub_url": "https://arxiv.org/abs/2502.10341", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:C9s4AxD6TlgJ:scholar.google.com/", "cites_per_year": {}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Accelerating Scientific Paper Skimming with Augmented Intelligence Through Customizable Faceted Highlights", "pub_year": 2024, "citation": "ACM Transactions on Interactive Intelligent Systems 14 (4), 1-30, 2024", "author": "Raymond Fok and Luca Soldaini and Cassidy Trier and Erin Bransom and Kelsey MacMillan and Evie Cheng and Hita Kambhamettu and Jonathan Bragg and Kyle Lo and Marti A Hearst and Andrew Head and Daniel S Weld", "journal": "ACM Transactions on Interactive Intelligent Systems", "volume": "14", "number": "4", "pages": "1-30", "publisher": "ACM", "abstract": "Scholars need to keep up with an exponentially increasing flood of scientific papers. To aid this challenge, we introduce Scim, a novel intelligent interface that helps scholars skim papers to rapidly review and gain a cursory understanding of its contents. Scim supports the skimming process by highlighting salient content within a paper, directing a scholar\u2019s attention. These automatically-extracted highlights are faceted by content type, evenly-distributed across a paper, and have a density configurable by scholars. We evaluate Scim with an in-lab usability study and a longitudinal diary study, revealing how its highlights facilitate the more efficient construction of a conceptualization of a paper. Finally, we describe the process of scaling highlights from their conception within Scim, a research prototype, to production on over 521,000 papers within the Semantic Reader, a publicly-available augmented reading interface \u2026"}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:ZHo1McVdvXMC", "num_citations": 0, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3665648", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:NzM5ShnMU54J:scholar.google.com/", "cites_per_year": {}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Evaluating Language Model Math Reasoning via Grounding in Educational Curricula", "pub_year": 2024, "citation": "arXiv e-prints, arXiv: 2408.04226, 2024", "author": "Li Lucy and Tal August and Rose E Wang and Luca Soldaini and Courtney Allison and Kyle Lo", "journal": "arXiv e-prints", "pages": "arXiv: 2408.04226", "abstract": "Our work presents a novel angle for evaluating language models'(LMs) mathematical abilities, by investigating whether they can discern skills and concepts enabled by math content. We contribute two datasets: one consisting of 385 fine-grained descriptions of K-12 math skills and concepts, or standards, from Achieve the Core (ATC), and another of 9.9 K problems labeled with these standards (MathFish). Working with experienced teachers, we find that LMs struggle to tag and verify standards linked to problems, and instead predict labels that are close to ground truth, but differ in subtle ways. We also show that LMs often generate problems that do not fully align with standards described in prompts. Finally, we categorize problems in GSM8k using math standards, allowing us to better understand why some problems are more difficult to solve for models than others."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:4OULZ7Gr8RgC", "num_citations": 0, "pub_url": "https://ui.adsabs.harvard.edu/abs/2024arXiv240804226L/abstract", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:3T8nZXfZlNgJ:scholar.google.com/", "cites_per_year": {}}
{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Complex Mathematical Symbol Definition Structures: A Dataset and Model for Coordination Resolution in Definition Extraction", "pub_year": 2023, "citation": "arXiv preprint arXiv:2305.14660, 2023", "author": "Anna Martin-Boyle and Andrew Head and Kyle Lo and Risham Sidhu and Marti A Hearst and Dongyeop Kang", "journal": "arXiv preprint arXiv:2305.14660", "abstract": "Mathematical symbol definition extraction is important for improving scholarly reading interfaces and scholarly information extraction (IE). However, the task poses several challenges: math symbols are difficult to process as they are not composed of natural language morphemes; and scholarly papers often contain sentences that require resolving complex coordinate structures. We present SymDef, an English language dataset of 5,927 sentences from full-text scientific papers where each sentence is annotated with all mathematical symbols linked with their corresponding definitions. This dataset focuses specifically on complex coordination structures such as \"respectively\" constructions, which often contain overlapping definition spans. We also introduce a new definition extraction method that masks mathematical symbols, creates a copy of each sentence for each symbol, specifies a target symbol, and predicts its corresponding definition spans using slot filling. Our experiments show that our definition extraction model significantly outperforms RoBERTa and other strong IE baseline systems by 10.9 points with a macro F1 score of 84.82. With our dataset and model, we can detect complex definitions in scholarly documents to make scientific writing more readable."}, "filled": true, "author_pub_id": "VJS12uMAAAAJ:YFjsv_pBGBYC", "num_citations": 0, "pub_url": "https://arxiv.org/abs/2305.14660", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:HvJBuRuqYoYJ:scholar.google.com/", "cites_per_year": {}}
